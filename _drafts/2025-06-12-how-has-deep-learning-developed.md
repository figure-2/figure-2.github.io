---
title: "ğŸš€ ê°œë¡ : Deep Learning ì€ ì–´ë–»ê²Œ ë°œì „í•´ì™”ì„ê¹Œ?"
date: 2025-06-12 10:45:00 +0900
categories:
  - DEEP_LEARNING
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
  - AI
  - ë”¥ëŸ¬ë‹
  - deeplearning
  - machinelearning
  - ë¨¸ì‹ ëŸ¬ë‹
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ¯ AI, ML, DLì˜ ê´€ê³„ ì´í•´í•˜ê¸°

AI(Artificial Intelligence), ML(Machine Learning), DL(Deep Learning)ì˜ ê´€ê³„ë¥¼ ë¨¼ì € ì •ë¦¬í•´ë³´ì.

### AI, ML, DLì˜ ê°œë…ì  í¬í•¨ ê´€ê³„

**AI(ì¸ê³µì§€ëŠ¥)**ëŠ” ê°ì§€, ì¶”ë¡ , í–‰ë™, ì ì‘ì´ ê°€ëŠ¥í•œ í”„ë¡œê·¸ë¨ì„ ì˜ë¯¸í•˜ë©°, ê°€ì¥ í¬ê´„ì ì¸ ê°œë…ì´ë‹¤. **ML(ë¨¸ì‹ ëŸ¬ë‹)**ì€ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ëŠ” AIì˜ í•œ ë¶„ì•¼ì´ê³ , **DL(ë”¥ëŸ¬ë‹)**ì€ ëª¨ë¸ì˜ êµ¬ì¡°ê°€ ë‰´ëŸ´ë„· ê¸°ë°˜ì¸ MLì˜ í•œ ë¶„ì•¼ë‹¤.

```python
# AI, ML, DLì˜ ê´€ê³„ë¥¼ ì‹œê°í™”
import matplotlib.pyplot as plt
import matplotlib.patches as patches

fig, ax = plt.subplots(figsize=(8, 8))

# AI ì› (ê°€ì¥ í° ì›)
ai_circle = patches.Circle((0.5, 0.5), 0.4, fill=False, edgecolor='blue', linewidth=2)
ax.add_patch(ai_circle)
ax.text(0.5, 0.85, 'AI', ha='center', fontsize=14, weight='bold')

# ML ì›
ml_circle = patches.Circle((0.5, 0.45), 0.25, fill=False, edgecolor='green', linewidth=2)
ax.add_patch(ml_circle)
ax.text(0.5, 0.65, 'ML', ha='center', fontsize=12, weight='bold')

# DL ì›
dl_circle = patches.Circle((0.5, 0.4), 0.12, fill=False, edgecolor='red', linewidth=2)
ax.add_patch(dl_circle)
ax.text(0.5, 0.4, 'DL', ha='center', fontsize=10, weight='bold')

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_aspect('equal')
ax.axis('off')
plt.title('AI, ML, DLì˜ í¬í•¨ ê´€ê³„', fontsize=16)
plt.show()
```

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°]

## ğŸ“Š ë”¥ëŸ¬ë‹ ë°œì „ì˜ 5ë‹¨ê³„ ê°œìš”

AI/ML/DL ê´€ì ì—ì„œ ê°œë°œ ë°©ë²•ë¡ ì€ í¬ê²Œ 5ë‹¨ê³„ë¡œ ì§„í™”í•´ì™”ë‹¤. ì´ë¥¼ ì†Œí”„íŠ¸ì›¨ì–´ ë²„ì „ìœ¼ë¡œ í‘œí˜„í•˜ë©´ SW1.0, SW1.5, SW2.0, SW2.5, SW3.0ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

```mermaid
graph LR
    A[1. Rule based<br/>programming] --> B[2. Conventional<br/>machine learning]
    B --> C[3. Deep<br/>Learning]
    C --> D[4. Pre-training &<br/>Fine-tuning]
    D --> E[5. Big Model &<br/>zero/few shot]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style C fill:#bfb,stroke:#333,stroke-width:2px
    style D fill:#fbf,stroke:#333,stroke-width:2px
    style E fill:#ffb,stroke:#333,stroke-width:2px
```

## ğŸ”§ 1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ í”„ë¡œê·¸ë˜ë° (Rule-based Programming)

### ê°œë… ì†Œê°œ

ê·œì¹™ ê¸°ë°˜ í”„ë¡œê·¸ë˜ë°ì€ ëª©í‘œ ë‹¬ì„±ì— í•„ìš”í•œ ëª¨ë“  ì—°ì‚° ë°©ë²•ì„ ì‚¬ëŒì´ ì§ì ‘ ì„¤ê³„í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ì–‘ì´ë¥¼ ì¸ì‹í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ë§Œë“ ë‹¤ë©´ "ê·€ ê¸¸ì´ê°€ 10 ì´ìƒì´ê³ , ì½” ìƒ‰ê¹”ì´ ê²€ì€ìƒ‰ì´ê³ , ëˆˆ ìƒ‰ê¹”ì´ ì´ˆë¡ìƒ‰ì´ë©´ ê³ ì–‘ì´"ì™€ ê°™ì€ ê·œì¹™ì„ ì‚¬ëŒì´ ì§ì ‘ ì½”ë”©í•œë‹¤.

### ì½”ë“œ ì˜ˆì‹œ

```python
def classify_animal(ear_length, nose_color, eye_color, nose_size):
    """
    ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë™ë¬¼ì„ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜
    """
    # ê³ ì–‘ì´ íŒë³„ ê·œì¹™
    if ear_length > 10 and nose_color == "black" and eye_color == "green":
        if nose_size > 3:
            return "CAT"
    
    # ê°•ì•„ì§€ íŒë³„ ê·œì¹™
    elif ear_length > 15 and nose_color == "brown" and eye_color == "brown":
        return "DOG"
    
    # ê¸°íƒ€
    else:
        return "UNKNOWN"

# ì‚¬ìš© ì˜ˆì‹œ
result = classify_animal(12, "black", "green", 4)
print(f"ë¶„ë¥˜ ê²°ê³¼: {result}")  # ì¶œë ¥: ë¶„ë¥˜ ê²°ê³¼: CAT
```

### ë¨¸ì‹ ëŸ¬ë‹/ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì—ì„œì˜ í™œìš© ì‚¬ë¡€

- **ì „ë¬¸ê°€ ì‹œìŠ¤í…œ**: ì˜ë£Œ ì§„ë‹¨, ë²•ë¥  ìë¬¸ ë“±ì—ì„œ ì‚¬ìš©
- **ê²Œì„ AI**: ì²´ìŠ¤, ë°”ë‘‘ ë“±ì˜ ì´ˆê¸° AI (ê·œì¹™ ê¸°ë°˜)
- **ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§**: ì‹ ìš© í‰ê°€, ë³´í—˜ ì‹¬ì‚¬ ë“±

> ê·œì¹™ ê¸°ë°˜ í”„ë¡œê·¸ë˜ë°ì˜ í•œê³„ëŠ” ë³µì¡í•œ íŒ¨í„´ì„ ëª¨ë‘ ê·œì¹™ìœ¼ë¡œ ì •ì˜í•˜ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì´ë‹¤. ì‹¤ì œ ì„¸ê³„ì˜ ê³ ì–‘ì´ëŠ” ë„ˆë¬´ë‚˜ ë‹¤ì–‘í•œ ëª¨ìŠµì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤. {: .prompt-tip}

## ğŸ¤– 2ë‹¨ê³„: ì „í†µ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²• (Conventional Machine Learning)

### ê°œë… ì†Œê°œ

ì „í†µ ë¨¸ì‹ ëŸ¬ë‹ì€ **íŠ¹ì§•ê°’(feature) ì¶”ì¶œì€ ì‚¬ëŒì´ ì„¤ê³„**í•˜ë˜, **íŠ¹ì§•ê°’ë“¤ë¡œ íŒë³„í•˜ëŠ” ë¡œì§ì€ ê¸°ê³„ê°€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ**í•˜ëŠ” ë°©ì‹ì´ë‹¤. SW1.0ê³¼ SW2.0ì˜ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

### ìˆ˜í•™ì  í‘œí˜„

ë¨¸ì‹ ëŸ¬ë‹ì˜ í•µì‹¬ì€ **ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” í•¨ìˆ˜**ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤:

$$ \min_{\theta} \sum_{i=1}^{n} L(f_{\theta}(x_i), y_i) $$

ì—¬ê¸°ì„œ $f_{\theta}$ëŠ” í•™ìŠµí•  ëª¨ë¸, $x_i$ëŠ” ì…ë ¥ íŠ¹ì§•ê°’, $y_i$ëŠ” ì •ë‹µ ë ˆì´ë¸”, $L$ì€ ì†ì‹¤ í•¨ìˆ˜ë‹¤.

### ë¨¸ì‹ ëŸ¬ë‹ ë™ì‘ ê³¼ì • ìƒì„¸ ì„¤ëª…

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°]

#### í•™ìŠµ ë°ì´í„° ì¤€ë¹„

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# íŠ¹ì§•ê°’ ì¶”ì¶œ (ì‚¬ëŒì´ ì„¤ê³„)
def extract_features(image):
    """
    ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§•ê°’ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    ì‚¬ëŒì´ ì–´ë–¤ íŠ¹ì§•ì„ ë³¼ì§€ ë¯¸ë¦¬ ì •ì˜
    """
    features = {
        'ear_length': measure_ear_length(image),
        'nose_color_r': get_nose_color(image)[0],
        'nose_color_g': get_nose_color(image)[1],
        'nose_color_b': get_nose_color(image)[2],
        'eye_color_r': get_eye_color(image)[0],
        'eye_color_g': get_eye_color(image)[1],
        'eye_color_b': get_eye_color(image)[2],
        'hair_color_r': get_hair_color(image)[0],
        'hair_color_g': get_hair_color(image)[1],
        'hair_color_b': get_hair_color(image)[2]
    }
    return features

# ë°ì´í„° ì¤€ë¹„
data = []
labels = []

for image, label in dataset:
    features = extract_features(image)
    data.append(list(features.values()))
    labels.append(label)

X = np.array(data)
y = np.array(labels)

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

#### Try & Error ë°©ì‹ì˜ í•™ìŠµ

```python
# ì—¬ëŸ¬ ëª¨ë¸ì„ ì‹œë„í•´ë³´ê³  ìµœì ì˜ ëª¨ë¸ ì°¾ê¸°
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

models = {
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'SVM': SVC(kernel='rbf'),
    'DecisionTree': DecisionTreeClassifier()
}

best_model = None
best_score = 0

for name, model in models.items():
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡ ë° í‰ê°€
    predictions = model.predict(X_test)
    score = accuracy_score(y_test, predictions)
    
    print(f"{name} ì •í™•ë„: {score:.2f}")
    
    # ìµœì  ëª¨ë¸ ì—…ë°ì´íŠ¸
    if score > best_score:
        best_score = score
        best_model = model

print(f"\nìµœì  ëª¨ë¸ì˜ ì •í™•ë„: {best_score:.2f}")
```

### ë¨¸ì‹ ëŸ¬ë‹/ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì—ì„œì˜ í™œìš© ì‚¬ë¡€

- **ìŠ¤íŒ¸ í•„í„°ë§**: ì´ë©”ì¼ì˜ íŠ¹ì§•(ë‹¨ì–´ ë¹ˆë„, ë°œì‹ ì ë“±)ì„ ì¶”ì¶œí•˜ì—¬ ë¶„ë¥˜
- **ì¶”ì²œ ì‹œìŠ¤í…œ**: ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ë§¤ì¹­
- **ì´ìƒ íƒì§€**: ì •ìƒ íŒ¨í„´ì—ì„œ ë²—ì–´ë‚œ íŠ¹ì§•ì„ ê°ì§€

> ì „í†µ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì„±ëŠ¥ì€ íŠ¹ì§• ì¶”ì¶œì˜ í’ˆì§ˆì— í¬ê²Œ ì¢Œìš°ëœë‹¤. ë„ë©”ì¸ ì „ë¬¸ê°€ì˜ ì§€ì‹ì´ ë§¤ìš° ì¤‘ìš”í•œ ì´ìœ ë‹¤. {: .prompt-warning}

## ğŸ§  3ë‹¨ê³„: ë”¥ëŸ¬ë‹ (Deep Learning)

### ê°œë… ì†Œê°œ

ë”¥ëŸ¬ë‹ì€ **íŠ¹ì§• ì¶”ì¶œë¶€í„° íŒë³„ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ê¸°ê³„ê°€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ**í•˜ëŠ” í˜ëª…ì ì¸ ë°©ì‹ì´ë‹¤. ì‚¬ëŒì€ ë‹¨ì§€ ì›ì‹œ ë°ì´í„°(raw data)ì™€ ì •ë‹µë§Œ ì œê³µí•˜ë©´ ëœë‹¤.

### ë”¥ëŸ¬ë‹ì˜ êµ¬ì¡°ì  íŠ¹ì§•

```python
import tensorflow as tf
from tensorflow import keras

# ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì„± ì˜ˆì‹œ
def create_deep_learning_model(input_shape, num_classes):
    """
    ê°„ë‹¨í•œ CNN ëª¨ë¸ ìƒì„±
    """
    model = keras.Sequential([
        # íŠ¹ì§• ì¶”ì¶œ ë¶€ë¶„ (ìë™ìœ¼ë¡œ í•™ìŠµë¨)
        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        keras.layers.MaxPooling2D((2, 2)),
        keras.layers.Conv2D(64, (3, 3), activation='relu'),
        keras.layers.MaxPooling2D((2, 2)),
        keras.layers.Conv2D(64, (3, 3), activation='relu'),
        
        # ë¶„ë¥˜ ë¶€ë¶„
        keras.layers.Flatten(),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(num_classes, activation='softmax')
    ])
    
    return model

# ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼
model = create_deep_learning_model((224, 224, 3), 2)  # ê³ ì–‘ì´/ê°•ì•„ì§€ ë¶„ë¥˜
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
model.summary()
```

### ì „í†µ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ë°ì´í„° ì¤€ë¹„ ê³¼ì • ë¹„êµ

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°]

#### ì „í†µ ë¨¸ì‹ ëŸ¬ë‹ì˜ ë°ì´í„° ì¤€ë¹„

```python
# ì „í†µ ë¨¸ì‹ ëŸ¬ë‹: íŠ¹ì§•ì„ ìˆ˜ë™ìœ¼ë¡œ ì¶”ì¶œ
traditional_ml_data = {
    'ear_length': [3, 5, 4, ...],
    'nose_color': [(124,10,25), (200,150,100), ...],
    'eye_color': [(33,77,88), (120,80,50), ...],
    'label': ['cat', 'dog', 'cat', ...]
}
```

#### ë”¥ëŸ¬ë‹ì˜ ë°ì´í„° ì¤€ë¹„

```python
# ë”¥ëŸ¬ë‹: ì›ì‹œ ì´ë¯¸ì§€ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# ì´ë¯¸ì§€ ë°ì´í„° ì œë„ˆë ˆì´í„°
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ì´ë¯¸ì§€ ë¡œë“œ
train_generator = datagen.flow_from_directory(
    'data/train',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)
```

### ë”¥ëŸ¬ë‹ì´ ë” ë§ì€ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ì´ìœ 

ë”¥ëŸ¬ë‹ì€ íŠ¹ì§• ì¶”ì¶œê³¼ ë¶„ë¥˜ë¥¼ ëª¨ë‘ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•´ì•¼ í•˜ë¯€ë¡œ, ì „í†µ ë¨¸ì‹ ëŸ¬ë‹ë³´ë‹¤ í›¨ì”¬ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤.

```python
# ë°ì´í„° ì–‘ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ ì‹œë®¬ë ˆì´ì…˜
import matplotlib.pyplot as plt

data_sizes = [100, 500, 1000, 5000, 10000, 50000, 100000]
traditional_ml_acc = [0.65, 0.72, 0.75, 0.78, 0.79, 0.80, 0.80]
deep_learning_acc = [0.45, 0.55, 0.65, 0.75, 0.82, 0.88, 0.92]

plt.figure(figsize=(10, 6))
plt.plot(data_sizes, traditional_ml_acc, 'o-', label='Traditional ML', linewidth=2)
plt.plot(data_sizes, deep_learning_acc, 's-', label='Deep Learning', linewidth=2)
plt.xlabel('ë°ì´í„° í¬ê¸°', fontsize=12)
plt.ylabel('ì •í™•ë„', fontsize=12)
plt.title('ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ', fontsize=14)
plt.xscale('log')
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()
```

### ë¨¸ì‹ ëŸ¬ë‹/ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì—ì„œì˜ í™œìš© ì‚¬ë¡€

- **ì»´í“¨í„° ë¹„ì „**: ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ íƒì§€, ì„¸ê·¸ë©˜í…Œì´ì…˜
- **ìì—°ì–´ ì²˜ë¦¬**: ë²ˆì—­, ê°ì • ë¶„ì„, ì±—ë´‡
- **ìŒì„± ì¸ì‹**: ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- **ììœ¨ì£¼í–‰**: ë„ë¡œ ìƒí™© ì¸ì‹ ë° íŒë‹¨

## ğŸ¯ 4ë‹¨ê³„: Pre-training & Fine-tuning

### ê¸°ì¡´ ë”¥ëŸ¬ë‹ì˜ ë¬¸ì œì 

3ë‹¨ê³„ ë”¥ëŸ¬ë‹ì˜ ê°€ì¥ í° ë¬¸ì œëŠ” **íƒœìŠ¤í¬ê°€ ë°”ë€” ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ ëª¨ë¸ì´ í•„ìš”**í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´:

- ê³ ì–‘ì´/ê°•ì•„ì§€ ë¶„ë¥˜ ëª¨ë¸
- ê°œêµ¬ë¦¬/ì†Œ ë¶„ë¥˜ ëª¨ë¸
- í† ë¼/ê°œêµ¬ë¦¬ ë¶„ë¥˜ ëª¨ë¸

ê°ê° ë³„ë„ë¡œ í•™ìŠµí•´ì•¼ í–ˆë‹¤.

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°]

### Pre-training & Fine-tuningì˜ í•µì‹¬ ì•„ì´ë””ì–´

**"ë²”ìš©ì ì¸ íŠ¹ì§•ì„ ë¨¼ì € í•™ìŠµí•˜ê³ , íŠ¹ì • íƒœìŠ¤í¬ì— ë§ê²Œ ì¡°ì •í•˜ì!"**

```python
import torch
from transformers import AutoModel, AutoTokenizer

# Step 1: Pre-trained ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
pretrained_model = AutoModel.from_pretrained("bert-base-uncased")

# Step 2: íƒœìŠ¤í¬ë³„ í—¤ë“œ ì¶”ê°€
class TaskSpecificModel(torch.nn.Module):
    def __init__(self, pretrained_model, num_classes):
        super().__init__()
        self.base_model = pretrained_model
        self.classifier = torch.nn.Linear(768, num_classes)  # BERT hidden size = 768
        
        # Pre-trained ë¶€ë¶„ì€ freeze (ì„ íƒì )
        for param in self.base_model.parameters():
            param.requires_grad = False
    
    def forward(self, input_ids, attention_mask):
        outputs = self.base_model(input_ids=input_ids, 
                                 attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token
        return self.classifier(pooled_output)

# íƒœìŠ¤í¬ë³„ ëª¨ë¸ ìƒì„±
model_task1 = TaskSpecificModel(pretrained_model, num_classes=2)  # ì´ì§„ ë¶„ë¥˜
model_task2 = TaskSpecificModel(pretrained_model, num_classes=5)  # 5ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜
```

### ì»´í“¨í„° ë¹„ì „ì—ì„œì˜ Pre-training & Fine-tuning

```mermaid
graph TD
    subgraph "Pre-training (Step 1)"
        A[ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹<br/>ImageNet ë“±] --> B[Feature Extractor<br/>í•™ìŠµ]
        B --> C[ì¼ë°˜ì ì¸ ì‹œê°ì  íŠ¹ì§•<br/>ì—£ì§€, í…ìŠ¤ì²˜, í˜•íƒœ ë“±]
    end
    
    subgraph "Fine-tuning (Step 2)"
        C --> D[Frozen Layers<br/>íŠ¹ì§• ì¶”ì¶œ ë¶€ë¶„ ê³ ì •]
        D --> E[Trainable Layers<br/>ë¶„ë¥˜ ë¶€ë¶„ë§Œ í•™ìŠµ]
        E --> F[íŠ¹ì • íƒœìŠ¤í¬ ìˆ˜í–‰<br/>ì˜ë£Œ ì˜ìƒ, ìœ„ì„± ì‚¬ì§„ ë“±]
    end
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#9f9,stroke:#333,stroke:width:2px
```

### í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œì˜ í˜ì‹ : GPT ì‹œë¦¬ì¦ˆ

#### Self-supervised Pre-trainingì˜ ë“±ì¥

GPTì˜ í•µì‹¬ í˜ì‹ ì€ **ë ˆì´ë¸”ì´ ì—†ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ìë™ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±**í•˜ëŠ” ê²ƒì´ë‹¤.

```python
def create_language_modeling_data(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì–¸ì–´ ëª¨ë¸ í•™ìŠµ ë°ì´í„° ìƒì„±
    ì…ë ¥: "ê³° ì„¸ë§ˆë¦¬ê°€ í•œ ì§‘ì— ìˆì–´"
    """
    tokens = text.split()
    training_examples = []
    
    for i in range(1, len(tokens)):
        input_text = " ".join(tokens[:i])
        target = tokens[i]
        training_examples.append({
            "input": input_text,
            "target": target
        })
    
    return training_examples

# ì˜ˆì‹œ
text = "ê³° ì„¸ë§ˆë¦¬ê°€ í•œ ì§‘ì— ìˆì–´ ì•„ë¹ ê³° ì—„ë§ˆê³° ì•„ê¸°ê³°"
examples = create_language_modeling_data(text)

for ex in examples[:3]:
    print(f"ì…ë ¥: {ex['input']} â†’ ì •ë‹µ: {ex['target']}")
    
# ì¶œë ¥:
# ì…ë ¥: ê³° â†’ ì •ë‹µ: ì„¸ë§ˆë¦¬ê°€
# ì…ë ¥: ê³° ì„¸ë§ˆë¦¬ê°€ â†’ ì •ë‹µ: í•œ
# ì…ë ¥: ê³° ì„¸ë§ˆë¦¬ê°€ í•œ â†’ ì •ë‹µ: ì§‘ì—
```

> Self-supervised learningì€ ë ˆì´ë¸”ì´ í•„ìš” ì—†ì–´ ì¸í„°ë„·ì˜ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤. ì´ê²ƒì´ GPTì˜ ì„±ê³µ ë¹„ê²°ì´ë‹¤. {: .prompt-tip}

### GPT ì‹œë¦¬ì¦ˆì˜ ë†€ë¼ìš´ ì„±ì¥

GPT1, GPT2, GPT3ì˜ ë°œì „ ê³¼ì •ì„ ë³´ë©´ **"í¬ê¸°ê°€ ê³§ ì„±ëŠ¥"**ì´ë¼ëŠ” ê³µì‹ì´ ì„±ë¦½í•œë‹¤.

```python
import matplotlib.pyplot as plt
import numpy as np

# GPT ì‹œë¦¬ì¦ˆ ë°ì´í„°
models = ['GPT-1', 'GPT-2', 'GPT-3']
params = [0.117, 1.5, 175]  # ë‹¨ìœ„: Billion
data_size = [5, 40, 45000]  # ë‹¨ìœ„: GB
tokens = [512, 1024, 2048]

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))

# íŒŒë¼ë¯¸í„° ìˆ˜
ax1.bar(models, params, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
ax1.set_ylabel('Parameters (Billion)')
ax1.set_title('ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ì˜ ì¦ê°€')
ax1.set_yscale('log')

# ë°ì´í„° í¬ê¸°
ax2.bar(models, data_size, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
ax2.set_ylabel('Data Size (GB)')
ax2.set_title('í•™ìŠµ ë°ì´í„° í¬ê¸°ì˜ ì¦ê°€')
ax2.set_yscale('log')

# í† í° ê¸¸ì´
ax3.bar(models, tokens, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
ax3.set_ylabel('Max Tokens')
ax3.set_title('ì²˜ë¦¬ ê°€ëŠ¥í•œ í† í° ìˆ˜')

plt.tight_layout()
plt.show()
```

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°]

#### ì‹¤ì œ Fine-tuning ì˜ˆì‹œ

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments

# Pre-trained GPT2 ëª¨ë¸ ë¡œë“œ
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Fine-tuningì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
def prepare_dataset(texts, labels):
    """ê°ì • ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
    inputs = [f"Review: {text}\nSentiment:" for text in texts]
    targets = [f" {label}" for label in labels]
    
    # í† í°í™”
    encodings = tokenizer(inputs, targets, truncation=True, 
                         padding=True, return_tensors='pt')
    return encodings

# Fine-tuning
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# íŠ¹ì • ë ˆì´ì–´ë§Œ í•™ìŠµ (íš¨ìœ¨ì„±ì„ ìœ„í•´)
for param in model.transformer.h[:10].parameters():
    param.requires_grad = False
```

### ë¨¸ì‹ ëŸ¬ë‹/ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì—ì„œì˜ í™œìš© ì‚¬ë¡€

- **BERT**: êµ¬ê¸€ì˜ ê²€ìƒ‰ ì—”ì§„ ê°œì„ 
- **GPT-2**: GitHub Copilotì˜ ê¸°ë°˜ ê¸°ìˆ 
- **Vision Transformer**: ì˜ë£Œ ì˜ìƒ ì§„ë‹¨
- **CLIP**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹œìŠ¤í…œ

## ğŸš€ 5ë‹¨ê³„: Big Model & Zero/Few shot

### íŒ¨ëŸ¬ë‹¤ì„ì˜ ì „í™˜: In-context Learning

5ë‹¨ê³„ì˜ í•µì‹¬ì€ **í•™ìŠµ ì—†ì´ë„ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰**í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°]

### Zero-shot, One-shot, Few-shotì˜ ê°œë…

```python
class InContextLearning:
    def __init__(self, model):
        self.model = model
    
    def zero_shot(self, task_description, query):
        """
        Zero-shot: íƒœìŠ¤í¬ ì„¤ëª…ë§Œìœ¼ë¡œ ìˆ˜í–‰
        """
        prompt = f"{task_description}\n\nInput: {query}\nOutput:"
        return self.model.generate(prompt)
    
    def one_shot(self, task_description, example, query):
        """
        One-shot: í•˜ë‚˜ì˜ ì˜ˆì‹œì™€ í•¨ê»˜
        """
        prompt = f"""{task_description}
        
Example:
Input: {example['input']}
Output: {example['output']}

Input: {query}
Output:"""
        return self.model.generate(prompt)
    
    def few_shot(self, task_description, examples, query):
        """
        Few-shot: ì—¬ëŸ¬ ì˜ˆì‹œì™€ í•¨ê»˜
        """
        prompt = f"{task_description}\n\n"
        
        for ex in examples:
            prompt += f"Input: {ex['input']}\nOutput: {ex['output']}\n\n"
        
        prompt += f"Input: {query}\nOutput:"
        return self.model.generate(prompt)

# ì‚¬ìš© ì˜ˆì‹œ
icl = InContextLearning(gpt3_model)

# Zero-shot ë²ˆì—­
result = icl.zero_shot(
    "Translate English to French:",
    "Hello, how are you?"
)

# One-shot ê°ì • ë¶„ì„
result = icl.one_shot(
    "Classify the sentiment of movie reviews:",
    {"input": "This movie was fantastic!", "output": "Positive"},
    "The acting was terrible and the plot made no sense."
)

# Few-shot ì½”ë“œ ìƒì„±
examples = [
    {"input": "Sort a list", "output": "def sort_list(lst): return sorted(lst)"},
    {"input": "Find maximum", "output": "def find_max(lst): return max(lst)"}
]
result = icl.few_shot(
    "Generate Python functions:",
    examples,
    "Calculate factorial"
)
```

### GPT-3ì˜ ë†€ë¼ìš´ ì„±ëŠ¥

```python
import matplotlib.pyplot as plt
import numpy as np

# ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
model_sizes = [0.1, 0.4, 0.8, 1.3, 2.6, 6.7, 13, 175]  # Billion parameters
zero_shot = [10, 15, 20, 30, 40, 50, 60, 85]  # ì •í™•ë„
one_shot = [15, 20, 25, 35, 45, 55, 65, 90]
few_shot = [20, 25, 30, 40, 50, 60, 70, 95]

plt.figure(figsize=(10, 6))
plt.semilogx(model_sizes, zero_shot, 'o-', label='Zero-shot', linewidth=2)
plt.semilogx(model_sizes, one_shot, 's-', label='One-shot', linewidth=2)
plt.semilogx(model_sizes, few_shot, '^-', label='Few-shot (K=64)', linewidth=2)

plt.xlabel('Parameters (Billions)', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.title('ëª¨ë¸ í¬ê¸°ì™€ In-context Learning ì„±ëŠ¥', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°]

### In-context Learningì˜ ì¥ì 

**ê¸°ì¡´ ë°©ì‹ (Fine-tuning):**

- íƒœìŠ¤í¬ë³„ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”
- íƒœìŠ¤í¬ë³„ ëª¨ë¸ í•™ìŠµ í•„ìš”
- ì‹œê°„ê³¼ ë¹„ìš©ì´ ë§ì´ ë“¦

**ìƒˆë¡œìš´ ë°©ì‹ (In-context Learning):**

- ë°ì´í„° ìˆ˜ì§‘ ë¶ˆí•„ìš”
- ëª¨ë¸ í•™ìŠµ ë¶ˆí•„ìš”
- ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥

> GPT-3ì˜ ë“±ì¥ìœ¼ë¡œ AI ê°œë°œ íŒ¨ëŸ¬ë‹¤ì„ì´ ì™„ì „íˆ ë°”ë€Œì—ˆë‹¤. ì´ì œëŠ” "ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¬ê¹Œ"ê°€ ì•„ë‹ˆë¼ "ì–´ë–»ê²Œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í• ê¹Œ"ê°€ ì¤‘ìš”í•´ì¡Œë‹¤. {: .prompt-tip}

### ì´ˆê±°ëŒ€ ëª¨ë¸ ê²½ìŸ

2022ë…„ ì´í›„ ì´ˆê±°ëŒ€ ëª¨ë¸ ê²½ìŸì´ ë³¸ê²©í™”ë˜ì—ˆë‹¤:

```mermaid
timeline
    title ì´ˆê±°ëŒ€ ì–¸ì–´ëª¨ë¸ì˜ ë°œì „ íƒ€ì„ë¼ì¸
    
    2020 : GPT-3 (175B)
         : OpenAI
    
    2021 : PanGu-Î± (200B)
         : Huawei
         : FLAN (137B)
         : Google
    
    2022 : OPT (175B)
         : Meta
         : PaLM (540B)
         : Google
         : BLOOM (176B)
         : BigScience
         
    2023 : GPT-4
         : Claude
         : Llama 2
```

### ë¨¸ì‹ ëŸ¬ë‹/ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì—ì„œì˜ í™œìš© ì‚¬ë¡€

**Zero-shot ì‘ìš©:**

- ë‹¤êµ­ì–´ ë²ˆì—­ (í•™ìŠµí•˜ì§€ ì•Šì€ ì–¸ì–´ìŒë„ ë²ˆì—­)
- ì½”ë“œ ìƒì„± (ìì—°ì–´ë¡œ ì„¤ëª…í•˜ë©´ ì½”ë“œ ìƒì„±)
- ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ

**Few-shot ì‘ìš©:**

- ë§ì¶¤í˜• ì±—ë´‡ (ëª‡ ê°€ì§€ ì˜ˆì‹œë¡œ ìŠ¤íƒ€ì¼ í•™ìŠµ)
- ë„ë©”ì¸ íŠ¹í™” ë¶„ë¥˜ê¸°
- ì°½ì˜ì  ê¸€ì“°ê¸°

## ğŸ’¡ 5ë‹¨ê³„ ë°œì „ì˜ ì˜ë¯¸ì™€ ë¯¸ë˜

### ê° ë‹¨ê³„ë³„ ë¹„êµ

|ë‹¨ê³„|íŠ¹ì§• ì¶”ì¶œ|ë¶„ë¥˜/íŒë³„|í•„ìš” ë°ì´í„°|ê°œë°œ ì†ë„|
|---|---|---|---|---|
|1ë‹¨ê³„ (ê·œì¹™)|ì‚¬ëŒ|ì‚¬ëŒ|ì—†ìŒ|ë§¤ìš° ëŠë¦¼|
|2ë‹¨ê³„ (ì „í†µ ML)|ì‚¬ëŒ|ê¸°ê³„|ì¤‘ê°„|ì¤‘ê°„|
|3ë‹¨ê³„ (ë”¥ëŸ¬ë‹)|ê¸°ê³„|ê¸°ê³„|ë§ìŒ|ëŠë¦¼|
|4ë‹¨ê³„ (Pre-train)|ê¸°ê³„|ê¸°ê³„|ì ìŒ|ë¹ ë¦„|
|5ë‹¨ê³„ (Zero-shot)|ê¸°ê³„|ê¸°ê³„|ì—†ìŒ|ì¦‰ì‹œ|

### ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ

```python
def choose_approach(task_complexity, data_availability, time_constraint):
    """
    í”„ë¡œì íŠ¸ì— ì í•©í•œ ì ‘ê·¼ ë°©ì‹ ì„ íƒ
    """
    if data_availability == "none" and time_constraint == "immediate":
        return "5ë‹¨ê³„: Zero/Few-shot with GPT-3/4"
    
    elif data_availability == "limited" and task_complexity == "specific":
        return "4ë‹¨ê³„: Fine-tune pre-trained model"
    
    elif data_availability == "abundant" and task_complexity == "complex":
        return "3ë‹¨ê³„: Train deep learning from scratch"
    
    elif task_complexity == "simple" and data_availability == "structured":
        return "2ë‹¨ê³„: Traditional ML"
    
    else:
        return "1ë‹¨ê³„: Rule-based (ìµœí›„ì˜ ìˆ˜ë‹¨)"

# ì˜ˆì‹œ
approach = choose_approach(
    task_complexity="specific",
    data_availability="limited",
    time_constraint="moderate"
)
print(f"ì¶”ì²œ ì ‘ê·¼ë²•: {approach}")
```

## ğŸ¯ í•µì‹¬ ì •ë¦¬

ë”¥ëŸ¬ë‹ì˜ ë°œì „ì€ ë‹¨ìˆœíˆ ê¸°ìˆ ì˜ ì§„ë³´ê°€ ì•„ë‹ˆë¼ **ë¬¸ì œ í•´ê²° ë°©ì‹ì˜ í˜ëª…**ì´ë‹¤.

1. **ìë™í™”ì˜ í™•ëŒ€**: ì‚¬ëŒì˜ ê°œì…ì´ ì ì  ì¤„ì–´ë“¦
2. **ë²”ìš©ì„±ì˜ ì¦ê°€**: í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ìˆ˜í–‰
3. **ì ‘ê·¼ì„±ì˜ í–¥ìƒ**: ì „ë¬¸ ì§€ì‹ ì—†ì´ë„ AI í™œìš© ê°€ëŠ¥

> ë¯¸ë˜ì˜ AIëŠ” ë” í¬ê³ , ë” ë˜‘ë˜‘í•˜ê³ , ë” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì¤‘ìš”í•œ ê²ƒì€ ì´ëŸ¬í•œ ë„êµ¬ë¥¼ ì–´ë–»ê²Œ í™œìš©í•  ê²ƒì¸ê°€ì´ë‹¤.
{: .prompt-warning}
