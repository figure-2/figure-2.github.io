---
title: "ğŸ¬ ì¶”ì²œ ì‹œìŠ¤í…œ ê¸°ì´ˆ: ëª¨ë¸ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§(Collaborative Filtering)"
date: 2025-09-23 12:10:00 +0900
categories:
  - MACHINE_LEARNING
  - RECOMMENDER_SYSTEM
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
  - GeekAndChill
  - ê¸°ê¹¬ì¹ 
  - ì—ì´ì•„ì´
  - ì—…ìŠ¤í…Œì´ì§€ì—ì´ì•„ì´ë©
  - UpstageAILab
  - UpstageAILab6ê¸°
  - ML
  - DL
  - machinelearning
  - deeplearning
  - ì¶”ì²œì‹œìŠ¤í…œ
  - recommender-system
  - Model-BasedCF
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” Python íŒ¨í‚¤ì§€/ë²„ì „ ì •ë³´

- numpy==1.26.4
- pandas==2.2.3
- scikit-learn==1.6.1
- torch==2.6.0
- surprise (ì¶”ì²œ ì‹œìŠ¤í…œ ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬)
- implicit (ì•”ì‹œì  í”¼ë“œë°± ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬)
- matplotlib==3.10.1
- scipy==1.15.2

## ğŸš€ TL;DR

- **ëª¨ë¸ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§**ì€ ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°©ì‹ì˜ í™•ì¥ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¶”ì²œ ì‹œìŠ¤í…œ ì ‘ê·¼ë²•ì´ë‹¤
- **Matrix Factorization (MF)**ì€ ì‚¬ìš©ì-ì•„ì´í…œ í–‰ë ¬ì„ ì €ì°¨ì› ì ì¬ ìš”ì¸ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ë°œê²¬í•œë‹¤
- **WRMF**ëŠ” ì•”ì‹œì  í”¼ë“œë°±ì—ì„œ preferenceì™€ confidenceë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” í˜ì‹ ì  ë°©ë²•ì´ë‹¤
- **BPR (Bayesian Personalized Ranking)**ì€ ì•”ì‹œì  í”¼ë“œë°±ì—ì„œ ì‚¬ìš©ì ì„ í˜¸ë„ ìˆœìœ„ë¥¼ ì§ì ‘ í•™ìŠµí•œë‹¤
- **SLIM**ê³¼ ê°™ì€ User-free ëª¨ë¸ì€ ìƒˆë¡œìš´ ì‚¬ìš©ìì— ëŒ€í•œ ì½œë“œ ìŠ¤íƒ€íŠ¸ ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•œë‹¤
- ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• ê³¼ Leave-One-Last í‰ê°€ëŠ” ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì„ ë°˜ì˜í•œ í‰ê°€ ë°©ë²•ì´ë‹¤
- ëª…ì‹œì  í”¼ë“œë°±ì—ëŠ” **Surprise**, ì•”ì‹œì  í”¼ë“œë°±ì—ëŠ” **Implicit** ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìµœì í™”ë˜ì–´ ìˆë‹¤

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [ëª¨ë¸ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì‹¤ìŠµ ë…¸íŠ¸ë¶](https://github.com/username/recommender-system-tutorial/blob/main/model_based_cf.ipynb)

## ğŸ”„ í˜‘ì—… í•„í„°ë§ì˜ ì§„í™”: ë©”ëª¨ë¦¬ ê¸°ë°˜ì—ì„œ ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ

ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì—­ì‚¬ë¥¼ ì´í•´í•˜ë©´ ì™œ ëª¨ë¸ ê¸°ë°˜ ë°©ì‹ì´ ë“±ì¥í–ˆëŠ”ì§€ ëª…í™•í•´ì§‘ë‹ˆë‹¤. ì´ˆê¸°ì˜ ë©”ëª¨ë¦¬ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ì€ ì§ê´€ì ì´ì—ˆì§€ë§Œ, ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì¹˜ëª…ì ì¸ í•œê³„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

### ë©”ëª¨ë¦¬ ê¸°ë°˜ CFì˜ ì„¸ ê°€ì§€ ê·¼ë³¸ì  í•œê³„

**1. í™•ì¥ì„±(Scalability) ë¬¸ì œ**

Netflixë‚˜ Amazon ê°™ì€ ì„œë¹„ìŠ¤ë¥¼ ìƒê°í•´ë³´ì„¸ìš”. ìˆ˜ë°±ë§Œ ì‚¬ìš©ìì™€ ìˆ˜ì‹­ë§Œ ì•„ì´í…œì´ ìˆì„ ë•Œ, ê° ì¶”ì²œë§ˆë‹¤ ëª¨ë“  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•œë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”?

```python
# ë©”ëª¨ë¦¬ ê¸°ë°˜ CFì˜ ê³„ì‚° ë³µì¡ë„ ì˜ˆì‹œ
n_users = 1_000_000
n_items = 100_000

# ìœ ì‚¬ë„ ê³„ì‚° ë³µì¡ë„
user_similarity_computations = n_users * (n_users - 1) / 2  # O(nÂ²)
print(f"ìœ ì‚¬ë„ ê³„ì‚° íšŸìˆ˜: {user_similarity_computations:,.0f}")
# ì¶œë ¥: ìœ ì‚¬ë„ ê³„ì‚° íšŸìˆ˜: 499,999,500,000

# ê° ê³„ì‚°ì´ 0.001msë¼ê³  í•´ë„...
time_hours = user_similarity_computations * 0.001 / 1000 / 3600
print(f"ì˜ˆìƒ ì‹œê°„: {time_hours:,.1f} ì‹œê°„")
# ì¶œë ¥: ì˜ˆìƒ ì‹œê°„: 138.9 ì‹œê°„
```

**2. í¬ì†Œì„±(Sparsity) ë¬¸ì œ**

ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš©ìëŠ” ì „ì²´ ì•„ì´í…œì˜ 1% ë¯¸ë§Œë§Œ í‰ê°€í•©ë‹ˆë‹¤. ì´ëŠ” ìœ ì‚¬ë„ ê³„ì‚°ì„ ê±°ì˜ ë¶ˆê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

```python
# ì‹¤ì œ ë°ì´í„°ì˜ í¬ì†Œì„±
total_possible_interactions = n_users * n_items
actual_interactions = n_users * 20  # í‰ê·  20ê°œ ì•„ì´í…œ í‰ê°€
sparsity = 1 - (actual_interactions / total_possible_interactions)
print(f"í¬ì†Œì„±: {sparsity:.4%}")
# ì¶œë ¥: í¬ì†Œì„±: 99.98%
```

**3. íœ´ë¦¬ìŠ¤í‹± ë°©ë²•ì˜ í•œê³„**

ë©”ëª¨ë¦¬ ê¸°ë°˜ CFëŠ” "ë¹„ìŠ·í•œ ì‚¬ëŒì€ ë¹„ìŠ·í•œ ê²ƒì„ ì¢‹ì•„í•œë‹¤"ëŠ” ì§ê´€ì— ì˜ì¡´í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ê²ƒì„ ì–´ë–»ê²Œ ìµœì í™”í• ê¹Œìš”? ëª©ì  í•¨ìˆ˜ê°€ ì—†ë‹¤ëŠ” ê²ƒì€ ê°œì„  ë°©í–¥ì„ ëª¨ë¥¸ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

### ëª¨ë¸ ê¸°ë°˜ CF: íŒ¨ëŸ¬ë‹¤ì„ì˜ ì „í™˜

ëª¨ë¸ ê¸°ë°˜ CFëŠ” ì´ ë¬¸ì œë“¤ì„ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥´ê²Œ ì ‘ê·¼í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "ë©”ëª¨ë¦¬ ê¸°ë°˜ ì ‘ê·¼"
        A[ì›ì‹œ ë°ì´í„°] --> B[ìœ ì‚¬ë„ ê³„ì‚°<br/>O(nÂ²)]
        B --> C[ì˜ˆì¸¡ ì‹œë§ˆë‹¤<br/>ì „ì²´ íƒìƒ‰]
        C --> D[ëŠë¦° ì‘ë‹µ]
    end
    
    subgraph "ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼"
        E[ì›ì‹œ ë°ì´í„°] --> F[ì˜¤í”„ë¼ì¸ í•™ìŠµ<br/>O(kÃ—iterations)]
        F --> G[ì»´íŒ©íŠ¸í•œ ëª¨ë¸<br/>kì°¨ì› ë²¡í„°]
        G --> H[ë¹ ë¥¸ ì˜ˆì¸¡<br/>O(k)]
    end
    
    style D fill:#ffccbc
    style H fill:#c8e6c9
```

ì´ì œ ê° ë°©ë²•ì˜ íŠ¹ì§•ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## ğŸ¯ Matrix Factorization: ì°¨ì› ì¶•ì†Œì˜ ë§ˆë²•

### í–‰ë ¬ ë¶„í•´ì˜ ì§ê´€ì  ì´í•´

ì˜í™” ì¶”ì²œì„ ì˜ˆë¡œ ë“¤ì–´ Matrix Factorizationì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë‹¨ê³„ì ìœ¼ë¡œ ì´í•´í•´ë´…ì‹œë‹¤.

ë¨¼ì € ìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ëŠ” ê±°ëŒ€í•˜ê³  í¬ì†Œí•œ í‰ì  í–‰ë ¬ì…ë‹ˆë‹¤:

```python
import numpy as np
import pandas as pd

# ì‹¤ì œ í‰ì  í–‰ë ¬ ì˜ˆì‹œ (7ëª… ì‚¬ìš©ì, 6ê°œ ì˜í™”)
movies = ['ë„¤ë¡œ', 'ìœ¨ë¦¬ìš°ìŠ¤ ì‹œì €', 'í•´ë¦¬ ë§Œë‚˜ ìƒë¦¬', 'ë…¸íŒ…í', 'íƒ€ì´íƒ€ë‹‰', 'ëŸ¬ë¸Œ ì•¡ì¸„ì–¼ë¦¬']
users = ['ì‚¬ìš©ì1', 'ì‚¬ìš©ì2', 'ì‚¬ìš©ì3', 'ì‚¬ìš©ì4', 'ì‚¬ìš©ì5', 'ì‚¬ìš©ì6', 'ì‚¬ìš©ì7']

R = np.array([
    [5, 4, 0, 1, 0, 0],  # ì‚¬ìš©ì1: ì—­ì‚¬ë¬¼ ì¢‹ì•„í•¨
    [4, 5, 0, 0, 1, 0],  # ì‚¬ìš©ì2: ì—­ì‚¬ë¬¼ ì¢‹ì•„í•¨
    [0, 4, 5, 0, 0, 1],  # ì‚¬ìš©ì3: ì—­ì‚¬ë¬¼ ì¢‹ì•„í•¨
    [3, 3, 0, 3, 3, 0],  # ì‚¬ìš©ì4: ëª¨ë‘ ì¢‹ì•„í•¨
    [1, 0, 4, 5, 0, 0],  # ì‚¬ìš©ì5: ë¡œë§¨ìŠ¤ ì¢‹ì•„í•¨
    [0, 1, 0, 4, 5, 0],  # ì‚¬ìš©ì6: ë¡œë§¨ìŠ¤ ì¢‹ì•„í•¨
    [0, 0, 1, 0, 4, 5],  # ì‚¬ìš©ì7: ë¡œë§¨ìŠ¤ ì¢‹ì•„í•¨
])

df_ratings = pd.DataFrame(R, index=users, columns=movies)
print("ì›ë³¸ í‰ì  í–‰ë ¬:")
print(df_ratings)
```

MFëŠ” ì´ í–‰ë ¬ì„ ë‘ ê°œì˜ ì‘ì€ í–‰ë ¬ë¡œ ë¶„í•´í•©ë‹ˆë‹¤. ë†€ë¼ìš´ ì ì€ **ì¥ë¥´ ì •ë³´ë¥¼ ì „í˜€ ì œê³µí•˜ì§€ ì•Šì•˜ëŠ”ë°ë„** íŒ¨í„´ì„ ë°œê²¬í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤:

```python
# MF ì ìš© í›„ ë°œê²¬ëœ ì ì¬ ìš”ì¸
# k=2ë¡œ ì„¤ì • (2ê°œì˜ ì ì¬ ìš”ì¸)

# ì‚¬ìš©ì í–‰ë ¬ P (7Ã—2)
# ê° í–‰ì€ [ì—­ì‚¬ ì„ í˜¸ë„, ë¡œë§¨ìŠ¤ ì„ í˜¸ë„]ë¥¼ ë‚˜íƒ€ëƒ„
P = np.array([
    [0.9, 0.1],  # ì‚¬ìš©ì1: ì—­ì‚¬ ê°•í•˜ê²Œ ì„ í˜¸
    [0.8, 0.2],  # ì‚¬ìš©ì2: ì—­ì‚¬ ì„ í˜¸
    [0.7, 0.3],  # ì‚¬ìš©ì3: ì—­ì‚¬ ì„ í˜¸
    [0.5, 0.5],  # ì‚¬ìš©ì4: ê· í˜•
    [0.2, 0.8],  # ì‚¬ìš©ì5: ë¡œë§¨ìŠ¤ ì„ í˜¸
    [0.1, 0.9],  # ì‚¬ìš©ì6: ë¡œë§¨ìŠ¤ ê°•í•˜ê²Œ ì„ í˜¸
    [0.0, 1.0],  # ì‚¬ìš©ì7: ë¡œë§¨ìŠ¤ë§Œ ì„ í˜¸
])

# ì•„ì´í…œ í–‰ë ¬ Q (2Ã—6)
# ê° ì—´ì€ ì˜í™”ì˜ [ì—­ì‚¬ ì •ë„, ë¡œë§¨ìŠ¤ ì •ë„]
Q = np.array([
    [5, 4, 3, 1, 0, 0],  # ì—­ì‚¬ íŠ¹ì„±
    [0, 1, 2, 4, 5, 5],  # ë¡œë§¨ìŠ¤ íŠ¹ì„±
])

# ë³µì›ëœ í–‰ë ¬
R_pred = P @ Q
print("\në³µì›ëœ í‰ì  í–‰ë ¬:")
print(pd.DataFrame(R_pred.round(1), index=users, columns=movies))

# ë¹ˆ ì¹¸ì´ ì±„ì›Œì§„ ê²ƒì„ í™•ì¸!
```

### SGD vs ALS: ë‘ ê°€ì§€ í•™ìŠµ ë°©ë²•ì˜ ì°¨ì´

Matrix Factorizationì„ í•™ìŠµí•˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ê°ê°ì˜ ì¥ë‹¨ì ì„ ì´í•´í•˜ë©´ ìƒí™©ì— ë§ëŠ” ì„ íƒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 1. Stochastic Gradient Descent (SGD)

SGDëŠ” ê° ê´€ì¸¡ê°’ì— ëŒ€í•´ ì ì§„ì ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:

```python
def matrix_factorization_sgd(R, k, steps=5000, alpha=0.002, beta=0.02):
    """
    SGDë¥¼ ì´ìš©í•œ Matrix Factorization
    
    ëª©ì  í•¨ìˆ˜: minimize Î£(r_ui - p_uÂ·q_i)Â² + Î»(||p_u||Â² + ||q_i||Â²)
    
    Parameters:
    - R: í‰ì  í–‰ë ¬ (mÃ—n)
    - k: ì ì¬ ìš”ì¸ ìˆ˜
    - steps: í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
    - alpha: í•™ìŠµë¥  (ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦° ìˆ˜ë ´)
    - beta: L2 ì •ê·œí™” ê³„ìˆ˜ (ê³¼ì í•© ë°©ì§€)
    """
    m, n = R.shape
    
    # ëœë¤ ì´ˆê¸°í™” (ì‘ì€ ê°’ìœ¼ë¡œ ì‹œì‘)
    P = np.random.normal(scale=1./k, size=(m, k))
    Q = np.random.normal(scale=1./k, size=(k, n))
    
    # ë°”ì´ì–´ìŠ¤ í•­ (ê¸€ë¡œë²Œ í‰ê· , ì‚¬ìš©ì í¸í–¥, ì•„ì´í…œ í¸í–¥)
    b = np.mean(R[R > 0])  # ì „ì²´ í‰ê· 
    b_u = np.zeros(m)      # ì‚¬ìš©ì ë°”ì´ì–´ìŠ¤
    b_i = np.zeros(n)      # ì•„ì´í…œ ë°”ì´ì–´ìŠ¤
    
    # í•™ìŠµ ê³¼ì •
    samples = [(i, j, R[i,j]) for i in range(m) for j in range(n) if R[i,j] > 0]
    
    for step in range(steps):
        np.random.shuffle(samples)  # ìˆœì„œ ì„ê¸° (ë” ë‚˜ì€ ìˆ˜ë ´)
        
        for i, j, r in samples:
            # ì˜ˆì¸¡ê°’ ê³„ì‚°
            prediction = b + b_u[i] + b_i[j] + P[i,:] @ Q[:,j]
            
            # ì˜¤ì°¨ ê³„ì‚°
            e = r - prediction
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ì—…ë°ì´íŠ¸
            # âˆ‚Loss/âˆ‚p_u = -2eÂ·q_i + 2Î»p_u
            # âˆ‚Loss/âˆ‚q_i = -2eÂ·p_u + 2Î»q_i
            b_u[i] += alpha * (e - beta * b_u[i])
            b_i[j] += alpha * (e - beta * b_i[j])
            P[i,:] += alpha * (e * Q[:,j] - beta * P[i,:])
            Q[:,j] += alpha * (e * P[i,:] - beta * Q[:,j])
            
        # ìˆ˜ë ´ í™•ì¸ (ì„ íƒì )
        if step % 100 == 0:
            loss = 0
            for i, j, r in samples:
                pred = b + b_u[i] + b_i[j] + P[i,:] @ Q[:,j]
                loss += (r - pred)**2
            print(f"Step {step}, Loss: {loss:.4f}")
    
    return P, Q, b, b_u, b_i
```

#### 2. Alternating Least Squares (ALS)

ALSëŠ” í•œ ë²ˆì— í•œ í–‰ë ¬ì„ ê³ ì •í•˜ê³  ë‹¤ë¥¸ í–‰ë ¬ì„ ìµœì í™”í•©ë‹ˆë‹¤:

```python
def matrix_factorization_als(R, k, iterations=10, lambda_reg=0.01):
    """
    ALSë¥¼ ì´ìš©í•œ Matrix Factorization
    
    í•µì‹¬ ì•„ì´ë””ì–´: Pë¥¼ ê³ ì •í•˜ë©´ Q ìµœì í™”ëŠ” least squares ë¬¸ì œ
                  Që¥¼ ê³ ì •í•˜ë©´ P ìµœì í™”ëŠ” least squares ë¬¸ì œ
    
    ì¥ì :
    - Closed-form í•´ê°€ ì¡´ì¬ (ë¹ ë¥¸ ìˆ˜ë ´)
    - í¬ì†Œ ë°ì´í„°ì— ê°•ê±´
    - ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
    """
    m, n = R.shape
    
    # ì´ˆê¸°í™”
    P = np.random.normal(size=(m, k))
    Q = np.random.normal(size=(k, n))
    
    # ê´€ì¸¡ëœ ì¸ë±ìŠ¤
    R_indices = [(i, j) for i in range(m) for j in range(n) if R[i,j] > 0]
    
    for iteration in range(iterations):
        # Step 1: P ê³ ì •, Q ìµœì í™”
        # ê° ì•„ì´í…œ jì— ëŒ€í•´: q_j = (P_j^T P_j + Î»I)^(-1) P_j^T r_j
        for j in range(n):
            # jë²ˆì§¸ ì•„ì´í…œì„ í‰ê°€í•œ ì‚¬ìš©ìë“¤
            users_j = [i for i in range(m) if R[i,j] > 0]
            if not users_j:
                continue
                
            P_j = P[users_j, :]  # í•´ë‹¹ ì‚¬ìš©ìë“¤ì˜ ë²¡í„°
            r_j = R[users_j, j]  # í•´ë‹¹ í‰ì ë“¤
            
            # Closed-form í•´
            A = P_j.T @ P_j + lambda_reg * np.eye(k)
            b = P_j.T @ r_j
            Q[:, j] = np.linalg.solve(A, b)
        
        # Step 2: Q ê³ ì •, P ìµœì í™”
        # ê° ì‚¬ìš©ì iì— ëŒ€í•´: p_i = (Q_i Q_i^T + Î»I)^(-1) Q_i r_i^T
        for i in range(m):
            # ië²ˆì§¸ ì‚¬ìš©ìê°€ í‰ê°€í•œ ì•„ì´í…œë“¤
            items_i = [j for j in range(n) if R[i,j] > 0]
            if not items_i:
                continue
                
            Q_i = Q[:, items_i]  # í•´ë‹¹ ì•„ì´í…œë“¤ì˜ ë²¡í„°
            r_i = R[i, items_i]  # í•´ë‹¹ í‰ì ë“¤
            
            # Closed-form í•´
            A = Q_i @ Q_i.T + lambda_reg * np.eye(k)
            b = Q_i @ r_i
            P[i, :] = np.linalg.solve(A, b)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = 0
        for i, j in R_indices:
            loss += (R[i,j] - P[i,:] @ Q[:,j])**2
        loss += lambda_reg * (np.sum(P**2) + np.sum(Q**2))
        print(f"Iteration {iteration}, Loss: {loss:.4f}")
    
    return P, Q
```

ë‘ ë°©ë²•ì˜ ë¹„êµ:

|íŠ¹ì„±|SGD|ALS|
|---|---|---|
|ìˆ˜ë ´ ì†ë„|ëŠë¦¼|ë¹ ë¦„|
|ë©”ëª¨ë¦¬ ì‚¬ìš©|ì ìŒ|ë§ìŒ|
|ë³‘ë ¬í™”|ì–´ë ¤ì›€|ì‰¬ì›€|
|í¬ì†Œ ë°ì´í„°|ë³´í†µ|ê°•ê±´|
|ëŒ€ê·œëª¨ ë°ì´í„°|ì í•©|ë©”ëª¨ë¦¬ ì œí•œ|

## ğŸ¯ WRMF: ì•”ì‹œì  í”¼ë“œë°±ì˜ ì •êµí•œ ì²˜ë¦¬

ì•”ì‹œì  í”¼ë“œë°± ë°ì´í„°ëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. WRMF(Weighted Regularized Matrix Factorization)ëŠ” ì´ë¥¼ ìœ„í•œ í˜ì‹ ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.

### Preference vs Confidence: í•µì‹¬ ê°œë…ì˜ ë¶„ë¦¬

WRMFì˜ í•µì‹¬ í†µì°°ì€ **"ì‚¬ìš©ìê°€ ì•„ì´í…œì„ ì¢‹ì•„í•˜ëŠ”ì§€"(preference)ì™€ "ê·¸ íŒë‹¨ì„ ì–¼ë§ˆë‚˜ í™•ì‹ í•˜ëŠ”ì§€"(confidence)ë¥¼ ë¶„ë¦¬**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

```python
def compute_preference_and_confidence(R, alpha=40, epsilon=1e-8):
    """
    WRMFì˜ í•µì‹¬: Preferenceì™€ Confidence ë¶„ë¦¬
    
    Preference p_ui:
    - 1 if r_ui > 0 (ìƒí˜¸ì‘ìš© ìˆìŒ = ì„ í˜¸)
    - 0 if r_ui = 0 (ìƒí˜¸ì‘ìš© ì—†ìŒ = ë¹„ì„ í˜¸...ì¼ìˆ˜ë„?)
    
    Confidence c_ui:
    - ìƒí˜¸ì‘ìš©ì´ ë§ì„ìˆ˜ë¡ ë” í™•ì‹ 
    - c_ui = 1 + alpha * r_ui
    
    Parameters:
    - R: ìƒí˜¸ì‘ìš© í–‰ë ¬ (êµ¬ë§¤ íšŸìˆ˜, ì‹œì²­ ì‹œê°„ ë“±)
    - alpha: confidence ì¦ê°€ìœ¨
    """
    # Preference: ì´ì§„ ê°’
    P = (R > 0).astype(float)
    
    # Confidence: ê°€ì¤‘ì¹˜
    # ë°©ë²• 1: ì„ í˜• ì¦ê°€
    C = 1 + alpha * R
    
    # ë°©ë²• 2: ë¡œê·¸ ìŠ¤ì¼€ì¼ (ëŒ€ì•ˆ)
    # C = 1 + alpha * np.log(1 + R/epsilon)
    
    return P, C

# ì˜ˆì‹œ
R_implicit = np.array([
    [0, 5, 0, 1, 0],  # ì‚¬ìš©ì1: ì•„ì´í…œ2ë¥¼ 5ë²ˆ, ì•„ì´í…œ4ë¥¼ 1ë²ˆ êµ¬ë§¤
    [3, 0, 0, 0, 2],  # ì‚¬ìš©ì2: ì•„ì´í…œ1ì„ 3ë²ˆ, ì•„ì´í…œ5ë¥¼ 2ë²ˆ êµ¬ë§¤
    [0, 0, 0, 4, 0],  # ì‚¬ìš©ì3: ì•„ì´í…œ4ë¥¼ 4ë²ˆ êµ¬ë§¤
])

P, C = compute_preference_and_confidence(R_implicit, alpha=40)

print("Preference í–‰ë ¬ (ì„ í˜¸ ì—¬ë¶€):")
print(P)
print("\nConfidence í–‰ë ¬ (í™•ì‹ ë„):")
print(C)
```

### WRMFì˜ ëª©ì  í•¨ìˆ˜

WRMFëŠ” confidenceë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ì—¬ ìµœì í™”í•©ë‹ˆë‹¤:

```python
def wrmf_loss(P, C, X, Y, lambda_reg=0.01):
    """
    WRMF ëª©ì  í•¨ìˆ˜
    
    minimize: Î£ c_ui (p_ui - x_u^T y_i)Â² + Î»(Î£ ||x_u||Â² + Î£ ||y_i||Â²)
    
    ì—¬ê¸°ì„œ:
    - c_uiê°€ ë†’ì€ í•­ëª©(ìì£¼ êµ¬ë§¤í•œ ì•„ì´í…œ)ì— ë” í° ê°€ì¤‘ì¹˜
    - c_uiê°€ ë‚®ì€ í•­ëª©(ë¯¸ê´€ì¸¡)ì—ë„ ì‘ì€ ê°€ì¤‘ì¹˜ (0ì´ ì•„ë‹˜!)
    """
    m, n = P.shape
    loss = 0
    
    # ê°€ì¤‘ ì œê³± ì˜¤ì°¨
    for u in range(m):
        for i in range(n):
            prediction = X[u] @ Y[i]
            loss += C[u, i] * (P[u, i] - prediction) ** 2
    
    # L2 ì •ê·œí™”
    loss += lambda_reg * (np.sum(X**2) + np.sum(Y**2))
    
    return loss
```

### WRMFì˜ ì‹¤ì œ êµ¬í˜„

```python
class WRMF:
    """Weighted Regularized Matrix Factorization"""
    
    def __init__(self, n_factors=100, alpha=40, lambda_reg=0.01, iterations=15):
        self.n_factors = n_factors
        self.alpha = alpha
        self.lambda_reg = lambda_reg
        self.iterations = iterations
    
    def fit(self, R):
        """
        ALSë¥¼ ì‚¬ìš©í•œ WRMF í•™ìŠµ
        """
        m, n = R.shape
        
        # Preferenceì™€ Confidence ê³„ì‚°
        self.P = (R > 0).astype(float)
        self.C = 1 + self.alpha * R
        
        # ì´ˆê¸°í™”
        self.X = np.random.normal(size=(m, self.n_factors)) * 0.01
        self.Y = np.random.normal(size=(n, self.n_factors)) * 0.01
        
        # ALS ë°˜ë³µ
        for iteration in range(self.iterations):
            # ì‚¬ìš©ì ë²¡í„° ì—…ë°ì´íŠ¸
            for u in range(m):
                # C^u: uë²ˆì§¸ ì‚¬ìš©ìì˜ confidence ëŒ€ê° í–‰ë ¬
                Cu = np.diag(self.C[u])
                
                # x_u = (Y^T C^u Y + Î»I)^(-1) Y^T C^u p_u
                YT_Cu_Y = self.Y.T @ Cu @ self.Y
                YT_Cu_pu = self.Y.T @ Cu @ self.P[u]
                
                self.X[u] = np.linalg.solve(
                    YT_Cu_Y + self.lambda_reg * np.eye(self.n_factors),
                    YT_Cu_pu
                )
            
            # ì•„ì´í…œ ë²¡í„° ì—…ë°ì´íŠ¸
            for i in range(n):
                # C^i: ië²ˆì§¸ ì•„ì´í…œì˜ confidence ëŒ€ê° í–‰ë ¬
                Ci = np.diag(self.C[:, i])
                
                # y_i = (X^T C^i X + Î»I)^(-1) X^T C^i p_i
                XT_Ci_X = self.X.T @ Ci @ self.X
                XT_Ci_pi = self.X.T @ Ci @ self.P[:, i]
                
                self.Y[i] = np.linalg.solve(
                    XT_Ci_X + self.lambda_reg * np.eye(self.n_factors),
                    XT_Ci_pi
                )
            
            # ì†ì‹¤ ê³„ì‚°
            loss = self._compute_loss()
            print(f"Iteration {iteration}: loss = {loss:.4f}")
    
    def predict(self, user_idx, n_items=10):
        """Top-N ì¶”ì²œ"""
        scores = self.X[user_idx] @ self.Y.T
        top_items = np.argsort(scores)[::-1][:n_items]
        return top_items, scores[top_items]
```

## ğŸ² BPR: ìˆœìœ„ í•™ìŠµì˜ í˜ëª…

### ì•”ì‹œì  í”¼ë“œë°±ì˜ ê·¼ë³¸ì  ë¬¸ì œ

WRMFê°€ confidenceë¥¼ ë„ì…í–ˆì§€ë§Œ, ì—¬ì „íˆ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. **ê´€ì¸¡ë˜ì§€ ì•Šì€ í•­ëª©ì„ ëª¨ë‘ 0(ë¶€ì •ì )ìœ¼ë¡œ ì²˜ë¦¬**í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. BPRì€ ì´ë¥¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "WRMFì˜ ê´€ì "
        A[ìƒí˜¸ì‘ìš© ìˆìŒ] --> B[ê¸ì •: 1]
        C[ìƒí˜¸ì‘ìš© ì—†ìŒ] --> D[ë¶€ì •: 0]
        D --> E[ë¬¸ì œ: ì•„ì§ ëª°ë¼ì„œ<br/>ì•ˆ ë³¸ ê²ƒë„ 0]
    end
    
    subgraph "BPRì˜ ê´€ì "
        F[ìƒí˜¸ì‘ìš© ìˆìŒ] --> G[í™•ì‹¤íˆ ì„ í˜¸]
        H[ìƒí˜¸ì‘ìš© ì—†ìŒ] --> I[ë¶ˆí™•ì‹¤]
        G --> J[ìƒëŒ€ì  ë¹„êµ:<br/>ë³¸ ê²ƒ > ì•ˆ ë³¸ ê²ƒ]
    end
    
    style E fill:#ffccbc
    style J fill:#c8e6c9
```

### BPRì˜ í•µì‹¬: Pairwise Ranking

BPRì€ ì ˆëŒ€ì  ì ìˆ˜ê°€ ì•„ë‹Œ **ìƒëŒ€ì  ìˆœìœ„**ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤:

```python
class BPR:
    """Bayesian Personalized Ranking"""
    
    def __init__(self, n_factors=100, learning_rate=0.01, reg=0.01, n_epochs=10):
        self.n_factors = n_factors
        self.lr = learning_rate
        self.reg = reg
        self.n_epochs = n_epochs
    
    def fit(self, interactions):
        """
        BPR í•™ìŠµ
        
        interactions: (user, item) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        """
        # ì‚¬ìš©ìë³„ ìƒí˜¸ì‘ìš© ì•„ì´í…œ ì €ì¥
        self.user_items = {}
        all_items = set()
        
        for user, item in interactions:
            if user not in self.user_items:
                self.user_items[user] = set()
            self.user_items[user].add(item)
            all_items.add(item)
        
        self.all_items = list(all_items)
        n_users = len(self.user_items)
        n_items = len(all_items)
        
        # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        self.W = np.random.normal(size=(n_users, self.n_factors)) * 0.01
        self.H = np.random.normal(size=(n_items, self.n_factors)) * 0.01
        
        # í•™ìŠµ
        for epoch in range(self.n_epochs):
            loss = 0
            n_samples = 0
            
            # ê° ì‚¬ìš©ìì— ëŒ€í•´
            for u, items_u in self.user_items.items():
                # ê¸ì • ì•„ì´í…œ ìƒ˜í”Œë§
                for i in items_u:
                    # ë¶€ì • ì•„ì´í…œ ìƒ˜í”Œë§ (ì‚¬ìš©ìê°€ ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šì€ ì•„ì´í…œ)
                    j = self._sample_negative_item(u)
                    
                    # ì„ í˜¸ë„ ì°¨ì´ ê³„ì‚°
                    x_ui = self.W[u] @ self.H[i]
                    x_uj = self.W[u] @ self.H[j]
                    x_uij = x_ui - x_uj
                    
                    # Sigmoid í•¨ìˆ˜
                    sigmoid = 1 / (1 + np.exp(-x_uij))
                    
                    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° ì—…ë°ì´íŠ¸
                    # âˆ‚L/âˆ‚Î¸ = -(1-Ïƒ(x_uij)) * âˆ‚x_uij/âˆ‚Î¸
                    grad_multiplier = 1 - sigmoid
                    
                    # ì‚¬ìš©ì ë²¡í„° ì—…ë°ì´íŠ¸
                    self.W[u] += self.lr * (
                        grad_multiplier * (self.H[i] - self.H[j]) 
                        - self.reg * self.W[u]
                    )
                    
                    # ì•„ì´í…œ ë²¡í„° ì—…ë°ì´íŠ¸
                    self.H[i] += self.lr * (
                        grad_multiplier * self.W[u] 
                        - self.reg * self.H[i]
                    )
                    self.H[j] += self.lr * (
                        -grad_multiplier * self.W[u] 
                        - self.reg * self.H[j]
                    )
                    
                    # ì†ì‹¤ ëˆ„ì 
                    loss += -np.log(sigmoid) + self.reg * (
                        np.sum(self.W[u]**2) + 
                        np.sum(self.H[i]**2) + 
                        np.sum(self.H[j]**2)
                    )
                    n_samples += 1
            
            avg_loss = loss / n_samples if n_samples > 0 else 0
            print(f"Epoch {epoch}: loss = {avg_loss:.4f}")
    
    def _sample_negative_item(self, user):
        """ì‚¬ìš©ìê°€ ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šì€ ì•„ì´í…œ ìƒ˜í”Œë§"""
        user_items = self.user_items[user]
        while True:
            j = np.random.choice(self.all_items)
            if j not in user_items:
                return j
    
    def predict(self, user, n_items=10):
        """Top-N ì¶”ì²œ"""
        scores = self.W[user] @ self.H.T
        
        # ì´ë¯¸ ë³¸ ì•„ì´í…œ ì œì™¸
        seen_items = self.user_items.get(user, set())
        scores_with_idx = [(score, idx) for idx, score in enumerate(scores) 
                          if idx not in seen_items]
        scores_with_idx.sort(reverse=True)
        
        top_items = [idx for score, idx in scores_with_idx[:n_items]]
        top_scores = [score for score, idx in scores_with_idx[:n_items]]
        
        return top_items, top_scores
```

### Negative Sampling ì „ëµ

BPRì˜ ì„±ëŠ¥ì€ ë¶€ì • ìƒ˜í”Œë§ ì „ëµì— í¬ê²Œ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤:

```python
def advanced_negative_sampling(user_items, all_items, strategy='uniform'):
    """
    ë‹¤ì–‘í•œ ë¶€ì • ìƒ˜í”Œë§ ì „ëµ
    
    strategy:
    - 'uniform': ê· ë“± ìƒ˜í”Œë§ (ê¸°ë³¸)
    - 'popularity': ì¸ê¸°ë„ ê¸°ë°˜ (ì¸ê¸° ìˆëŠ” ì•„ì´í…œì¼ìˆ˜ë¡ ìì£¼ ìƒ˜í”Œë§)
    - 'hard': ì–´ë ¤ìš´ ë¶€ì • ìƒ˜í”Œ (ì ìˆ˜ê°€ ë†’ì€ ë¯¸ê´€ì¸¡ ì•„ì´í…œ)
    """
    if strategy == 'uniform':
        # ê· ë“± ìƒ˜í”Œë§
        negative_items = [i for i in all_items if i not in user_items]
        return np.random.choice(negative_items)
    
    elif strategy == 'popularity':
        # ì¸ê¸°ë„ ê¸°ë°˜ ìƒ˜í”Œë§
        item_popularity = compute_item_popularity()  # ì‚¬ì „ ê³„ì‚°ëœ ì¸ê¸°ë„
        negative_items = [i for i in all_items if i not in user_items]
        probs = [item_popularity[i] for i in negative_items]
        probs = np.array(probs) / np.sum(probs)
        return np.random.choice(negative_items, p=probs)
    
    elif strategy == 'hard':
        # ì–´ë ¤ìš´ ë¶€ì • ìƒ˜í”Œ (í˜„ì¬ ëª¨ë¸ì´ ë†’ì€ ì ìˆ˜ë¥¼ ì£¼ëŠ” ë¯¸ê´€ì¸¡ ì•„ì´í…œ)
        # í•™ìŠµì„ ë” íš¨ê³¼ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ
        negative_items = [i for i in all_items if i not in user_items]
        scores = [model.score(user, i) for i in negative_items]
        # ìƒìœ„ 20% ì¤‘ì—ì„œ ìƒ˜í”Œë§
        top_indices = np.argsort(scores)[-len(scores)//5:]
        return negative_items[np.random.choice(top_indices)]
```

## ğŸš« User-free ëª¨ë¸: ì‹¤ì‹œê°„ ì¶”ì²œì˜ í•´ê²°ì±…

### Cold Start Problemì˜ ë³¸ì§ˆ

í˜‘ì—… í•„í„°ë§ì˜ Cold Start ë¬¸ì œë¥¼ CV(Computer Vision)ì™€ ë¹„êµí•˜ë©´ ê·¸ ë³¸ì§ˆì´ ëª…í™•í•´ì§‘ë‹ˆë‹¤:

```python
# ì´ë¯¸ì§€ ë¶„ë¥˜ vs ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì°¨ì´

# ì´ë¯¸ì§€ ë¶„ë¥˜: Universal Features
class ImageClassifier:
    def predict(self, image):
        # í”½ì…€ì€ universal - ì–´ë–¤ ì´ë¯¸ì§€ë“  ê°™ì€ í˜•íƒœ
        pixels = image.reshape(-1)  # [R, G, B, R, G, B, ...]
        features = self.extract_features(pixels)
        return self.classifier(features)

# ì¶”ì²œ ì‹œìŠ¤í…œ: Non-universal Features  
class RecommenderSystem:
    def predict(self, user_id):
        # user_idëŠ” non-universal - ìƒˆ ì‚¬ìš©ìëŠ” ì²˜ë¦¬ ë¶ˆê°€!
        if user_id not in self.user_embeddings:
            raise KeyError("Unknown user - need retraining!")
        user_embedding = self.user_embeddings[user_id]
        return self.compute_recommendations(user_embedding)
```

ì´ ê·¼ë³¸ì ì¸ ì°¨ì´ê°€ Cold Start ë¬¸ì œë¥¼ ë§Œë“­ë‹ˆë‹¤. User-free ëª¨ë¸ì€ ì´ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

### SLIM: í•™ìŠµëœ ì•„ì´í…œ ìœ ì‚¬ë„

SLIM(Sparse LInear Method)ì€ ì•„ì´í…œ ê°„ ìœ ì‚¬ë„ë¥¼ **í•™ìŠµ**í•©ë‹ˆë‹¤:

```python
class SLIM:
    """
    Sparse Linear Method for Top-N Recommendations
    
    í•µì‹¬: ë©”ëª¨ë¦¬ ê¸°ë°˜ CFì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ìœ ì‚¬ë„ë¥¼ í•™ìŠµ!
    """
    
    def __init__(self, l1_reg=0.001, l2_reg=0.0001):
        self.l1_reg = l1_reg
        self.l2_reg = l2_reg
    
    def fit(self, R):
        """
        ëª©ì  í•¨ìˆ˜: minimize ||R - RW||Â² + Î»â‚||W||â‚ + Î»â‚‚||W||Â²
        
        ì œì•½ì¡°ê±´:
        1. W â‰¥ 0 (non-negativity)
        2. diag(W) = 0 (ìê¸° ìì‹  ì‚¬ìš© ê¸ˆì§€)
        
        í•´ì„:
        - W[i,j]: ì•„ì´í…œ jê°€ ì•„ì´í…œ i ì˜ˆì¸¡ì— ê¸°ì—¬í•˜ëŠ” ì •ë„
        - L1 ì •ê·œí™”: í¬ì†Œì„± (ëŒ€ë¶€ë¶„ ì•„ì´í…œê³¼ ë¬´ê´€)
        - L2 ì •ê·œí™”: ê³¼ì í•© ë°©ì§€
        """
        n_items = R.shape[1]
        self.W = np.zeros((n_items, n_items))
        
        # ê° ì•„ì´í…œì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ìµœì í™”
        for j in range(n_items):
            # jë²ˆì§¸ ì•„ì´í…œ ì˜ˆì¸¡ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ í•™ìŠµ
            
            # ëª©í‘œ: r_j â‰ˆ R @ w_j (ìê¸° ìì‹  ì œì™¸)
            target = R[:, j].copy()
            
            # Elastic Net íšŒê·€ (L1 + L2 ì •ê·œí™”)
            from sklearn.linear_model import ElasticNet
            
            # ìê¸° ìì‹  ì œì™¸í•œ ë‹¤ë¥¸ ì•„ì´í…œë“¤
            X = np.delete(R, j, axis=1)
            
            # Elastic Net í•™ìŠµ
            model = ElasticNet(
                alpha=self.l1_reg + self.l2_reg,
                l1_ratio=self.l1_reg / (self.l1_reg + self.l2_reg),
                positive=True,  # non-negativity ì œì•½
                max_iter=1000
            )
            
            model.fit(X, target)
            
            # ê°€ì¤‘ì¹˜ ì €ì¥ (ëŒ€ê°ì„  ì œì™¸)
            w = model.coef_
            self.W[:j, j] = w[:j]
            self.W[j+1:, j] = w[j:]
        
        # í¬ì†Œì„± í™•ì¸
        sparsity = np.mean(self.W == 0)
        print(f"í•™ìŠµëœ Wì˜ í¬ì†Œì„±: {sparsity:.2%}")
    
    def predict(self, user_vector, n_items=10):
        """
        ìƒˆë¡œìš´ ì‚¬ìš©ìë„ ì¦‰ì‹œ ì¶”ì²œ ê°€ëŠ¥!
        
        user_vector: ì‚¬ìš©ìì˜ ì•„ì´í…œ í‰ì /ìƒí˜¸ì‘ìš© ë²¡í„°
        """
        # ë‹¨ìˆœ í–‰ë ¬ ê³±ì…ˆìœ¼ë¡œ ì˜ˆì¸¡
        scores = user_vector @ self.W
        
        # ì´ë¯¸ ë³¸ ì•„ì´í…œ ì œì™¸
        seen_items = np.where(user_vector > 0)[0]
        scores[seen_items] = -np.inf
        
        # Top-N ì„ íƒ
        top_items = np.argsort(scores)[::-1][:n_items]
        return top_items, scores[top_items]
```

SLIMì˜ ì¥ì :

- **ì¦‰ê°ì ì¸ ì¶”ì²œ**: ìƒˆ ì‚¬ìš©ìë„ ì¬í•™ìŠµ ì—†ì´ ì¶”ì²œ
- **Long-tail ê°•ì **: ë¹„ì¸ê¸° ì•„ì´í…œë„ ì˜ ì¶”ì²œ
- **í•´ì„ ê°€ëŠ¥ì„±**: W í–‰ë ¬ì´ ì•„ì´í…œ ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ„

### ë‹¤ë¥¸ User-free ì ‘ê·¼ë²•ë“¤

```python
# 1. Item2Vec: Word2Vecì„ ì¶”ì²œì— ì ìš©
class Item2Vec:
    """
    ì„¸ì…˜/ì‹œí€€ìŠ¤ë¥¼ ë¬¸ì¥ìœ¼ë¡œ, ì•„ì´í…œì„ ë‹¨ì–´ë¡œ ì·¨ê¸‰
    """
    def train(self, sessions):
        from gensim.models import Word2Vec
        
        # ê° ì„¸ì…˜ì„ "ë¬¸ì¥"ìœ¼ë¡œ ì·¨ê¸‰
        self.model = Word2Vec(
            sentences=sessions,
            vector_size=100,
            window=5,
            min_count=1,
            sg=1  # Skip-gram
        )
        
        # ì•„ì´í…œ ì„ë² ë”© ì €ì¥
        self.item_embeddings = {
            item: self.model.wv[item] 
            for item in self.model.wv.index_to_key
        }
    
    def recommend(self, session, n_items=10):
        # ì„¸ì…˜ì˜ ì•„ì´í…œë“¤ì˜ í‰ê·  ì„ë² ë”©
        session_embedding = np.mean([
            self.item_embeddings[item] 
            for item in session if item in self.item_embeddings
        ], axis=0)
        
        # ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œ ì°¾ê¸°
        similarities = {}
        for item, embedding in self.item_embeddings.items():
            if item not in session:
                similarities[item] = np.dot(session_embedding, embedding)
        
        top_items = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
        return [item for item, _ in top_items[:n_items]]


# 2. AutoRec: Autoencoder ê¸°ë°˜
class UserAutoRec:
    """
    ì‚¬ìš©ì ë²¡í„°ë¥¼ ì¬êµ¬ì„±í•˜ëŠ” Autoencoder
    Î³_uë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ!
    """
    def __init__(self, n_items, hidden_size=200):
        self.n_items = n_items
        self.hidden_size = hidden_size
        
        # Encoderì™€ Decoder
        self.encoder = self._build_encoder()
        self.decoder = self._build_decoder()
    
    def _build_encoder(self):
        import torch.nn as nn
        return nn.Sequential(
            nn.Linear(self.n_items, self.hidden_size),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
    
    def _build_decoder(self):
        import torch.nn as nn
        return nn.Sequential(
            nn.Linear(self.hidden_size, self.n_items),
            nn.Sigmoid()
        )
    
    def forward(self, user_vector):
        # ì‚¬ìš©ì ë²¡í„°ë¥¼ ì••ì¶•í–ˆë‹¤ê°€ ë³µì›
        hidden = self.encoder(user_vector)
        reconstructed = self.decoder(hidden)
        return reconstructed
    
    def recommend(self, user_vector, n_items=10):
        # ìƒˆë¡œìš´ ì‚¬ìš©ìë„ ì²˜ë¦¬ ê°€ëŠ¥!
        reconstructed = self.forward(user_vector)
        
        # ì´ë¯¸ ë³¸ ì•„ì´í…œ ì œì™¸í•˜ê³  Top-N
        seen_items = user_vector > 0
        reconstructed[seen_items] = -float('inf')
        
        top_items = torch.argsort(reconstructed, descending=True)[:n_items]
        return top_items
```

## ğŸ“Š í‰ê°€: ì¶”ì²œ í’ˆì§ˆì˜ ì •í™•í•œ ì¸¡ì •

### ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• : í˜„ì‹¤ì ì¸ í‰ê°€

ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ê³¼ê±° ë°ì´í„°ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤. ëœë¤ ë¶„í• ì€ ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤:

```python
def temporal_train_test_split(df, split_date='2011-10'):
    """
    ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• 
    
    ì™œ ì¤‘ìš”í•œê°€?
    - ì‹¤ì œ ì„œë¹„ìŠ¤: ê³¼ê±°ë¡œ ë¯¸ë˜ ì˜ˆì¸¡
    - ëœë¤ ë¶„í• : ë¯¸ë˜ ì •ë³´ ìœ ì¶œ (data leakage)
    """
    # ì—°ì›” ì •ë³´ ì¶”ì¶œ
    df['year_month'] = df['InvoiceDate'].dt.strftime('%Y-%m')
    
    # ì‹œê°„ ê¸°ì¤€ ë¶„í• 
    train = df[df['year_month'] <= split_date]
    test = df[df['year_month'] > split_date]
    
    # ë¶„í•  í†µê³„
    print(f"Train: {train['year_month'].min()} ~ {train['year_month'].max()}")
    print(f"Test: {test['year_month'].min()} ~ {test['year_month'].max()}")
    print(f"Train size: {len(train):,} ({len(train)/len(df):.1%})")
    print(f"Test size: {len(test):,} ({len(test)/len(df):.1%})")
    
    # ì‹œê°í™”
    import matplotlib.pyplot as plt
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Train ë¶„í¬
    train.groupby('year_month').size().plot(kind='bar', ax=ax1, color='blue', alpha=0.7)
    ax1.set_title('Train Set Distribution')
    ax1.set_xlabel('Month')
    ax1.set_ylabel('Number of Transactions')
    
    # Test ë¶„í¬
    test.groupby('year_month').size().plot(kind='bar', ax=ax2, color='red', alpha=0.7)
    ax2.set_title('Test Set Distribution')
    ax2.set_xlabel('Month')
    ax2.set_ylabel('Number of Transactions')
    
    plt.tight_layout()
    plt.show()
    
    return train, test

# UCI Online Retail ë°ì´í„°ì…‹ ì˜ˆì‹œ
train_df, test_df = temporal_train_test_split(retail_df, '2011-10')
```

### Leave-One-Last í‰ê°€: ê°€ì¥ ìµœê·¼ ìƒí˜¸ì‘ìš© ì˜ˆì¸¡

ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì‚¬ìš©ìì˜ ë‹¤ìŒ í–‰ë™ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤:

```python
def leave_one_last_evaluation(test_ratings):
    """
    Leave-One-Last: ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš©ë§Œ í…ŒìŠ¤íŠ¸
    
    ì¥ì :
    - ì‹¤ì œ ìƒí™© ë°˜ì˜ (ë‹¤ìŒ ì•„ì´í…œ ì˜ˆì¸¡)
    - ì‹œê°„ ìˆœì„œ ë³´ì¡´
    - í‰ê°€ íš¨ìœ¨ì„±
    """
    # ì‹œê°„ìˆœ ì •ë ¬
    test_ratings = test_ratings.sort_values(['user_id', 'timestamp'])
    
    # ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì•„ì´í…œë§Œ ì„ íƒ
    test_last = test_ratings.groupby('user_id').tail(1)
    
    print(f"ì›ë³¸ í…ŒìŠ¤íŠ¸: {len(test_ratings):,} interactions")
    print(f"Leave-One-Last: {len(test_last):,} interactions")
    print(f"ì‚¬ìš©ìë‹¹ í‰ê·  1ê°œì”© í‰ê°€")
    
    return test_last

# ì˜ˆì‹œ
test_last = leave_one_last_evaluation(test_ratings)
```

### Stratified Sampling: ê· í˜•ì¡íŒ í‰ê°€

ì‚¬ìš©ìë³„ í™œë™ëŸ‰ì´ ë‹¤ë¥¼ ë•Œ, ê· í˜•ì¡íŒ í‰ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤:

```python
from sklearn.model_selection import train_test_split

def stratified_split(ratings, test_size=0.2):
    """
    ê³„ì¸µí™” ìƒ˜í”Œë§: ê° ì‚¬ìš©ìì˜ í™œë™ ë¹„ìœ¨ ìœ ì§€
    
    ì™œ í•„ìš”í•œê°€?
    - Heavy userì™€ Light userì˜ ê· í˜•
    - ê° ì‚¬ìš©ìë³„ë¡œ ì¼ì • ë¹„ìœ¨ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ë³´
    """
    train_list = []
    test_list = []
    
    for user_id, user_data in ratings.groupby('user_id'):
        if len(user_data) >= 5:  # ìµœì†Œ 5ê°œ ì´ìƒ í‰ê°€í•œ ì‚¬ìš©ìë§Œ
            # ê° ì‚¬ìš©ìë³„ë¡œ test_size ë¹„ìœ¨ ë¶„í• 
            user_train, user_test = train_test_split(
                user_data,
                test_size=test_size,
                random_state=42
            )
            train_list.append(user_train)
            test_list.append(user_test)
        else:
            # í™œë™ì´ ì ì€ ì‚¬ìš©ìëŠ” ëª¨ë‘ trainì—
            train_list.append(user_data)
    
    train = pd.concat(train_list)
    test = pd.concat(test_list) if test_list else pd.DataFrame()
    
    # ë¶„í•  ê²€ì¦
    print("ì‚¬ìš©ìë³„ í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ ë¶„í¬:")
    for user_id in ratings['user_id'].unique()[:10]:  # ìƒ˜í”Œ 10ëª…
        user_total = len(ratings[ratings['user_id'] == user_id])
        user_test = len(test[test['user_id'] == user_id]) if len(test) > 0 else 0
        ratio = user_test / user_total if user_total > 0 else 0
        print(f"  User {user_id}: {ratio:.1%}")
    
    return train, test
```

### í‰ê°€ ì§€í‘œ: ìƒí™©ì— ë§ëŠ” ì„ íƒ

```python
class RecommenderEvaluator:
    """ì¶”ì²œ ì‹œìŠ¤í…œ ì¢…í•© í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, k_values=[5, 10, 20]):
        self.k_values = k_values
    
    def evaluate_rating_prediction(self, true_ratings, pred_ratings):
        """ëª…ì‹œì  í”¼ë“œë°± í‰ê°€ (í‰ì  ì˜ˆì¸¡)"""
        from sklearn.metrics import mean_squared_error, mean_absolute_error
        
        rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings))
        mae = mean_absolute_error(true_ratings, pred_ratings)
        
        return {
            'RMSE': rmse,
            'MAE': mae
        }
    
    def evaluate_ranking(self, recommendations, ground_truth, k=10):
        """ì•”ì‹œì  í”¼ë“œë°± í‰ê°€ (ë­í‚¹)"""
        metrics = {}
        
        for user_id in ground_truth:
            if user_id not in recommendations:
                continue
                
            rec_items = recommendations[user_id][:k]
            true_items = ground_truth[user_id]
            
            # Precision@K
            hits = len(set(rec_items) & set(true_items))
            precision = hits / k if k > 0 else 0
            
            # Recall@K  
            recall = hits / len(true_items) if true_items else 0
            
            # NDCG@K
            dcg = sum([
                1 / np.log2(i + 2) 
                for i, item in enumerate(rec_items) 
                if item in true_items
            ])
            idcg = sum([
                1 / np.log2(i + 2) 
                for i in range(min(k, len(true_items)))
            ])
            ndcg = dcg / idcg if idcg > 0 else 0
            
            # MAP@K
            avg_precision = 0
            hits_count = 0
            for i, item in enumerate(rec_items):
                if item in true_items:
                    hits_count += 1
                    precision_at_i = hits_count / (i + 1)
                    avg_precision += precision_at_i
            map_score = avg_precision / min(k, len(true_items)) if true_items else 0
            
            # ì‚¬ìš©ìë³„ ë©”íŠ¸ë¦­ ì €ì¥
            if 'precision' not in metrics:
                metrics['precision'] = []
            metrics['precision'].append(precision)
            metrics['recall'].append(recall)
            metrics['ndcg'].append(ndcg)
            metrics['map'].append(map_score)
        
        # í‰ê·  ê³„ì‚°
        return {
            f'Precision@{k}': np.mean(metrics.get('precision', [0])),
            f'Recall@{k}': np.mean(metrics.get('recall', [0])),
            f'NDCG@{k}': np.mean(metrics.get('ndcg', [0])),
            f'MAP@{k}': np.mean(metrics.get('map', [0]))
        }
    
    def plot_metrics_comparison(self, models_metrics):
        """ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        metrics = ['Precision', 'Recall', 'NDCG', 'MAP']
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx // 2, idx % 2]
            
            for model_name, model_metrics in models_metrics.items():
                k_values = []
                metric_values = []
                
                for k in self.k_values:
                    key = f'{metric}@{k}'
                    if key in model_metrics:
                        k_values.append(k)
                        metric_values.append(model_metrics[key])
                
                ax.plot(k_values, metric_values, marker='o', label=model_name)
            
            ax.set_xlabel('K')
            ax.set_ylabel(metric)
            ax.set_title(f'{metric}@K Comparison')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
```

## ğŸ’» ì‹¤ì „ êµ¬í˜„: ì„¸ ê°€ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì™„ë²½ í™œìš©

### 1. Surprise ë¼ì´ë¸ŒëŸ¬ë¦¬ (ëª…ì‹œì  í”¼ë“œë°±)

SurpriseëŠ” ëª…ì‹œì  í‰ì  ë°ì´í„°ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Jester ë°ì´í„°ì…‹(ë†ë‹´ í‰ì )ì„ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤:

```python
import pandas as pd
from surprise import Dataset, Reader, SVD, NMF, SlopeOne, CoClustering
from surprise.model_selection import cross_validate, GridSearchCV

# Jester ë°ì´í„°ì…‹ ë¡œë“œ (í‰ì : -10 ~ +10)
jester_df = pd.read_csv('jester_ratings.csv')
print(f"ë°ì´í„° í¬ê¸°: {len(jester_df):,} ratings")
print(f"ì‚¬ìš©ì ìˆ˜: {jester_df['user_id'].nunique():,}")
print(f"ì•„ì´í…œ ìˆ˜: {jester_df['joke_id'].nunique():,}")

# Surprise í˜•ì‹ìœ¼ë¡œ ë³€í™˜
reader = Reader(rating_scale=(-10, 10))
data = Dataset.load_from_df(
    jester_df[['user_id', 'joke_id', 'rating']], 
    reader
)

# ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
algorithms = {
    'NormalPredictor': NormalPredictor(),  # ëœë¤ ë² ì´ìŠ¤ë¼ì¸
    'BaselineOnly': BaselineOnly(),        # ë°”ì´ì–´ìŠ¤ë§Œ ì‚¬ìš©
    'SVD': SVD(n_factors=50),             # íŠ¹ì´ê°’ ë¶„í•´
    'SVD++': SVDpp(n_factors=50),         # ì•”ì‹œì  í”¼ë“œë°± ê³ ë ¤
    'NMF': NMF(n_factors=50),              # Non-negative MF
    'SlopeOne': SlopeOne(),                # ê°€ì¤‘ í‰ê· 
    'CoClustering': CoClustering()         # ë™ì‹œ í´ëŸ¬ìŠ¤í„°ë§
}

# 3-fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ
results = {}
for name, algorithm in algorithms.items():
    print(f"\n{name} í‰ê°€ ì¤‘...")
    cv_results = cross_validate(
        algorithm, data, 
        measures=['RMSE', 'MAE'], 
        cv=3, 
        n_jobs=-1,
        verbose=False
    )
    
    results[name] = {
        'RMSE': np.mean(cv_results['test_rmse']),
        'MAE': np.mean(cv_results['test_mae']),
        'Fit_time': np.mean(cv_results['fit_time']),
        'Test_time': np.mean(cv_results['test_time'])
    }

# ê²°ê³¼ ì •ë¦¬
results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('RMSE')
print("\n=== ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ===")
print(results_df)
```

### GridSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

```python
# SVD í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
param_grid = {
    'n_factors': [50, 100, 150],
    'lr_all': [0.002, 0.005, 0.01],
    'reg_all': [0.02, 0.05, 0.1]
}

gs = GridSearchCV(
    SVD, 
    param_grid, 
    measures=['rmse'], 
    cv=3,
    n_jobs=-1
)

gs.fit(data)

# ìµœì  íŒŒë¼ë¯¸í„°
print(f"ìµœì  RMSE: {gs.best_score['rmse']:.4f}")
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {gs.best_params['rmse']}")

# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡
best_model = gs.best_estimator['rmse']
trainset = data.build_full_trainset()
best_model.fit(trainset)

# íŠ¹ì • ì‚¬ìš©ì-ì•„ì´í…œ ì˜ˆì¸¡
user_id = '1'
item_id = '10'
prediction = best_model.predict(user_id, item_id)
print(f"\nì‚¬ìš©ì {user_id}ì˜ ì•„ì´í…œ {item_id} ì˜ˆì¸¡ í‰ì : {prediction.est:.2f}")
```

### 2. Implicit ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì•”ì‹œì  í”¼ë“œë°±)

UCI Online Retail ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì‹¤ì œ êµ¬í˜„:

```python
import implicit
from scipy.sparse import csr_matrix
import pandas as pd

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
retail_df = pd.read_excel('online_retail.xlsx')

# ë°ì´í„° ì •ì œ
retail_df = retail_df[retail_df['CustomerID'].notna()]
retail_df = retail_df[retail_df['Quantity'] > 0]
retail_df['CustomerID'] = retail_df['CustomerID'].astype(int)

# ì‹œê°„ ê¸°ë°˜ ë¶„í• 
retail_df['YearMonth'] = retail_df['InvoiceDate'].dt.strftime('%Y-%m')
train_df = retail_df[retail_df['YearMonth'] <= '2011-10']
test_df = retail_df[retail_df['YearMonth'] > '2011-10']

print(f"Train: {train_df['YearMonth'].min()} ~ {train_df['YearMonth'].max()}")
print(f"Test: {test_df['YearMonth'].min()} ~ {test_df['YearMonth'].max()}")

# êµ¬ë§¤ íšŸìˆ˜ë¥¼ confidenceë¡œ ì‚¬ìš©
interaction_matrix = train_df.groupby(['CustomerID', 'StockCode'])['Quantity'].sum()
interaction_matrix = interaction_matrix.unstack(fill_value=0)

# Sparse matrix ìƒì„±
sparse_user_item = csr_matrix(interaction_matrix.values)
sparse_item_user = sparse_user_item.T

# WRMF ëª¨ë¸ í•™ìŠµ
model = implicit.als.AlternatingLeastSquares(
    factors=128,
    regularization=0.01,
    alpha=40,  # confidence = 1 + alpha * interaction
    iterations=15,
    use_gpu=False
)

print("\nWRMF ëª¨ë¸ í•™ìŠµ ì¤‘...")
model.fit(sparse_item_user)

# ì¶”ì²œ ìƒì„± ë° í‰ê°€
def evaluate_implicit_model(model, train_sparse, test_df, k=10):
    """ì•”ì‹œì  í”¼ë“œë°± ëª¨ë¸ í‰ê°€"""
    
    # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ìë³„ ì‹¤ì œ êµ¬ë§¤ ì•„ì´í…œ
    test_items_per_user = test_df.groupby('CustomerID')['StockCode'].apply(list).to_dict()
    
    precisions = []
    recalls = []
    
    for user_idx, user_id in enumerate(interaction_matrix.index):
        if user_id in test_items_per_user:
            # ì¶”ì²œ ìƒì„±
            recommendations, scores = model.recommend(
                userid=user_idx,
                user_items=train_sparse[user_idx],
                N=k,
                filter_already_liked_items=True
            )
            
            # ì‹¤ì œ êµ¬ë§¤ ì•„ì´í…œ
            true_items = test_items_per_user[user_id]
            true_item_indices = [
                interaction_matrix.columns.get_loc(item) 
                for item in true_items 
                if item in interaction_matrix.columns
            ]
            
            # Precision & Recall ê³„ì‚°
            hits = len(set(recommendations) & set(true_item_indices))
            precision = hits / k if k > 0 else 0
            recall = hits / len(true_item_indices) if true_item_indices else 0
            
            precisions.append(precision)
            recalls.append(recall)
    
    print(f"\nPrecision@{k}: {np.mean(precisions):.4f}")
    print(f"Recall@{k}: {np.mean(recalls):.4f}")
    
    return np.mean(precisions), np.mean(recalls)

# í‰ê°€
evaluate_implicit_model(model, sparse_user_item, test_df, k=10)

# Cold Start ì‚¬ìš©ì ì²˜ë¦¬
cold_start_users = set(test_df['CustomerID'].unique()) - set(train_df['CustomerID'].unique())
print(f"\nCold Start ì‚¬ìš©ì ìˆ˜: {len(cold_start_users):,}")

# ì¸ê¸°ë„ ê¸°ë°˜ í´ë°±
popular_items = train_df['StockCode'].value_counts().head(10).index.tolist()
print(f"Cold Start ì‚¬ìš©ìë¥¼ ìœ„í•œ ì¸ê¸° ì•„ì´í…œ: {popular_items[:5]}")
```

### 3. PyTorch êµ¬í˜„ (ì™„ì „í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§•)

MovieLens ë°ì´í„°ë¡œ MFì™€ BPRì„ ì§ì ‘ êµ¬í˜„:

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class RatingsDataset(Dataset):
    def __init__(self, ratings_df):
        self.users = torch.LongTensor(ratings_df['user_idx'].values)
        self.items = torch.LongTensor(ratings_df['item_idx'].values)
        self.ratings = torch.FloatTensor(ratings_df['rating'].values)
    
    def __len__(self):
        return len(self.ratings)
    
    def __getitem__(self, idx):
        return self.users[idx], self.items[idx], self.ratings[idx]

# Matrix Factorization with Bias
class MatrixFactorization(nn.Module):
    def __init__(self, n_users, n_items, n_factors=100):
        super().__init__()
        
        # ì„ë² ë”© ë ˆì´ì–´
        self.user_factors = nn.Embedding(n_users, n_factors)
        self.item_factors = nn.Embedding(n_items, n_factors)
        self.user_bias = nn.Embedding(n_users, 1)
        self.item_bias = nn.Embedding(n_items, 1)
        
        # ê¸€ë¡œë²Œ ë°”ì´ì–´ìŠ¤
        self.global_bias = nn.Parameter(torch.zeros(1))
        
        # ì´ˆê¸°í™”
        self._init_weights()
    
    def _init_weights(self):
        nn.init.normal_(self.user_factors.weight, std=0.01)
        nn.init.normal_(self.item_factors.weight, std=0.01)
        nn.init.zeros_(self.user_bias.weight)
        nn.init.zeros_(self.item_bias.weight)
    
    def forward(self, user, item):
        # ì ì¬ ìš”ì¸ ë‚´ì 
        dot = (self.user_factors(user) * self.item_factors(item)).sum(1)
        
        # ë°”ì´ì–´ìŠ¤ ì¶”ê°€
        rating = (self.global_bias + 
                 self.user_bias(user).squeeze() + 
                 self.item_bias(item).squeeze() + 
                 dot)
        
        return rating
    
    def predict(self, user_idx, n_items=10):
        """íŠ¹ì • ì‚¬ìš©ìë¥¼ ìœ„í•œ Top-N ì¶”ì²œ"""
        with torch.no_grad():
            user = torch.LongTensor([user_idx])
            items = torch.arange(self.item_factors.num_embeddings)
            
            # ëª¨ë“  ì•„ì´í…œì— ëŒ€í•œ ì ìˆ˜ ê³„ì‚°
            user_factor = self.user_factors(user)
            item_factors = self.item_factors.weight
            
            scores = (user_factor @ item_factors.T).squeeze()
            scores += self.user_bias(user).squeeze()
            scores += self.item_bias.weight.squeeze()
            scores += self.global_bias
            
            # Top-N ì„ íƒ
            top_scores, top_items = torch.topk(scores, n_items)
            
            return top_items.numpy(), top_scores.numpy()

# BPR with Negative Sampling
class BPRDataset(Dataset):
    def __init__(self, interactions_df, n_items, is_train=True):
        self.is_train = is_train
        self.n_items = n_items
        
        # ì‚¬ìš©ìë³„ ìƒí˜¸ì‘ìš© ì•„ì´í…œ ì €ì¥
        self.user_items = interactions_df.groupby('user_idx')['item_idx'].apply(set).to_dict()
        
        # í•™ìŠµìš© ë°ì´í„°
        if is_train:
            self.interactions = [(u, i) for u, items in self.user_items.items() for i in items]
    
    def __len__(self):
        return len(self.interactions) if self.is_train else 0
    
    def __getitem__(self, idx):
        if not self.is_train:
            return None
            
        user, pos_item = self.interactions[idx]
        
        # ë¶€ì • ìƒ˜í”Œë§
        neg_item = np.random.randint(self.n_items)
        while neg_item in self.user_items[user]:
            neg_item = np.random.randint(self.n_items)
        
        return user, pos_item, neg_item

class BPRModel(nn.Module):
    def __init__(self, n_users, n_items, n_factors=100):
        super().__init__()
        
        self.user_factors = nn.Embedding(n_users, n_factors)
        self.item_factors = nn.Embedding(n_items, n_factors)
        
        nn.init.normal_(self.user_factors.weight, std=0.01)
        nn.init.normal_(self.item_factors.weight, std=0.01)
    
    def forward(self, user, item_i, item_j):
        user_vec = self.user_factors(user)
        item_i_vec = self.item_factors(item_i)
        item_j_vec = self.item_factors(item_j)
        
        # ì ìˆ˜ ê³„ì‚°
        pos_score = (user_vec * item_i_vec).sum(1)
        neg_score = (user_vec * item_j_vec).sum(1)
        
        return pos_score, neg_score
    
    def predict(self, user_idx, user_items, n_items=10):
        """BPR ì¶”ì²œ ìƒì„±"""
        with torch.no_grad():
            user = torch.LongTensor([user_idx])
            user_vec = self.user_factors(user)
            
            # ëª¨ë“  ì•„ì´í…œ ì ìˆ˜
            scores = (user_vec @ self.item_factors.weight.T).squeeze()
            
            # ì´ë¯¸ ë³¸ ì•„ì´í…œ ì œì™¸
            scores[list(user_items)] = -float('inf')
            
            # Top-N
            top_scores, top_items = torch.topk(scores, n_items)
            
            return top_items.numpy(), top_scores.numpy()

# í•™ìŠµ í•¨ìˆ˜
def train_model(model, train_loader, val_loader, n_epochs=10, lr=0.001):
    """ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜"""
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # MFìš© ì†ì‹¤ í•¨ìˆ˜
    if isinstance(model, MatrixFactorization):
        criterion = nn.MSELoss()
    # BPRìš© ì†ì‹¤ í•¨ìˆ˜
    else:
        criterion = lambda pos, neg: -torch.log(torch.sigmoid(pos - neg)).mean()
    
    train_losses = []
    val_losses = []
    
    for epoch in range(n_epochs):
        # Training
        model.train()
        train_loss = 0
        
        for batch in train_loader:
            if isinstance(model, MatrixFactorization):
                user, item, rating = batch
                prediction = model(user, item)
                loss = criterion(prediction, rating)
            else:
                user, pos_item, neg_item = batch
                pos_score, neg_score = model(user, pos_item, neg_item)
                loss = criterion(pos_score, neg_score)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validation
        model.eval()
        val_loss = 0
        
        with torch.no_grad():
            for batch in val_loader:
                if isinstance(model, MatrixFactorization):
                    user, item, rating = batch
                    prediction = model(user, item)
                    loss = criterion(prediction, rating)
                else:
                    user, pos_item, neg_item = batch
                    pos_score, neg_score = model(user, pos_item, neg_item)
                    loss = criterion(pos_score, neg_score)
                
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
        val_losses.append(avg_val_loss)
        
        print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")
    
    return train_losses, val_losses

# ì‹¤ì œ ì‚¬ìš©
# MovieLens ë°ì´í„° ë¡œë“œ (ìƒëµ)
# ...

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
mf_model = MatrixFactorization(n_users, n_items, n_factors=100)
bpr_model = BPRModel(n_users, n_items, n_factors=100)

# í•™ìŠµ
train_losses_mf, val_losses_mf = train_model(mf_model, train_loader_mf, val_loader_mf)
train_losses_bpr, val_losses_bpr = train_model(bpr_model, train_loader_bpr, val_loader_bpr)

# ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# MF ì†ì‹¤
ax1.plot(train_losses_mf, label='Train', color='blue')
ax1.plot(val_losses_mf, label='Validation', color='red')
ax1.set_title('Matrix Factorization Learning Curve')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('MSE Loss')
ax1.legend()
ax1.grid(True, alpha=0.3)

# BPR ì†ì‹¤
ax2.plot(train_losses_bpr, label='Train', color='blue')
ax2.plot(val_losses_bpr, label='Validation', color='red')
ax2.set_title('BPR Learning Curve')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('BPR Loss')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## ğŸš€ ì‹¤ì „ ì ìš© ê°€ì´ë“œ

### ìƒí™©ë³„ ìµœì  ì „ëµ ì„ íƒ

```mermaid
graph TD
    A[ë°ì´í„° íƒ€ì…?] --> B[ëª…ì‹œì  í”¼ë“œë°±<br/>í‰ì , ë³„ì ]
    A --> C[ì•”ì‹œì  í”¼ë“œë°±<br/>í´ë¦­, êµ¬ë§¤, ì‹œì²­]
    
    B --> D[ë°ì´í„° ê·œëª¨?]
    D --> E[ì†Œê·œëª¨<br/><100K] --> F[Surprise + SVD/NMF]
    D --> G[ì¤‘ê·œëª¨<br/>100K-10M] --> H[Surprise + GridSearch]
    D --> I[ëŒ€ê·œëª¨<br/>>10M] --> J[Custom PyTorch/TF]
    
    C --> K[ì‹¤ì‹œê°„ì„±?]
    K --> L[ì¤‘ìš”] --> M[SLIM/Item2Vec]
    K --> N[ë³´í†µ] --> O[Implicit + ALS]
    K --> P[ëœ ì¤‘ìš”] --> Q[BPR + GPU]
    
    style B fill:#e3f2fd
    style C fill:#fff3e0
    style F fill:#e8f5e8
    style M fill:#e8f5e8
```

### ì‹¤ì œ ì„œë¹„ìŠ¤ íŒŒì´í”„ë¼ì¸

```python
class ProductionRecommenderPipeline:
    """í”„ë¡œë•ì…˜ ë ˆë²¨ ì¶”ì²œ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.main_model = None
        self.fallback_model = None
        self.popular_items = None
    
    def train_pipeline(self, interactions_df):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í•™ìŠµ"""
        
        print("1. ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬...")
        interactions_df = self.validate_and_clean(interactions_df)
        
        print("2. ì‹œê°„ ê¸°ë°˜ ë¶„í• ...")
        train_df, val_df, test_df = self.temporal_split(interactions_df)
        
        print("3. ë©”ì¸ ëª¨ë¸ í•™ìŠµ (WRMF)...")
        self.main_model = self.train_main_model(train_df, val_df)
        
        print("4. í´ë°± ëª¨ë¸ ì¤€ë¹„ (ì¸ê¸°ë„)...")
        self.popular_items = self.compute_popular_items(train_df)
        
        print("5. User-free ëª¨ë¸ í•™ìŠµ (SLIM)...")
        self.fallback_model = self.train_slim(train_df)
        
        print("6. í‰ê°€...")
        metrics = self.evaluate(test_df)
        
        return metrics
    
    def serve_recommendations(self, user_id, user_history=None, n_items=10):
        """ì‹¤ì‹œê°„ ì¶”ì²œ ì„œë¹™"""
        
        try:
            # 1ì°¨: ë©”ì¸ ëª¨ë¸
            if self.main_model and user_id in self.main_model.user_mapping:
                recs = self.main_model.recommend(user_id, n_items)
                return recs, 'main_model'
            
            # 2ì°¨: User-free ëª¨ë¸ (ì‹ ê·œ ì‚¬ìš©ì)
            elif self.fallback_model and user_history:
                recs = self.fallback_model.recommend(user_history, n_items)
                return recs, 'user_free_model'
            
            # 3ì°¨: ì¸ê¸°ë„ ê¸°ë°˜
            else:
                recs = self.popular_items[:n_items]
                return recs, 'popularity_fallback'
                
        except Exception as e:
            # ì—ëŸ¬ ì‹œ ì•ˆì „í•œ í´ë°±
            print(f"Error in recommendation: {e}")
            return self.popular_items[:n_items], 'error_fallback'
    
    def update_incremental(self, new_interactions):
        """ì¦ë¶„ í•™ìŠµ (ì¼ì¼ ë°°ì¹˜)"""
        
        # ìƒˆë¡œìš´ ì¸ê¸° ì•„ì´í…œ ì—…ë°ì´íŠ¸
        self.popular_items = self.compute_popular_items(
            new_interactions, 
            decay=0.95  # ì‹œê°„ ê°ì‡ 
        )
        
        # SLIM ëª¨ë¸ ë¶€ë¶„ ì—…ë°ì´íŠ¸
        if hasattr(self.fallback_model, 'partial_fit'):
            self.fallback_model.partial_fit(new_interactions)
        
        # ë©”ì¸ ëª¨ë¸ì€ ì£¼ê¸°ì  ì¬í•™ìŠµ (ì˜ˆ: ì£¼ 1íšŒ)
        
    def monitor_performance(self):
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        
        metrics = {
            'coverage': self.compute_coverage(),
            'diversity': self.compute_diversity(),
            'novelty': self.compute_novelty(),
            'cold_start_rate': self.compute_cold_start_rate()
        }
        
        return metrics
```

## ğŸ¯ ë§ˆë¬´ë¦¬: í•µì‹¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

ëª¨ë¸ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ì„ ë§ˆìŠ¤í„°í•˜ê¸° ìœ„í•œ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë‹¤ì‹œ í•œ ë²ˆ ì •ë¦¬í•˜ë©´:

**ì´ë¡ ì  ì´í•´:**

- Matrix Factorizationì€ í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ ì €ì°¨ì› í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•œë‹¤
- WRMFëŠ” preferenceì™€ confidenceë¥¼ ë¶„ë¦¬í•˜ì—¬ ì•”ì‹œì  í”¼ë“œë°±ì„ ì •êµí•˜ê²Œ ì²˜ë¦¬í•œë‹¤
- BPRì€ pairwise ë¹„êµë¡œ ìˆœìœ„ë¥¼ ì§ì ‘ í•™ìŠµí•œë‹¤
- User-free ëª¨ë¸ì€ Cold Start ë¬¸ì œì˜ ì‹¤ìš©ì  í•´ê²°ì±…ì´ë‹¤

**ì‹¤ë¬´ ì ìš©:**

- ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• ì´ ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì„ ë°˜ì˜í•œë‹¤
- Leave-One-LastëŠ” íš¨ìœ¨ì ì´ê³  í˜„ì‹¤ì ì¸ í‰ê°€ ë°©ë²•ì´ë‹¤
- Stratified Samplingìœ¼ë¡œ ê· í˜•ì¡íŒ í‰ê°€ê°€ ê°€ëŠ¥í•˜ë‹¤
- ìƒí™©ì— ë§ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒì´ ì¤‘ìš”í•˜ë‹¤

**ì„±ëŠ¥ ìµœì í™”:**

- SGD vs ALS: ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ì„ íƒ
- Negative Sampling ì „ëµì´ BPR ì„±ëŠ¥ì„ ì¢Œìš°
- GridSearchCVë¡œ ì²´ê³„ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ë‹¤ë‹¨ê³„ í´ë°± ì „ëµìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´

ì´ëŸ¬í•œ ê¸°ìˆ ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°í•©í•˜ë©´, Netflixë‚˜ Amazon ìˆ˜ì¤€ì˜ ì¶”ì²œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆë¥¼ ê°–ì¶”ê²Œ ë©ë‹ˆë‹¤. ê° ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ì´í•´í•˜ê³  ìƒí™©ì— ë§ê²Œ ì ìš©í•˜ëŠ” ê²ƒì´ ì„±ê³µì˜ ì—´ì‡ ì…ë‹ˆë‹¤!