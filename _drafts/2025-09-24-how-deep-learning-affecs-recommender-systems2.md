---
title: "ğŸ§  ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ: ì „í†µì  CFì—ì„œ Graph Neural Networkê¹Œì§€"
date: 2025-09-24 15:29:00 +0900
categories: []
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
  - GeekAndChill
  - ê¸°ê¹¬ì¹ 
  - ì—ì´ì•„ì´
  - ì—…ìŠ¤í…Œì´ì§€ì—ì´ì•„ì´ë©
  - UpstageAILab
  - UpstageAILab6ê¸°
  - ML
  - DL
  - machinelearning
  - deeplearning
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” íŒ¨í‚¤ì§€/ê¸°ìˆ  ë²„ì „ ì •ë³´

- torch==2.6.0
- numpy==1.26.4
- pandas==2.2.3
- scipy==1.15.2
- matplotlib==3.10.1
- gensim==4.3.2
- scikit-learn==1.6.1

## ğŸš€ TL;DR

- ë”¥ëŸ¬ë‹ ì¶”ì²œ ì‹œìŠ¤í…œì€ ì „í†µì ì¸ Matrix Factorizationì˜ ì„ í˜•ì  í•œê³„ë¥¼ ë¹„ì„ í˜• ì‹ ê²½ë§ìœ¼ë¡œ ê·¹ë³µí•˜ì—¬ ë” ë³µì¡í•œ ìœ ì €-ì•„ì´í…œ ìƒí˜¸ì‘ìš© íŒ¨í„´ì„ í¬ì°©í•©ë‹ˆë‹¤
- Neural CFëŠ” ì„ í˜• ìƒí˜¸ì‘ìš©(GMF)ê³¼ ë¹„ì„ í˜• ìƒí˜¸ì‘ìš©(MLP)ì„ ê²°í•©í•˜ì—¬ ë‘ ê°€ì§€ íŒ¨í„´ì„ ëª¨ë‘ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤
- Item2Vecì€ Word2Vecì˜ ì•„ì´ë””ì–´ë¥¼ ì¶”ì²œ ì‹œìŠ¤í…œì— ì ìš©í•˜ì—¬, ìœ ì €ì˜ ì„¸ì…˜ ë°ì´í„°ë§Œìœ¼ë¡œë„ ì•„ì´í…œ ì„ë² ë”©ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- Autoencoder ê¸°ë°˜ CFëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ë³µì›í•˜ëŠ” ê³¼ì •ì—ì„œ ìœ ì €-ì•„ì´í…œ ìƒí˜¸ì‘ìš©ì˜ ì ì¬ íŒ¨í„´ì„ í•™ìŠµí•˜ë©°, DAEì™€ VAEë¡œ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤
- GNN(Graph Neural Network)ì€ ìœ ì €-ì•„ì´í…œ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ì—¬ ê³ ì°¨ì› ì—°ê²°ì„±ì„ ê³ ë ¤í•œ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤
- ì‚°ì—…ê³„ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ë„ì…ìœ¼ë¡œ 9~40%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ê²½í—˜í–ˆìœ¼ë©°, ë„·í”Œë¦­ìŠ¤, ìœ íŠœë¸Œ, í•€í„°ë ˆìŠ¤íŠ¸, ì›”ë§ˆíŠ¸ ë“±ì´ ì ê·¹ í™œìš© ì¤‘ì…ë‹ˆë‹¤

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [Neural CF ì‹¤ìŠµ ë…¸íŠ¸ë¶](https://github.com/yuiyeong/notebooks/blob/main/machine_learning/neural_cf_implementation.ipynb)
- [Item2Vec ì‹¤ìŠµ ë…¸íŠ¸ë¶](https://github.com/yuiyeong/notebooks/blob/main/machine_learning/item2vec_implementation.ipynb)

## ğŸ¯ ë”¥ëŸ¬ë‹ì´ ì¶”ì²œ ì‹œìŠ¤í…œì„ ë§Œë‚¬ì„ ë•Œ

### ì™œ ë”¥ëŸ¬ë‹ì¸ê°€?

ì¶”ì²œ ì‹œìŠ¤í…œì— ë”¥ëŸ¬ë‹ì„ í™œìš©í•˜ëŠ” ê²ƒì€ ë§ˆì¹˜ 2ì°¨ì› ì§€ë„ë¡œë§Œ ì„¸ê³„ë¥¼ ë³´ë˜ ê²ƒì—ì„œ 3ì°¨ì› ì§€êµ¬ë³¸ìœ¼ë¡œ ì„¸ê³„ë¥¼ ë³´ëŠ” ê²ƒê³¼ ê°™ì€ ë³€í™”ì…ë‹ˆë‹¤. ì „í†µì ì¸ Matrix Factorizationì´ ì„ í˜•ì ì¸ ê´€ê³„ë§Œ í¬ì°©í•  ìˆ˜ ìˆì—ˆë‹¤ë©´, ë”¥ëŸ¬ë‹ì€ ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´ê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```mermaid
mindmap
  root((ë”¥ëŸ¬ë‹ ì¶”ì²œ ì‹œìŠ¤í…œ))
    (ì¥ì )
      [ë¹„ì„ í˜• ë³€í™˜]
        (ReLU, Sigmoid í™œì„±í™” í•¨ìˆ˜)
        (ë³µì¡í•œ íŒ¨í„´ í¬ì°©)
      [í‘œí˜„ í•™ìŠµ]
        (ìë™ í”¼ì²˜ ì¶”ì¶œ)
        (ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ì²˜ë¦¬)
      [ì‹œí€€ìŠ¤ ëª¨ë¸ë§]
        (ì‹œê°„ì  íŒ¨í„´ í•™ìŠµ)
        (ì„¸ì…˜ ê¸°ë°˜ ì¶”ì²œ)
      [ìœ ì—°ì„±]
        (ë‹¤ì–‘í•œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ê²°í•©)
        (ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ í™œìš©)
    (í•œê³„)
      [í•´ì„ ê°€ëŠ¥ì„±]
        (ë¸”ë™ë°•ìŠ¤ ë¬¸ì œ)
        (Explainable AI ì—°êµ¬ í•„ìš”)
      [ë°ì´í„° ìš”êµ¬ì‚¬í•­]
        (ëŒ€ëŸ‰ í•™ìŠµ ë°ì´í„° í•„ìš”)
        (Transfer Learningìœ¼ë¡œ ê·¹ë³µ)
      [í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹]
        (ë³µì¡í•œ ìµœì í™”)
        (AutoML í™œìš© ê°€ëŠ¥)
```

### ì‚°ì—…ê³„ì˜ ì‹¤ì œ ì„±ê³¼

ì‹¤ì œë¡œ ì‚°ì—…ê³„ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ë„ì…ìœ¼ë¡œ ë†€ë¼ìš´ ì„±ê³¼ë¥¼ ê±°ë‘ê³  ìˆìŠµë‹ˆë‹¤:

- **ë„·í”Œë¦­ìŠ¤**: ì¶”ì²œ ì •í™•ë„ 9% í–¥ìƒ
- **ìœ íŠœë¸Œ**: ì‹œì²­ ì‹œê°„ 20% ì¦ê°€
- **í•€í„°ë ˆìŠ¤íŠ¸**: ì‚¬ìš©ì ì°¸ì—¬ë„ 30% í–¥ìƒ
- **ì›”ë§ˆíŠ¸**: ì „í™˜ìœ¨ 40% ê°œì„ 

## ğŸ§  Neural Collaborative Filtering: ì„ í˜•ê³¼ ë¹„ì„ í˜•ì˜ ë§Œë‚¨

### MFì˜ ì„ í˜•ì  í•œê³„

ì „í†µì ì¸ Matrix Factorizationì˜ ë¬¸ì œì ì„ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¡œ ì´í•´í•´ë³´ê² ìŠµë‹ˆë‹¤. 3ëª…ì˜ ìœ ì €ì™€ 5ê°œì˜ ì•„ì´í…œì´ ìˆëŠ” ìƒí™©ì„ ìƒê°í•´ë³´ì„¸ìš”:

```python
import numpy as np
import torch
import torch.nn as nn

# ìœ ì €-ì•„ì´í…œ ìƒí˜¸ì‘ìš© í–‰ë ¬
R = np.array([
    [1, 1, 0, 1, 0],  # User 1
    [0, 1, 1, 0, 1],  # User 2  
    [0, 0, 1, 1, 1]   # User 3
])

# Jaccard ìœ ì‚¬ë„ ê³„ì‚°
def jaccard_similarity(u1, u2):
    intersection = np.sum(np.logical_and(u1, u2))
    union = np.sum(np.logical_or(u1, u2))
    return intersection / union

# ìœ ì € ê°„ ìœ ì‚¬ë„
print(f"U1-U2 ìœ ì‚¬ë„: {jaccard_similarity(R[0], R[1]):.2f}")  # 0.50
print(f"U1-U3 ìœ ì‚¬ë„: {jaccard_similarity(R[0], R[2]):.2f}")  # 0.40
print(f"U2-U3 ìœ ì‚¬ë„: {jaccard_similarity(R[1], R[2]):.2f}")  # 0.66
```

ì´ë•Œ ìƒˆë¡œìš´ ìœ ì € 4ê°€ [1, 0, 1, 1, 0] íŒ¨í„´ìœ¼ë¡œ ë“±ì¥í•˜ë©´, ìœ ì € 1ê³¼ ê°€ì¥ ìœ ì‚¬(0.60)í•˜ì§€ë§Œ, MFì˜ ì„ í˜•ì„± ë•Œë¬¸ì— latent spaceì—ì„œ ì´ë¥¼ ì œëŒ€ë¡œ í‘œí˜„í•˜ì§€ ëª»í•˜ëŠ” ëª¨ìˆœì´ ë°œìƒí•©ë‹ˆë‹¤.

### Neural CF êµ¬ì¡°ì˜ ì´í•´

Neural CFëŠ” ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ì ‘ê·¼ì„ ê²°í•©í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "ì…ë ¥ì¸µ"
        U[ìœ ì € ID]
        I[ì•„ì´í…œ ID]
    end
    
    subgraph "ì„ë² ë”©ì¸µ"
        U --> UE_GMF[ìœ ì € ì„ë² ë”© GMF]
        U --> UE_MLP[ìœ ì € ì„ë² ë”© MLP]
        I --> IE_GMF[ì•„ì´í…œ ì„ë² ë”© GMF]
        I --> IE_MLP[ì•„ì´í…œ ì„ë² ë”© MLP]
    end
    
    subgraph "GMF ê²½ë¡œ (ì„ í˜•)"
        UE_GMF --> GMF[Element-wise Product]
        IE_GMF --> GMF
        GMF --> GMF_OUT[GMF ì¶œë ¥]
    end
    
    subgraph "MLP ê²½ë¡œ (ë¹„ì„ í˜•)"
        UE_MLP --> CONCAT[Concatenation]
        IE_MLP --> CONCAT
        CONCAT --> MLP1[Dense + ReLU]
        MLP1 --> MLP2[Dense + ReLU]
        MLP2 --> MLP_OUT[MLP ì¶œë ¥]
    end
    
    GMF_OUT --> FINAL[ìµœì¢… ê²°í•©ì¸µ]
    MLP_OUT --> FINAL
    FINAL --> PRED[ì˜ˆì¸¡ê°’]
```

### Neural CF êµ¬í˜„

ì´ì œ ì‹¤ì œë¡œ Neural CFë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤. ë¨¼ì € GMF(Generalized Matrix Factorization) ë¶€ë¶„ì…ë‹ˆë‹¤:

```python
class GMF(nn.Module):
    def __init__(self, num_users, num_items, factor_num):
        super(GMF, self).__init__()
        # ìœ ì €ì™€ ì•„ì´í…œ ì„ë² ë”© ì´ˆê¸°í™”
        self.embed_user_GMF = nn.Embedding(num_users, factor_num)
        self.embed_item_GMF = nn.Embedding(num_items, factor_num)
        
        # ì¶œë ¥ì¸µ
        self.predict_layer = nn.Linear(factor_num, 1)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weight_()
    
    def _init_weight_(self):
        nn.init.normal_(self.embed_user_GMF.weight, std=0.01)
        nn.init.normal_(self.embed_item_GMF.weight, std=0.01)
        nn.init.kaiming_uniform_(self.predict_layer.weight, nonlinearity='relu')
    
    def forward(self, user, item):
        # ìœ ì €ì™€ ì•„ì´í…œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        embed_user_GMF = self.embed_user_GMF(user)
        embed_item_GMF = self.embed_item_GMF(item)
        
        # Element-wise product (ê¸°ì¡´ MFì˜ ë‚´ì ì„ ì¼ë°˜í™”)
        output_GMF = embed_user_GMF * embed_item_GMF
        
        # ìµœì¢… ì˜ˆì¸¡ê°’
        prediction = self.predict_layer(output_GMF)
        return prediction.view(-1)
```

ë‹¤ìŒì€ MLP(Multi-Layer Perceptron) ë¶€ë¶„ì…ë‹ˆë‹¤:

```python
class MLP(nn.Module):
    def __init__(self, num_users, num_items, factor_num, num_layers, dropout=0.0):
        super(MLP, self).__init__()
        # ì„ë² ë”© ì°¨ì› ê³„ì‚° (í”¼ë¼ë¯¸ë“œ êµ¬ì¡°)
        # ì˜ˆ: factor_num=32, num_layers=4 â†’ 256ì°¨ì›
        embedding_dim = factor_num * (2 ** (num_layers - 1))
        
        self.embed_user_MLP = nn.Embedding(num_users, embedding_dim)
        self.embed_item_MLP = nn.Embedding(num_items, embedding_dim)
        
        # MLP ë ˆì´ì–´ êµ¬ì„± (ì ì  ì¢ì•„ì§€ëŠ” êµ¬ì¡°)
        MLP_modules = []
        for i in range(num_layers):
            input_size = factor_num * (2 ** (num_layers - i))
            MLP_modules.append(nn.Dropout(p=dropout))
            MLP_modules.append(nn.Linear(input_size, input_size // 2))
            MLP_modules.append(nn.ReLU())
        
        self.MLP_layers = nn.Sequential(*MLP_modules)
        self.predict_layer = nn.Linear(factor_num, 1)
        
        self._init_weight_()
    
    def forward(self, user, item):
        # ìœ ì €ì™€ ì•„ì´í…œ ì„ë² ë”© ì—°ê²°
        embed_user_MLP = self.embed_user_MLP(user)
        embed_item_MLP = self.embed_item_MLP(item)
        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)
        
        # MLP í†µê³¼
        output_MLP = self.MLP_layers(interaction)
        
        # ìµœì¢… ì˜ˆì¸¡
        prediction = self.predict_layer(output_MLP)
        return prediction.view(-1)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ GMFì™€ MLPë¥¼ ê²°í•©í•œ Neural MFì…ë‹ˆë‹¤:

```python
class NeuralMF(nn.Module):
    def __init__(self, num_users, num_items, factor_num, num_layers, dropout=0.0):
        super(NeuralMF, self).__init__()
        # GMFì™€ MLP ì»´í¬ë„ŒíŠ¸
        self.GMF = GMF(num_users, num_items, factor_num)
        self.MLP = MLP(num_users, num_items, factor_num, num_layers, dropout)
        
        # ìµœì¢… ì˜ˆì¸¡ì¸µ
        self.predict_layer = nn.Linear(factor_num * 2, 1)
        
        self._init_weight_()
    
    def forward(self, user, item):
        # GMFì™€ MLP ì¶œë ¥ ì–»ê¸°
        output_GMF = self.GMF.embed_user_GMF(user) * self.GMF.embed_item_GMF(item)
        
        embed_user_MLP = self.MLP.embed_user_MLP(user)
        embed_item_MLP = self.MLP.embed_item_MLP(item)
        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)
        output_MLP = self.MLP.MLP_layers(interaction)
        
        # ë‘ ì¶œë ¥ ê²°í•©
        concat = torch.cat((output_GMF, output_MLP), -1)
        
        # ìµœì¢… ì˜ˆì¸¡
        prediction = self.predict_layer(concat)
        return prediction.view(-1)
```

## ğŸ­ Item2Vec: Word2Vecì˜ ì¶”ì²œ ì‹œìŠ¤í…œ ë³€ì‹ 

### Word2Vecì—ì„œ Item2Vecìœ¼ë¡œ

Word2Vecì´ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•œë‹¤ë©´, Item2Vecì€ ì•„ì´í…œ(ìƒí’ˆ, ì˜í™” ë“±)ì˜ íŠ¹ì„±ì„ ë²¡í„°ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” "í•¨ê»˜ ì†Œë¹„ë˜ëŠ” ì•„ì´í…œì€ ìœ ì‚¬í•˜ë‹¤"ëŠ” ê²ƒì…ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph "Word2Vec (NLP)"
        W1["The cat sat on mat"] --> WV["ë‹¨ì–´ ë²¡í„°"]
    end
    
    subgraph "Item2Vec (ì¶”ì²œ)"
        I1["í•´ë¦¬í¬í„° â†’ ë°˜ì§€ì˜ì œì™• â†’ í˜¸ë¹—"] --> IV["ì•„ì´í…œ ë²¡í„°"]
    end
    
    W1 -.ìœ ì‚¬í•œ ì›ë¦¬.-> I1
```

### Skip-gram with Negative Sampling (SGNS)

Item2Vecì—ì„œëŠ” ì£¼ë¡œ SGNSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì•„ì´í…œ ì„¸ì…˜ì´ ì£¼ì–´ì¡Œì„ ë•Œ í•™ìŠµ ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

```python
# ì„¸ì…˜ ì˜ˆì‹œ: í•œ ìœ ì €ê°€ ì‹œì²­í•œ ì˜í™”ë“¤
session = ["í•´ë¦¬í¬í„°", "ì•„ì¦ˆì¹´ë°˜", "ë¹„ë°€ì˜ë°©", "ë§ˆë²•ì‚¬ì˜ëŒ"]

# Skip-gram ë°ì´í„° êµ¬ì„±
def create_skipgram_data(session, window_size=2):
    training_data = []
    
    for i, center_item in enumerate(session):
        # ìœˆë„ìš° ë‚´ì˜ ì£¼ë³€ ì•„ì´í…œë“¤ê³¼ ìŒ ìƒì„±
        for j in range(max(0, i-window_size), 
                      min(len(session), i+window_size+1)):
            if i != j:
                # (ì¤‘ì‹¬ ì•„ì´í…œ, ì£¼ë³€ ì•„ì´í…œ, ë ˆì´ë¸”=1)
                training_data.append((center_item, session[j], 1))
    
    return training_data

# í•™ìŠµ ë°ì´í„° ìƒì„±
training_pairs = create_skipgram_data(session)
print(training_pairs[:3])
# [('í•´ë¦¬í¬í„°', 'ì•„ì¦ˆì¹´ë°˜', 1), 
#  ('í•´ë¦¬í¬í„°', 'ë¹„ë°€ì˜ë°©', 1),
#  ('ì•„ì¦ˆì¹´ë°˜', 'í•´ë¦¬í¬í„°', 1)]

# Negative Sampling ì¶”ê°€
import random

def add_negative_samples(training_data, all_items, num_neg=5):
    augmented_data = []
    
    for center, context, label in training_data:
        # Positive ìƒ˜í”Œ ì¶”ê°€
        augmented_data.append((center, context, 1))
        
        # Negative ìƒ˜í”Œ ì¶”ê°€
        for _ in range(num_neg):
            neg_item = random.choice(all_items)
            while neg_item in [center, context]:
                neg_item = random.choice(all_items)
            augmented_data.append((center, neg_item, 0))
    
    return augmented_data
```

### Gensimì„ í™œìš©í•œ Item2Vec êµ¬í˜„

ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Gensim ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ Item2Vecì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from gensim.models import Word2Vec
import pandas as pd

# ìœ ì €ë³„ ì•„ì´í…œ ì„¸ì…˜ ì¤€ë¹„
def prepare_item_sessions(ratings_df, threshold=4.0):
    """í‰ì  ê¸°ì¤€ìœ¼ë¡œ ì„¸ì…˜ ìƒì„±"""
    # ë†’ì€ í‰ì ì„ ë°›ì€ ì˜í™”ë“¤ì„ í•˜ë‚˜ì˜ ì„¸ì…˜ìœ¼ë¡œ
    positive_sessions = []
    
    for user_id, group in ratings_df.groupby('userId'):
        # 4ì  ì´ìƒ ì˜í™”ë“¤
        liked_movies = group[group['rating'] >= threshold]['movieId'].tolist()
        if len(liked_movies) > 1:
            positive_sessions.append([str(m) for m in liked_movies])
    
    return positive_sessions

# Item2Vec ëª¨ë¸ í•™ìŠµ
def train_item2vec(sessions, 
                   vector_size=100,
                   window=20,
                   min_count=10,
                   sg=1,  # Skip-gram ì‚¬ìš©
                   negative=5,  # Negative sampling
                   epochs=5):
    
    model = Word2Vec(
        sentences=sessions,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        sg=sg,
        negative=negative,
        epochs=epochs,
        workers=4
    )
    
    return model

# ëª¨ë¸ í•™ìŠµ
sessions = prepare_item_sessions(ratings_df)
item2vec_model = train_item2vec(sessions)

# ìœ ì‚¬ ì•„ì´í…œ ì°¾ê¸°
def find_similar_items(model, item_id, topn=5):
    try:
        similar = model.wv.most_similar(str(item_id), topn=topn)
        return [(item, score) for item, score in similar]
    except KeyError:
        return []

# í† ì´ìŠ¤í† ë¦¬ì™€ ìœ ì‚¬í•œ ì˜í™” ì°¾ê¸°
toy_story_id = "1"  # Toy Storyì˜ ID
similar_movies = find_similar_items(item2vec_model, toy_story_id)

for movie_id, similarity in similar_movies:
    movie_title = id_to_title.get(int(movie_id), "Unknown")
    print(f"{movie_title}: {similarity:.4f}")
```

### Item2Vecê³¼ MFì˜ ì—°ê²°ê³ ë¦¬

í¥ë¯¸ë¡­ê²Œë„, Word2Vec with SGNSëŠ” PMI(Pointwise Mutual Information) í–‰ë ¬ì„ ì•”ì‹œì ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•©ë‹ˆë‹¤:

```python
# PMI ê³„ì‚° ì˜ˆì‹œ
def compute_pmi_matrix(co_occurrence_matrix):
    """ë™ì‹œ ì¶œí˜„ í–‰ë ¬ì—ì„œ PMI ê³„ì‚°"""
    # P(i,j) = co_occurrence[i,j] / total
    # P(i) = sum(co_occurrence[i,:]) / total
    # PMI(i,j) = log(P(i,j) / (P(i) * P(j)))
    
    total = np.sum(co_occurrence_matrix)
    p_i = np.sum(co_occurrence_matrix, axis=1) / total
    p_j = np.sum(co_occurrence_matrix, axis=0) / total
    p_ij = co_occurrence_matrix / total
    
    pmi = np.log(p_ij / (p_i[:, np.newaxis] * p_j[np.newaxis, :]))
    pmi[pmi < 0] = 0  # Positive PMI
    
    return pmi

# Item2Vec â‰ˆ PMI Matrix Factorization
# ì¦‰, ìƒˆë¡œìš´ ê¸°ë²•ë„ ì „í†µì  MFì™€ ì—°ê²°ë˜ì–´ ìˆìŒ
```

## ğŸ”„ Autoencoder ê¸°ë°˜ ì¶”ì²œ: ì••ì¶•ê³¼ ë³µì›ì˜ ë¯¸í•™

### Autoencoderì˜ ê¸°ë³¸ ì›ë¦¬

AutoencoderëŠ” ì…ë ¥ì„ ì••ì¶•í–ˆë‹¤ê°€ ë‹¤ì‹œ ë³µì›í•˜ëŠ” ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ í•µì‹¬ íŠ¹ì§•ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ëŠ” ë§ˆì¹˜ ê·¸ë¦¼ì„ ìŠ¤ì¼€ì¹˜ë¡œ ë‹¨ìˆœí™”í–ˆë‹¤ê°€ ë‹¤ì‹œ ê·¸ë¦¼ìœ¼ë¡œ ë³µì›í•˜ëŠ” ê³¼ì •ê³¼ ê°™ìŠµë‹ˆë‹¤.

```mermaid
graph LR
    subgraph "Encoder (ì••ì¶•)"
        I[ì…ë ¥: ìœ ì € ë²¡í„°] --> H1[Hidden 1]
        H1 --> H2[Hidden 2]
        H2 --> L[Latent Code<br/>ì••ì¶•ëœ í‘œí˜„]
    end
    
    subgraph "Decoder (ë³µì›)"
        L --> D1[Hidden 3]
        D1 --> D2[Hidden 4]
        D2 --> O[ì¶œë ¥: ë³µì›ëœ ë²¡í„°]
    end
    
    style L fill:#f9f,stroke:#333,stroke-width:4px
```

### AutoRec: í˜‘ì—… í•„í„°ë§ì„ ìœ„í•œ Autoencoder

AutoRecì€ ìœ ì €ë‚˜ ì•„ì´í…œì˜ í‰ì  ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë³µì›í•©ë‹ˆë‹¤:

```python
class AutoRec(nn.Module):
    def __init__(self, num_features, hidden_dim=500):
        super(AutoRec, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(num_features, hidden_dim),
            nn.Sigmoid()
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, num_features),
            nn.Identity()  # ë˜ëŠ” nn.Sigmoid()
        )
    
    def forward(self, x):
        # x: ìœ ì €ì˜ í‰ì  ë²¡í„° (ë˜ëŠ” ì•„ì´í…œì˜ í‰ì  ë²¡í„°)
        # ê´€ì¸¡ë˜ì§€ ì•Šì€ í‰ì ì€ 0ìœ¼ë¡œ ì²˜ë¦¬
        
        # ì¸ì½”ë”©: ì••ì¶•
        encoded = self.encoder(x)
        
        # ë””ì½”ë”©: ë³µì›
        decoded = self.decoder(encoded)
        
        return decoded
    
    def loss(self, input_vector, output_vector, mask):
        """ê´€ì¸¡ëœ í‰ì ì— ëŒ€í•´ì„œë§Œ ì†ì‹¤ ê³„ì‚°"""
        # mask: ê´€ì¸¡ëœ í‰ì  ìœ„ì¹˜ëŠ” 1, ì•„ë‹ˆë©´ 0
        mse = (input_vector - output_vector) ** 2
        masked_mse = mse * mask
        return masked_mse.sum() / mask.sum()
```

### Denoising Autoencoder (DAE)

DAEëŠ” ì…ë ¥ì— ì˜ë„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ë” ê°•ê±´í•œ íŠ¹ì§•ì„ í•™ìŠµí•©ë‹ˆë‹¤:

```python
class CollaborativeDAE(nn.Module):
    def __init__(self, num_users, num_items, hidden_dim=200, 
                 corruption_ratio=0.3):
        super(CollaborativeDAE, self).__init__()
        
        self.num_items = num_items
        self.corruption_ratio = corruption_ratio
        
        # ìœ ì €ë³„ bias ì¶”ê°€ (Collaborative ë¶€ë¶„)
        self.user_bias = nn.Embedding(num_users, hidden_dim)
        
        # Encoder
        self.encoder = nn.Linear(num_items, hidden_dim)
        
        # Decoder
        self.decoder = nn.Linear(hidden_dim, num_items)
    
    def add_noise(self, x):
        """ì…ë ¥ì— ë“œë¡­ì•„ì›ƒ ë…¸ì´ì¦ˆ ì¶”ê°€"""
        if self.training:
            mask = torch.rand_like(x) > self.corruption_ratio
            return x * mask.float()
        return x
    
    def forward(self, x, user_id):
        # ë…¸ì´ì¦ˆ ì¶”ê°€
        x_corrupted = self.add_noise(x)
        
        # ì¸ì½”ë”© (ìœ ì € bias ì¶”ê°€)
        hidden = torch.sigmoid(
            self.encoder(x_corrupted) + self.user_bias(user_id)
        )
        
        # ë””ì½”ë”©
        output = self.decoder(hidden)
        
        return output
```

### Variational Autoencoder (VAE)

VAEëŠ” í™•ë¥ ì  ì ‘ê·¼ì„ í†µí•´ ë” í’ë¶€í•œ í‘œí˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤:

```python
class MultVAE(nn.Module):
    """Multinomial VAE for collaborative filtering"""
    def __init__(self, num_items, hidden_dim=[600, 200]):
        super(MultVAE, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(num_items, hidden_dim[0]),
            nn.Tanh(),
            nn.Linear(hidden_dim[0], hidden_dim[1] * 2)  # meanê³¼ logvar
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim[1], hidden_dim[0]),
            nn.Tanh(),
            nn.Linear(hidden_dim[0], num_items)
        )
    
    def encode(self, x):
        h = self.encoder(x)
        # í‰ê· ê³¼ ë¡œê·¸ ë¶„ì‚°ìœ¼ë¡œ ë¶„ë¦¬
        mu, logvar = torch.chunk(h, 2, dim=-1)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        """Reparameterization trick"""
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        return mu
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decoder(z), mu, logvar
    
    def loss(self, recon_x, x, mu, logvar, beta=1.0):
        """ELBO loss with beta annealing"""
        # Reconstruction loss (multinomial likelihood)
        recon_loss = -torch.sum(
            x * torch.log_softmax(recon_x, dim=-1), dim=-1
        ).mean()
        
        # KL divergence
        kld = -0.5 * torch.sum(
            1 + logvar - mu.pow(2) - logvar.exp(), dim=-1
        ).mean()
        
        return recon_loss + beta * kld
```

### EASE: ë‹¨ìˆœí•˜ì§€ë§Œ ê°•ë ¥í•œ ì„ í˜• ëª¨ë¸

EASEëŠ” ë³µì¡í•œ ë”¥ëŸ¬ë‹ êµ¬ì¡° ì—†ì´ë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤:

```python
class EASE:
    """Embarrassingly Shallow Autoencoder"""
    def __init__(self, lambda_reg=500):
        self.lambda_reg = lambda_reg
        self.B = None
    
    def fit(self, X):
        """X: ìœ ì €-ì•„ì´í…œ ìƒí˜¸ì‘ìš© í–‰ë ¬"""
        # Gram matrix ê³„ì‚°
        G = X.T @ X
        
        # ëŒ€ê° ì›ì†Œë¥¼ 0ìœ¼ë¡œ (ìê¸° ìì‹  ì˜ˆì¸¡ ë°©ì§€)
        diag_indices = np.diag_indices(G.shape[0])
        G[diag_indices] += self.lambda_reg
        
        # Closed-form solution
        P = np.linalg.inv(G)
        self.B = P / (-np.diag(P))
        self.B[diag_indices] = 0
        
        return self
    
    def predict(self, X):
        """ì¶”ì²œ ì ìˆ˜ ê³„ì‚°"""
        return X @ self.B
    
    # í•™ìŠµê³¼ ì˜ˆì¸¡ì´ ë§¤ìš° ê°„ë‹¨í•¨!
    ease = EASE(lambda_reg=500)
    ease.fit(train_matrix)
    scores = ease.predict(train_matrix)
```

## ğŸ•¸ï¸ Graph Neural Network: ê´€ê³„ì˜ ë„¤íŠ¸ì›Œí¬

### ì™œ ê·¸ë˜í”„ì¸ê°€?

ì¶”ì²œ ì‹œìŠ¤í…œì˜ ìœ ì €-ì•„ì´í…œ ê´€ê³„ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ê·¸ë˜í”„ êµ¬ì¡°ì…ë‹ˆë‹¤. GNNì€ ì´ëŸ¬í•œ ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```mermaid
graph TD
    subgraph "ì „í†µì  í–‰ë ¬ í‘œí˜„"
        M[ìœ ì €-ì•„ì´í…œ í–‰ë ¬<br/>2D êµ¬ì¡°]
    end
    
    subgraph "ê·¸ë˜í”„ í‘œí˜„"
        U1((ìœ ì €1)) --- I1[ì•„ì´í…œ1]
        U1 --- I2[ì•„ì´í…œ2]
        U2((ìœ ì €2)) --- I2
        U2 --- I3[ì•„ì´í…œ3]
        U3((ìœ ì €3)) --- I3
        U3 --- I4[ì•„ì´í…œ4]
        U4((ìœ ì €4)) --- I3
        U4 --- I5[ì•„ì´í…œ5]
    end
    
    M -.ë³€í™˜.-> U1
```

### NGCF: Neural Graph Collaborative Filtering

NGCFëŠ” ê³ ì°¨ì› ì—°ê²°ì„±ì„ ê³ ë ¤í•˜ì—¬ ì„ë² ë”©ì„ í•™ìŠµí•©ë‹ˆë‹¤:

```python
class NGCF(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim, 
                 num_layers=3):
        super(NGCF, self).__init__()
        
        self.num_users = num_users
        self.num_items = num_items
        self.num_layers = num_layers
        
        # ì´ˆê¸° ì„ë² ë”©
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # ê° ë ˆì´ì–´ë³„ transformation weights
        self.W1 = nn.ModuleList()
        self.W2 = nn.ModuleList()
        
        for l in range(num_layers):
            self.W1.append(nn.Linear(embedding_dim, embedding_dim))
            self.W2.append(nn.Linear(embedding_dim, embedding_dim))
    
    def forward(self, users, items, graph_adj):
        # ì´ˆê¸° ì„ë² ë”©
        user_emb = self.user_embedding(users)
        item_emb = self.item_embedding(items)
        
        # ë©”ì‹œì§€ ì „íŒŒ (Lì¸µ)
        all_embeddings = []
        
        for l in range(self.num_layers):
            # ë©”ì‹œì§€ êµ¬ì„±
            message = self.construct_message(user_emb, item_emb, graph_adj, l)
            
            # ë©”ì‹œì§€ ì§‘ê³„
            user_emb, item_emb = self.aggregate_message(message)
            
            all_embeddings.append((user_emb, item_emb))
        
        # ëª¨ë“  ë ˆì´ì–´ ê²°í•©
        final_user = torch.cat([emb[0] for emb in all_embeddings], dim=-1)
        final_item = torch.cat([emb[1] for emb in all_embeddings], dim=-1)
        
        # ì˜ˆì¸¡
        scores = torch.sum(final_user * final_item, dim=-1)
        return scores
    
    def construct_message(self, user_emb, item_emb, adj, layer):
        """ì´ì›ƒ ë…¸ë“œë¡œë¶€í„° ë©”ì‹œì§€ êµ¬ì„±"""
        # m_ui = (1/sqrt(|N_u||N_i|)) * (W1 * e_i + W2 * (e_i âŠ™ e_u))
        
        W1 = self.W1[layer]
        W2 = self.W2[layer]
        
        # ìê¸° ìì‹  ì •ë³´
        self_msg = W1(item_emb)
        
        # ìƒí˜¸ì‘ìš© ì •ë³´
        interaction_msg = W2(item_emb * user_emb)
        
        # ì •ê·œí™”
        norm_factor = torch.sqrt(
            torch.tensor(len(adj[user]) * len(adj[item]))
        )
        
        message = (self_msg + interaction_msg) / norm_factor
        return message
```

### LightGCN: ë‹¨ìˆœí•¨ì˜ ë¯¸í•™

LightGCNì€ NGCFì˜ ë³µì¡í•œ ì—°ì‚°ì„ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‚¨ê²¼ìŠµë‹ˆë‹¤:

```python
class LightGCN(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim, 
                 num_layers=3, alpha=None):
        super(LightGCN, self).__init__()
        
        self.num_users = num_users
        self.num_items = num_items
        self.num_layers = num_layers
        
        # ì´ˆê¸° ì„ë² ë”©ë§Œ í•™ìŠµ ê°€ëŠ¥
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # Layer combination weights
        if alpha is None:
            # ê· ë“± ê°€ì¤‘ì¹˜
            self.alpha = [1/(num_layers+1)] * (num_layers+1)
        else:
            self.alpha = alpha
    
    def forward(self, users, items, graph):
        # ëª¨ë“  ì„ë² ë”©
        all_emb = torch.cat([
            self.user_embedding.weight,
            self.item_embedding.weight
        ])
        
        embs = [all_emb]
        
        # Light Graph Convolution
        for layer in range(self.num_layers):
            # ë‹¨ìˆœ ì´ì›ƒ ì§‘ê³„ (ê°€ì¤‘ í‰ê· )
            all_emb = torch.sparse.mm(graph, all_emb)
            embs.append(all_emb)
        
        # Layer combination (weighted sum)
        embs = torch.stack(embs, dim=1)
        light_out = torch.sum(
            embs * torch.tensor(self.alpha).view(1, -1, 1),
            dim=1
        )
        
        # ìœ ì €ì™€ ì•„ì´í…œ ì„ë² ë”© ë¶„ë¦¬
        users_emb = light_out[:self.num_users]
        items_emb = light_out[self.num_users:]
        
        # íŠ¹ì • ìœ ì €ì™€ ì•„ì´í…œì˜ ì„ë² ë”© ì¶”ì¶œ
        user_emb = users_emb[users]
        item_emb = items_emb[items]
        
        # ë‚´ì ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
        scores = torch.sum(user_emb * item_emb, dim=-1)
        return scores
```

## ğŸ¯ ì‹¤ì „ í™œìš© ê°€ì´ë“œ

### ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

ê° ëª¨ë¸ì€ íŠ¹ì • ìƒí™©ì—ì„œ ê°•ì ì„ ê°€ì§‘ë‹ˆë‹¤:

```mermaid
graph TD
    Start[ë°ì´í„° íŠ¹ì„± íŒŒì•…] --> Q1{ë°ì´í„°ê°€ í¬ì†Œí•œê°€?}
    
    Q1 -->|Yes| EASE[EASE ì¶”ì²œ<br/>ë‹¨ìˆœí•˜ê³  íš¨ê³¼ì ]
    Q1 -->|No| Q2{ì‹œí€€ìŠ¤ê°€ ì¤‘ìš”í•œê°€?}
    
    Q2 -->|Yes| Item2Vec[Item2Vec<br/>ì„¸ì…˜ ê¸°ë°˜ í•™ìŠµ]
    Q2 -->|No| Q3{ë³µì¡í•œ íŒ¨í„´ì´ ìˆëŠ”ê°€?}
    
    Q3 -->|Yes| Q4{ê´€ê³„ê°€ ì¤‘ìš”í•œê°€?}
    Q3 -->|No| GMF[GMF<br/>ê¸°ë³¸ í˜‘ì—… í•„í„°ë§]
    
    Q4 -->|Yes| GNN[LightGCN<br/>ê·¸ë˜í”„ ê¸°ë°˜]
    Q4 -->|No| NCF[Neural CF<br/>ì„ í˜•+ë¹„ì„ í˜•]
```

### ì„±ëŠ¥ ìµœì í™” íŒ

ë”¥ëŸ¬ë‹ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì‹¤ì „ íŒë“¤ì…ë‹ˆë‹¤:

```python
# 1. ì„ë² ë”© ì´ˆê¸°í™” ìµœì í™”
def init_embeddings(embedding_layer, method='xavier'):
    if method == 'xavier':
        nn.init.xavier_uniform_(embedding_layer.weight)
    elif method == 'he':
        nn.init.kaiming_uniform_(embedding_layer.weight, nonlinearity='relu')
    elif method == 'normal':
        nn.init.normal_(embedding_layer.weight, std=0.01)

# 2. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
from torch.optim.lr_scheduler import ReduceLROnPlateau

scheduler = ReduceLROnPlateau(
    optimizer, 
    mode='min',
    patience=5,
    factor=0.5,
    verbose=True
)

# 3. Early Stopping
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# 4. ë°°ì¹˜ ì •ê·œí™” í™œìš©
class ImprovedMLP(nn.Module):
    def __init__(self, input_dim, hidden_dims, dropout=0.2):
        super(ImprovedMLP, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        self.mlp = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.mlp(x)
```

### í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„

ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì •í™•íˆ í‰ê°€í•˜ê¸° ìœ„í•œ ë©”íŠ¸ë¦­ë“¤ì…ë‹ˆë‹¤:

```python
def evaluate_model(model, test_loader, top_k=10):
    """ì¶”ì²œ ëª¨ë¸ í‰ê°€"""
    model.eval()
    
    precisions = []
    recalls = []
    ndcgs = []
    
    with torch.no_grad():
        for users, items, ratings in test_loader:
            # ì˜ˆì¸¡
            predictions = model(users, items)
            
            # Top-K ì¶”ì²œ
            _, top_indices = torch.topk(predictions, k=top_k)
            
            # Precision@K
            relevant = (ratings[top_indices] > 3.5).float()
            precision = relevant.sum() / top_k
            precisions.append(precision)
            
            # Recall@K
            total_relevant = (ratings > 3.5).sum()
            if total_relevant > 0:
                recall = relevant.sum() / total_relevant
                recalls.append(recall)
            
            # NDCG@K
            dcg = torch.sum(
                relevant / torch.log2(torch.arange(2, top_k+2).float())
            )
            idcg = torch.sum(
                torch.ones(min(top_k, total_relevant.item())) / 
                torch.log2(torch.arange(2, min(top_k, total_relevant.item())+2).float())
            )
            if idcg > 0:
                ndcg = dcg / idcg
                ndcgs.append(ndcg)
    
    return {
        'precision@k': np.mean(precisions),
        'recall@k': np.mean(recalls),
        'ndcg@k': np.mean(ndcgs)
    }
```

## ğŸš€ ì‹¤ìŠµ í”„ë¡œì íŠ¸: ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•

MovieLens ë°ì´í„°ì…‹ì„ í™œìš©í•œ ì™„ì „í•œ ì¶”ì²œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë´…ì‹œë‹¤:

```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

# 1. ë°ì´í„° ì¤€ë¹„
class MovieLensDataset(Dataset):
    def __init__(self, ratings_file):
        self.ratings = pd.read_csv(ratings_file)
        
        # ìœ ì €ì™€ ì•„ì´í…œ ID ë§¤í•‘
        self.user_ids = self.ratings['userId'].unique()
        self.item_ids = self.ratings['movieId'].unique()
        
        self.user2idx = {u: i for i, u in enumerate(self.user_ids)}
        self.item2idx = {i: j for j, i in enumerate(self.item_ids)}
        
        # ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        self.ratings['user_idx'] = self.ratings['userId'].map(self.user2idx)
        self.ratings['item_idx'] = self.ratings['movieId'].map(self.item2idx)
    
    def __len__(self):
        return len(self.ratings)
    
    def __getitem__(self, idx):
        row = self.ratings.iloc[idx]
        return (
            torch.tensor(row['user_idx'], dtype=torch.long),
            torch.tensor(row['item_idx'], dtype=torch.long),
            torch.tensor(row['rating'], dtype=torch.float)
        )

# 2. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
def train_model(model, train_loader, val_loader, epochs=50):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0
        for users, items, ratings in train_loader:
            optimizer.zero_grad()
            predictions = model(users, items)
            loss = criterion(predictions, ratings)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for users, items, ratings in val_loader:
                predictions = model(users, items)
                loss = criterion(predictions, ratings)
                val_loss += loss.item()
        
        train_losses.append(train_loss / len(train_loader))
        val_losses.append(val_loss / len(val_loader))
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, "
                  f"Val Loss = {val_losses[-1]:.4f}")
    
    return train_losses, val_losses

# 3. ì „ì²´ íŒŒì´í”„ë¼ì¸
def build_recommendation_system():
    # ë°ì´í„° ë¡œë“œ
    dataset = MovieLensDataset('ratings.csv')
    
    # Train/Test ë¶„í• 
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    
    # DataLoader ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    num_users = len(dataset.user_ids)
    num_items = len(dataset.item_ids)
    
    model = NeuralMF(
        num_users=num_users,
        num_items=num_items,
        factor_num=64,
        num_layers=3,
        dropout=0.2
    )
    
    # í•™ìŠµ
    train_losses, val_losses = train_model(
        model, train_loader, val_loader, epochs=50
    )
    
    # í‰ê°€
    metrics = evaluate_model(model, val_loader, top_k=10)
    print(f"\nFinal Performance:")
    print(f"Precision@10: {metrics['precision@k']:.4f}")
    print(f"Recall@10: {metrics['recall@k']:.4f}")
    print(f"NDCG@10: {metrics['ndcg@k']:.4f}")
    
    return model, dataset

# ì‹¤í–‰
model, dataset = build_recommendation_system()
```

## ğŸ’¡ ë§ºìŒë§

ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œì€ ì „í†µì ì¸ ë°©ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ë” ì •êµí•œ ê°œì¸í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. Neural CFëŠ” ì„ í˜•ê³¼ ë¹„ì„ í˜•ì„ ê²°í•©í•˜ê³ , Item2Vecì€ ì„¸ì…˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ë©°, AutoencoderëŠ” ì ì¬ íŒ¨í„´ì„ í•™ìŠµí•˜ê³ , GNNì€ ê´€ê³„ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì§ì ‘ ëª¨ë¸ë§í•©ë‹ˆë‹¤.

ì¤‘ìš”í•œ ê²ƒì€ ê° ë°©ë²•ì´ ë§ŒëŠ¥ì´ ì•„ë‹ˆë¼ëŠ” ì ì…ë‹ˆë‹¤. ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ì„ ì„ íƒí•˜ê³ , ë•Œë¡œëŠ” ë‹¨ìˆœí•œ EASE ê°™ì€ ëª¨ë¸ì´ ë³µì¡í•œ ë”¥ëŸ¬ë‹ë³´ë‹¤ ë‚˜ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•´ì•¼ í•©ë‹ˆë‹¤.

ì¶”ì²œ ì‹œìŠ¤í…œì˜ ë¯¸ë˜ëŠ” ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì˜ í™œìš©, ì‹¤ì‹œê°„ í•™ìŠµ, ê·¸ë¦¬ê³  ì„¤ëª… ê°€ëŠ¥í•œ AIë¡œ ë‚˜ì•„ê°€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì—¬ì •ì—ì„œ ë”¥ëŸ¬ë‹ì€ ê°•ë ¥í•œ ë„êµ¬ì´ì§€ë§Œ, ë°ì´í„°ì˜ í’ˆì§ˆê³¼ ë„ë©”ì¸ ì§€ì‹ì´ ì—¬ì „íˆ ê°€ì¥ ì¤‘ìš”í•œ ì„±ê³µ ìš”ì†Œì„ì„ ìŠì§€ ë§ì•„ì•¼ í•©ë‹ˆë‹¤.