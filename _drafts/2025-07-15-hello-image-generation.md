---
title: ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì€ ì–´ë–»ê²Œ ë°œì „í•´ì™”ì„ê¹Œ?
date: 2025-07-15 17:58:00 +0900
categories: 
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

- torch==2.1.0
- numpy==1.24.3
- matplotlib==3.7.1
- scipy==1.11.1
- scikit-learn==1.3.0

## ğŸš€ TL;DR

- **ìƒì„± ëª¨ë¸**ì€ ì£¼ì–´ì§„ ë°ì´í„°ì˜ í™•ë¥  ë¶„í¬ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
- **íŒë³„ ëª¨ë¸**ì€ ê²°ì • ê²½ê³„ë¥¼ í•™ìŠµí•˜ê³ , **ìƒì„± ëª¨ë¸**ì€ ë°ì´í„° ë¶„í¬ ìì²´ë¥¼ í•™ìŠµí•œë‹¤ëŠ” ê·¼ë³¸ì  ì°¨ì´
- **VAE(2014)**, **GAN(2014)**, **Diffusion Model(2015)** ë“±ì´ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ìƒì„± ëª¨ë¸ì˜ í•µì‹¬ ê¸°ë²•
- **ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •(MLE)** ê³¼ **KL Divergence** ê°€ ìƒì„± ëª¨ë¸ í•™ìŠµì˜ ì´ë¡ ì  ê¸°ë°˜
- ì´ë¯¸ì§€ ìƒì„±, í™”ì§ˆ ê°œì„ , AI í”„ë¡œí•„, í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë³€í™˜ ë“± ë‹¤ì–‘í•œ ì‹¤ìƒí™œ ì‘ìš© ì‚¬ë¡€ ì¡´ì¬
- í˜„ì¬ëŠ” **Stable Diffusion**, **Midjourney** ë“±ìœ¼ë¡œ ê°œì¸ë„ ì‰½ê²Œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„± ê°€ëŠ¥

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [ìƒì„± ëª¨ë¸ ê¸°ì´ˆ ì´ë¡ ê³¼ êµ¬í˜„](https://github.com/yuiyeong/notebooks/blob/main/deep_learning/generative_models.ipynb)

## ğŸ” ìƒì„± ëª¨ë¸(Generative Model)ì´ë€?

### ì–¸ì–´ì  í‘œí˜„ (Linguistic Expression)

ìƒì„± ëª¨ë¸ì€ ì£¼ì–´ì§„ í•™ìŠµ ë°ì´í„°ì…‹ì˜ **í™•ë¥  ë¶„í¬ë¥¼ í•™ìŠµ**í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ì¦‰, ì£¼ì–´ì§„ ë°ì´í„°ê°€ ì–´ë–¤ ë¶„í¬ë¥¼ ê°–ê³  ìˆëŠ”ì§€ í•™ìŠµí•˜ë¯€ë¡œ, í•´ë‹¹ ëª¨ë¸ë¡œë¶€í„° ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆê²Œ ëœë‹¤.

ìƒì„± ëª¨ë¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **ë³µì¡í•œ ë°ì´í„°ê°€ ì €ì°¨ì›ì˜ í•„ìˆ˜ì ì¸ ì •ë³´ì˜ ì¡°í•©ìœ¼ë¡œ ìƒì„±ë  ìˆ˜ ìˆë‹¤**ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ëŒ ì–¼êµ´ ì‚¬ì§„ì€ "ì„±ì¸", "ë‚¨ì„±", "ì•ˆê²½", "ì•ë¨¸ë¦¬ ì—†ëŠ” í—¤ì–´ìŠ¤íƒ€ì¼" ë“±ì˜ í•„ìˆ˜ì ì¸ ì •ë³´ë¡œ ê¸°ìˆ í•  ìˆ˜ ìˆë‹¤.

**"ë³µì¡í•œ ë°ì´í„°ê°€ ì—¬ëŸ¬ ê°„ë‹¨í•œ íŠ¹ì„±ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤" ì˜ ì˜ˆì‹œ)**

```mermaid
graph TD
    subgraph "ì €ì°¨ì› íŠ¹ì„±"
        L1["ì§ì„  1<br/>ê°ë„: 0Â°<br/>ê¸¸ì´: ì§§ìŒ"]
        L2["ì§ì„  2<br/>ê°ë„: 30Â°<br/>ê¸¸ì´: ì¤‘ê°„"]
        L3["ì§ì„  3<br/>ê°ë„: 60Â°<br/>ê¸¸ì´: ì§§ìŒ"]
        L4["ì§ì„  4<br/>ê°ë„: 90Â°<br/>ê¸¸ì´: ì¤‘ê°„"]
        L5["ì§ì„  5<br/>ê°ë„: 120Â°<br/>ê¸¸ì´: ì§§ìŒ"]
        L6["ì§ì„  6<br/>ê°ë„: 150Â°<br/>ê¸¸ì´: ì¤‘ê°„"]
        L7["...ë” ë§ì€ ì§ì„ ë“¤"]
    end
    
    subgraph "ì¡°í•© ê³¼ì •"
        C1["íŠ¹ì„± ì¡°í•© 1<br/>L1 + L2 + L3"]
        C2["íŠ¹ì„± ì¡°í•© 2<br/>L4 + L5 + L6"]
        C3["ì „ì²´ ì¡°í•© ëª¨ë“  ì§ì„ ì˜ ê°€ì¤‘í•©"]
    end
    
    subgraph "ê³ ì°¨ì› ê²°ê³¼"
        R1["ë¶€ë¶„ì  í˜¸(Partial Arc)"]
        R2["ë” í° í˜¸(Larger Arc)"]
        R3["ì™„ì „í•œ ì›(Complete Circle)"]
    end
    
    L1 --> C1
    L2 --> C1
    L3 --> C1
    L4 --> C2
    L5 --> C2
    L6 --> C2
    L7 --> C3
    
    C1 --> R1
    C2 --> R2
    C1 --> C3
    C2 --> C3
    C3 --> R3
    
    style L1 fill:#e1f5fe
    style L2 fill:#e1f5fe
    style L3 fill:#e1f5fe
    style L4 fill:#e1f5fe
    style L5 fill:#e1f5fe
    style L6 fill:#e1f5fe
    style L7 fill:#e1f5fe
    style C3 fill:#fff3e0
    style R3 fill:#e8f5e8
```

```mermaid
flowchart LR

subgraph "Step 1: ê¸°ë³¸ ì§ì„ ë“¤"

S1["| (ìˆ˜ì§ì„ )"]

S2["â€” (ìˆ˜í‰ì„ )"]

S3["/ (ëŒ€ê°ì„  1)"]

S4["\ (ëŒ€ê°ì„  2)"]

end

subgraph "Step 2: 4ê°œ ì§ì„  ì¡°í•©"

C1["ê±°ì¹œ ì‚¬ê°í˜•<br/>â–¡"]

end

subgraph "Step 3: 8ê°œ ì§ì„  ì¡°í•©"

C2["8ê°í˜•<br/>â¬Ÿ"]

end

subgraph "Step 4: 16ê°œ ì§ì„  ì¡°í•©"

C3["16ê°í˜•<br/>(ë” ë‘¥ê·¼ í˜•íƒœ)"]

end

subgraph "Step 5: ë¬´í•œíˆ ë§ì€ ì§ì„ "

C4["ì™„ë²½í•œ ì›<br/>â—"]

end

S1 --> C1

S2 --> C1

S3 --> C1

S4 --> C1

C1 --> C2

C2 --> C3

C3 --> C4

style S1 fill:#ffebee

style S2 fill:#ffebee

style S3 fill:#ffebee

style S4 fill:#ffebee

style C1 fill:#fff3e0

style C2 fill:#f1f8e9

style C3 fill:#e8f5e8

style C4 fill:#e0f2f1
```

### ìˆ˜í•™ì /ì´ë¡ ì  í‘œí˜„ (Mathematical Expression)

ìƒì„± ëª¨ë¸ì€ ë°ì´í„° $$X$$ ì™€ íŠ¹ì„± $$Y$$ì˜ **ê²°í•© ë¶„í¬** $$P(X,Y)$$ ë˜ëŠ” ì¡°ê±´ë¶€ ë¶„í¬ $$P(X|Y)$$ ë¥¼ í•™ìŠµí•œë‹¤.

ë ˆì´ë¸” $$Y$$ ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” ë°ì´í„°ì˜ **ì£¼ë³€ ë¶„í¬(Marginal Distribution)** $$P(X)$$ ë¥¼ ì§ì ‘ í•™ìŠµí•œë‹¤.

$$P(X) = \int P(X|Y)P(Y)dY$$
## ğŸ¯ í™•ë¥  ë¶„í¬ì˜ ì¢…ë¥˜ì™€ ì˜ë¯¸

### ê²°í•© ë¶„í¬ (Joint Distribution) **P(X,Y)**

**ê²°í•© ë¶„í¬**ëŠ” ë‘ ê°œ ì´ìƒì˜ í™•ë¥  ë³€ìˆ˜ê°€ **ë™ì‹œì— íŠ¹ì • ê°’ì„ ê°€ì§ˆ í™•ë¥ **ì„ ë‚˜íƒ€ë‚¸ë‹¤.

```
P(X,Y) = "ë°ì´í„° Xì™€ ë ˆì´ë¸” Yê°€ ë™ì‹œì— ë°œìƒí•  í™•ë¥ "
```

**ì˜ˆì‹œ**

- X: ì´ë¯¸ì§€ ë°ì´í„° (28Ã—28 í”½ì…€)
- Y: ìˆ«ì ë ˆì´ë¸” (0~9)
- P(X,Y): "íŠ¹ì • ì´ë¯¸ì§€ì™€ íŠ¹ì • ìˆ«ìê°€ í•¨ê»˜ ë‚˜íƒ€ë‚  í™•ë¥ "

###  ì¡°ê±´ë¶€ ë¶„í¬ (Conditional Distribution) **P(X|Y)**

**ì¡°ê±´ë¶€ ë¶„í¬**ëŠ” íŠ¹ì • ì¡°ê±´(Y)ì´ ì£¼ì–´ì¡Œì„ ë•Œ Xê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” í™•ë¥  ë¶„í¬ì´ë‹¤.

```
P(X|Y) = "ë ˆì´ë¸” Yê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë°ì´í„° Xì˜ í™•ë¥  ë¶„í¬"
```

**ìƒì„± ëª¨ë¸ì—ì„œì˜ ì˜ë¯¸**

- **"ìˆ«ì 7ì´ë¼ëŠ” ì¡°ê±´ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì–´ë–¤ ì´ë¯¸ì§€ë“¤ì´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ”ê°€?"**
- ì¡°ê±´ë¶€ ìƒì„±: ì›í•˜ëŠ” í´ë˜ìŠ¤ì˜ ë°ì´í„°ë¥¼ ìƒì„±

### ì£¼ë³€ ë¶„í¬ (Marginal Distribution) **P(X)**

**ì£¼ë³€ ë¶„í¬**ëŠ” ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ ëª¨ë‘ **ì ë¶„(ë˜ëŠ” í•©)ìœ¼ë¡œ ì œê±°**í•˜ê³  ë‚¨ì€ í•˜ë‚˜ì˜ ë³€ìˆ˜ì˜ ë¶„í¬ì´ë‹¤.

```
P(X) = "ë ˆì´ë¸”ì— ìƒê´€ì—†ì´ ë°ì´í„° X ìì²´ì˜ ë¶„í¬"
```

**í•µì‹¬ ê³µì‹**
$$P(X) = \int P(X|Y)P(Y)dY$$

ì´ëŠ” **ì „í™•ë¥  ë²•ì¹™(Law of Total Probability)** ì´ë‹¤.

## ğŸ¨ ê³ ì „ì  ìƒì„± ëª¨ë¸ë“¤

### ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ (Gaussian Mixture Model, GMM)

- ì—¬ëŸ¬ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ì¡°í•©í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ë¶„í¬ì— ê·¼ì‚¬
- ê° ê°€ìš°ì‹œì•ˆì˜ í‰ê· , ë¶„ì‚°, ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµ
- 1981ë…„ì— ë°œí‘œ

### ì œí•œëœ ë³¼ì¸ ë§Œ ë¨¸ì‹  (Restricted Boltzmann Machine, RBM)

- ì‹ ê²½ë§ ê¸°ë°˜ì˜ ìƒì„± ëª¨ë¸
- ë³¼ì¸ ë§Œ ë¶„í¬ì— ë”°ë¼ ì—ë„ˆì§€ê°€ ë‚®ì„ìˆ˜ë¡ í™•ë¥  ë°€ë„ê°€ ë†’ì•„ì§€ëŠ” ì›ë¦¬ í™œìš©
- 1985ë…„ì— ë°œí‘œ

### ìê¸°íšŒê·€ ë¶„í¬ ì¶”ì • (Auto-Regressive Distribution Estimator)

- í˜„ì¬ í”½ì…€ ê°’ì„ ì´ì „ í”½ì…€ ê°’ë“¤ì— ì˜ì¡´í•˜ì—¬ ì¶”ì •
- ë§ˆë¥´ì½”í”„ ì²´ì¸ ê°€ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆœì°¨ì  ìƒì„±
- 2011ë…„ì— ë°œí‘œ

> ìƒì„± ëª¨ë¸ì€ ë‹¨ìˆœíˆ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ê²ƒë¿ë§Œ ì•„ë‹ˆë¼, ë°ì´í„°ì˜ ë‚´ì¬ëœ êµ¬ì¡°ì™€ íŒ¨í„´ì„ ì´í•´í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.
{: .prompt-tip}

### ë§ˆë¥´ì½”í”„ ì²´ì¸ (Markov Chain) ì´ë¡ 

**"í˜„ì¬ ìƒíƒœê°€ ë¯¸ë˜ë¥¼ ê²°ì •í•˜ëŠ” ë° ì¶©ë¶„í•˜ë‹¤"**ëŠ” **ë§ˆë¥´ì½”í”„ ì„±ì§ˆ(Markov Property)**ì„ ë§Œì¡±í•˜ëŠ” í™•ë¥  ê³¼ì •ì´ë‹¤.

**ë§ˆë¥´ì½”í”„ ì„±ì§ˆ**

$$P(X_{t+1}|X_t, X_{t-1}, ..., X_1) = P(X_{t+1}|X_t)$$

"ë¯¸ë˜ëŠ” ê³¼ê±°ì™€ ë…ë¦½ì ì´ê³ , ì˜¤ì§ í˜„ì¬ ìƒíƒœì—ë§Œ ì˜ì¡´í•œë‹¤."

**ë§ˆë¥´ì½”í”„ ì²´ì¸ì˜ í•µì‹¬ ì„±ì§ˆ**

- ì‹œê°„ ë™ì§ˆì„± (Time Homogeneity): ì „ì´ í™•ë¥ ì´ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ì§€ ì•ŠìŒ
- ì •ìƒ ë¶„í¬ (Stationary Distribution): ì¶©ë¶„íˆ ì˜¤ëœ ì‹œê°„ í›„ ë„ë‹¬í•˜ëŠ” í‰í˜• ìƒíƒœ

> ë§ˆë¥´ì½”í”„ ì²´ì¸ì€ ìƒì„± ëª¨ë¸ì—ì„œ **ìˆœì°¨ì  ìƒì„± ê³¼ì •**ì„ ëª¨ë¸ë§í•˜ëŠ” í•µì‹¬ ë„êµ¬ë‹¤. "í˜„ì¬ ìƒíƒœë§Œ ì•Œë©´ ë‹¤ìŒì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤"ëŠ” ì„±ì§ˆ ë•ë¶„ì— ê³„ì‚°ìƒ íš¨ìœ¨ì ì´ë©´ì„œë„ ë³µì¡í•œ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤.
{: .prompt-tip}

## âš”ï¸ íŒë³„ ëª¨ë¸ vs ìƒì„± ëª¨ë¸

### íŒë³„ ëª¨ë¸ (Discriminative Model)

íŒë³„ ëª¨ë¸ì€ ë°ì´í„° $X$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ íŠ¹ì„± $Y$ê°€ ë‚˜íƒ€ë‚  **ì¡°ê±´ë¶€ í™•ë¥ ** $P(Y|X)$ë¥¼ ì§ì ‘ ë°˜í™˜í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

**íŠ¹ì§•**

- **ê²°ì • ê²½ê³„(Decision Boundary)** ë¥¼ í•™ìŠµ
- ë‘ í´ë˜ìŠ¤ ê°„ì˜ ê°€ì¥ ë„ë“œë¼ì§€ëŠ” íŠ¹ì„±ë§Œ í•™ìŠµí•˜ë©´ ì¶©ë¶„
- í•™ìŠµì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰½ê³  ë¹ ë¦„
- ê°ê´€ì ì¸ í‰ê°€ ì§€í‘œ(ì •í™•ë„, F1-score ë“±) ì¡´ì¬

### ìƒì„± ëª¨ë¸ (Generative Model)

ìƒì„± ëª¨ë¸ì€ ë°ì´í„°ì˜ **ë¶„í¬ ìì²´**ë¥¼ í•™ìŠµí•œë‹¤.

**íŠ¹ì§•**

- ê° í´ë˜ìŠ¤ê°€ **ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€** ëª¨ë“  íŠ¹ì„±ì„ í•™ìŠµ
- ë³µì¡í•˜ê²Œ ì–½íŒ ë¶„í¬ë¥¼ í•™ìŠµí•´ì•¼ í•´ì„œ ë” ì–´ë ¤ì›€
- í‰ê°€ ì§€í‘œê°€ ì£¼ê´€ì ì´ê³  ëª¨í˜¸í•¨
- ìƒˆë¡œìš´ ë°ì´í„° ìƒì„± ê°€ëŠ¥

### ë¹„êµ ì˜ˆì‹œ: ìˆ«ì 0ê³¼ 1 êµ¬ë¶„

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# íŒë³„ ëª¨ë¸ ì˜ˆì‹œ - ê°„ë‹¨í•œ ë¶„ë¥˜ê¸°
class DiscriminativeModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(784, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.classifier(x.view(-1, 784))

# ìƒì„± ëª¨ë¸ ì˜ˆì‹œ - ê°„ë‹¨í•œ ìƒì„±ê¸°
class GenerativeModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.generator = nn.Sequential(
            nn.Linear(10, 128),
            nn.ReLU(),
            nn.Linear(128, 784),
            nn.Sigmoid()
        )
    
    def forward(self, z):
        return self.generator(z).view(-1, 1, 28, 28)

# íŒë³„ ëª¨ë¸: "1ì€ ì„¸ë¡œì„ ì´ ë§ë‹¤, 0ì€ ë™ê·¸ë—ë‹¤"ë§Œ í•™ìŠµ
discriminative = DiscriminativeModel()

# ìƒì„± ëª¨ë¸: "0ì´ ì–¼ë§ˆë‚˜ ë™ê·¸ë—ê³ , ì–´ë–»ê²Œ ë¶„í¬ë˜ëŠ”ì§€" ì „ì²´ë¥¼ í•™ìŠµ
generative = GenerativeModel()

print("íŒë³„ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜:", sum(p.numel() for p in discriminative.parameters()))
print("ìƒì„± ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜:", sum(p.numel() for p in generative.parameters()))
# íŒë³„ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: 100609
# ìƒì„± ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: 101770
```

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: íŒë³„ ëª¨ë¸ì˜ ê²°ì • ê²½ê³„ vs ìƒì„± ëª¨ë¸ì˜ ë¶„í¬ í•™ìŠµ ë¹„êµ ë‹¤ì´ì–´ê·¸ë¨]

> **í•µì‹¬ ì°¨ì´ì **: íŒë³„ ëª¨ë¸ì€ "êµ¬ë¶„í•˜ëŠ” ë²•"ì„ ë°°ìš°ê³ , ìƒì„± ëª¨ë¸ì€ "ë§Œë“œëŠ” ë²•"ì„ ë°°ìš´ë‹¤. ìƒì„± ëª¨ë¸ì´ ë” ì–´ë µì§€ë§Œ, í•™ìŠµì— ì„±ê³µí•˜ë©´ íŒë³„ë„ ê°€ëŠ¥í•˜ê³  ìƒˆë¡œìš´ ë°ì´í„°ë„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. {: .prompt-tip}

## ğŸš€ ë”¥ëŸ¬ë‹ ì‹œëŒ€ì˜ ìƒì„± ëª¨ë¸ í˜ëª…

### 2013ë…„ ì´ì „: ê³ ì „ì  ì ‘ê·¼ë²•ì˜ í•œê³„

2012ë…„ ImageNet ë¶„ë¥˜ê¸°ì˜ ì„±ê³µì€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì— ë”¥ëŸ¬ë‹ ì—´í’ì„ ì¼ìœ¼ì¼°ê³ , ì´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„± ëª¨ë¸ë¡œë„ í™•ì‚°ë˜ì—ˆë‹¤.

### 2014ë…„: í˜ì‹ ì˜ ì›ë…„

**2014ë…„**ì€ ìƒì„± ëª¨ë¸ ì—­ì‚¬ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í•´ë‹¤. ì´ í•´ì— ë‘ ê°œì˜ íšê¸°ì ì¸ ëª¨ë¸ì´ ë°œí‘œë˜ì—ˆë‹¤:

- **Variational Auto Encoder (VAE)**: ì•ˆì •ì  í•™ìŠµê³¼ ë¹ ë¥¸ ìƒ˜í”Œë§ ì†ë„
- **Generative Adversarial Networks (GAN)**: ë‘ ë„¤íŠ¸ì›Œí¬ì˜ ê²½ìŸì  í•™ìŠµ

### 2015-2016ë…„: ë‹¤ì–‘ì„±ì˜ í™•ì¥

- **PixelRNN/PixelCNN**: ìê¸°íšŒê·€ ëª¨ë¸ì˜ ë°œì „í˜•
- **Normalizing Flow**: ì—­ì‚° ê°€ëŠ¥í•œ ì ì§„ì  í™•ë¥  ë¶„í¬ ë³€í™”
- **Diffusion Model**: í˜„ì¬ íŒ¨ëŸ¬ë‹¤ì„ì„ ì£¼ë„í•˜ëŠ” ëª¨ë¸ì˜ ì›í˜•

```python
# ìƒì„± ëª¨ë¸ ë°œì „ ê³¼ì • ì‹œê°í™”
import matplotlib.pyplot as plt
import numpy as np

years = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]
quality_scores = [2, 4, 5, 6, 7, 8, 8.5, 9, 9.5, 9.8, 9.9]  # ê°€ìƒì˜ í’ˆì§ˆ ì ìˆ˜

plt.figure(figsize=(12, 6))
plt.plot(years, quality_scores, 'bo-', linewidth=2, markersize=8)
plt.axvline(x=2014, color='red', linestyle='--', alpha=0.7, label='VAE & GAN ë“±ì¥')
plt.axvline(x=2020, color='green', linestyle='--', alpha=0.7, label='Diffusion ì¬ë“±ì¥')
plt.xlabel('ë…„ë„')
plt.ylabel('ìƒì„± í’ˆì§ˆ (ìƒëŒ€ì )')
plt.title('ìƒì„± ëª¨ë¸ì˜ ë°œì „ ê³¼ì •')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: VAE, GAN, Diffusion ëª¨ë¸ì˜ ìƒì„± ê²°ê³¼ ë¹„êµ ì´ë¯¸ì§€]

## ğŸ¯ VAE (Variational Auto Encoder)

### í•µì‹¬ ê°œë…

VAEëŠ” **ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°**ì™€ **ë³€ë¶„ ì¶”ë¡ (Variational Inference)**ì„ ê²°í•©í•œ ëª¨ë¸ì´ë‹¤.

**ì£¼ìš” íŠ¹ì§•:**

- ì•ˆì •ì ì¸ í•™ìŠµê³¼ ë¹ ë¥¸ ìƒ˜í”Œë§ ì†ë„
- ì ì§„ì  ë³€í™” ì œì–´ ê°€ëŠ¥
- ì´ˆê¸°ì—ëŠ” íë¦° ì˜ìƒ ìƒì„± ë¬¸ì œ ì¡´ì¬

### ìˆ˜í•™ì  í‘œí˜„

VAEì˜ ëª©ì  í•¨ìˆ˜ëŠ” **Evidence Lower BOund (ELBO)**ë¥¼ ìµœëŒ€í™”í•œë‹¤:

$$\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) | p(z))$$

ì—¬ê¸°ì„œ:

- $q_\phi(z|x)$: ì¸ì½”ë” (recognition model)
- $p_\theta(x|z)$: ë””ì½”ë” (generative model)
- $D_{KL}$: KL divergence

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=256, latent_dim=20):
        super(VAE, self).__init__()
        
        # ì¸ì½”ë”
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # ì ì¬ ë¶„í¬ íŒŒë¼ë¯¸í„°
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        
        # ë””ì½”ë”
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )
    
    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar

# VAE ì†ì‹¤ í•¨ìˆ˜
def vae_loss(recon_x, x, mu, logvar):
    # ì¬êµ¬ì„± ì†ì‹¤ (Reconstruction Loss)
    recon_loss = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    
    # KL divergence ì†ì‹¤
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    return recon_loss + kl_loss

# ëª¨ë¸ ì´ˆê¸°í™”
vae = VAE()
print(f"VAE ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in vae.parameters()):,}")
# VAE ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜: 1,149,033
```

## âš”ï¸ GAN (Generative Adversarial Networks)

### í•µì‹¬ ê°œë…

GANì€ **ìƒì„±ì(Generator)**ì™€ **íŒë³„ì(Discriminator)** ë‘ ë„¤íŠ¸ì›Œí¬ê°€ **ì ëŒ€ì ìœ¼ë¡œ ê²½ìŸ**í•˜ë©° í•™ìŠµí•˜ëŠ” ëª¨ë¸ì´ë‹¤.

**ì£¼ìš” íŠ¹ì§•:**

- ëª…ì‹œì  í™•ë¥  ë¶„í¬ ì •ì˜ ë¶ˆí•„ìš”
- ì„ ëª…í•œ ì˜ìƒ ìƒì„± ê°€ëŠ¥
- ì ì¬ ê³µê°„ì—ì„œ ì‚°ìˆ  ì—°ì‚° ê°€ëŠ¥
- í•™ìŠµ ë¶ˆì•ˆì •ì„± ë¬¸ì œ ì¡´ì¬

### ìˆ˜í•™ì  í‘œí˜„

GANì˜ ëª©ì  í•¨ìˆ˜ëŠ” **minimax ê²Œì„**ìœ¼ë¡œ í‘œí˜„ëœë‹¤:

$$\min_G \max_D V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1-D(G(z)))]$$

```python
import torch
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_shape=(1, 28, 28)):
        super(Generator, self).__init__()
        self.img_shape = img_shape
        
        def block(in_feat, out_feat, normalize=True):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers
        
        self.model = nn.Sequential(
            *block(latent_dim, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            nn.Linear(1024, int(np.prod(img_shape))),
            nn.Tanh()
        )
    
    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), *self.img_shape)
        return img

class Discriminator(nn.Module):
    def __init__(self, img_shape=(1, 28, 28)):
        super(Discriminator, self).__init__()
        
        self.model = nn.Sequential(
            nn.Linear(int(np.prod(img_shape)), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity

# GAN ì†ì‹¤ í•¨ìˆ˜
adversarial_loss = nn.BCELoss()

# ëª¨ë¸ ì´ˆê¸°í™”
generator = Generator()
discriminator = Discriminator()

print(f"Generator íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in generator.parameters()):,}")
print(f"Discriminator íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in discriminator.parameters()):,}")
# Generator íŒŒë¼ë¯¸í„° ìˆ˜: 1,493,324
# Discriminator íŒŒë¼ë¯¸í„° ìˆ˜: 533,505
```

### GANì˜ ì ì¬ ê³µê°„ ì‚°ìˆ  ì—°ì‚°

GANì˜ í¥ë¯¸ë¡œìš´ íŠ¹ì„± ì¤‘ í•˜ë‚˜ëŠ” ì ì¬ ê³µê°„ì—ì„œ **ë²¡í„° ì‚°ìˆ  ì—°ì‚°**ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤:

**ì•ˆê²½ ì“´ ë‚¨ì - ë‚¨ì + ì—¬ì = ì•ˆê²½ ì“´ ì—¬ì**

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: GAN ì ì¬ ê³µê°„ ì‚°ìˆ  ì—°ì‚° ì˜ˆì‹œ]

## ğŸŒŠ Diffusion Model

### í•µì‹¬ ê°œë…

Diffusion Modelì€ **ë¹„í‰í˜• í†µê³„ì—­í•™** ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒì„± ëª¨ë¸ë¡œ, 2015ë…„ì— ì œì•ˆë˜ì—ˆì§€ë§Œ 2020ë…„ê²½ë¶€í„° ì£¼ëª©ë°›ê¸° ì‹œì‘í–ˆë‹¤.

**ë™ì‘ ì›ë¦¬:**

1. **Forward Process**: ë°ì´í„°ì— ì ì§„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì¶”ê°€
2. **Reverse Process**: ë…¸ì´ì¦ˆë¡œë¶€í„° ì ì§„ì ìœ¼ë¡œ ë°ì´í„° ë³µì›

### ìˆ˜í•™ì  í‘œí˜„

**Forward Process (ë…¸ì´ì¦ˆ ì¶”ê°€):** $$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$

**Reverse Process (ë…¸ì´ì¦ˆ ì œê±°):** $$p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$$

```python
import torch
import torch.nn as nn
import math

class DiffusionModel(nn.Module):
    def __init__(self, img_size=28, in_channels=1, time_dim=256):
        super().__init__()
        self.time_dim = time_dim
        
        # ì‹œê°„ ì„ë² ë”©
        self.time_embed = nn.Sequential(
            nn.Linear(time_dim, time_dim),
            nn.ReLU(),
            nn.Linear(time_dim, time_dim)
        )
        
        # U-Net êµ¬ì¡° (ê°„ì†Œí™”)
        self.down1 = nn.Sequential(
            nn.Conv2d(in_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU()
        )
        
        self.down2 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU()
        )
        
        self.up1 = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU()
        )
        
        self.final = nn.Conv2d(64, in_channels, 1)
    
    def time_encoding(self, t, dim):
        """Sinusoidal time encoding"""
        half_dim = dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim) * -emb)
        emb = t[:, None] * emb[None, :]
        emb = torch.cat([emb.sin(), emb.cos()], dim=1)
        return emb
    
    def forward(self, x, t):
        # ì‹œê°„ ì¸ì½”ë”©
        t_emb = self.time_encoding(t, self.time_dim)
        t_emb = self.time_embed(t_emb)
        
        # U-Net forward pass (ì‹œê°„ ì •ë³´ í†µí•© ìƒëµ)
        h1 = self.down1(x)
        h2 = self.down2(h1)
        h3 = self.up1(h2)
        
        return self.final(h3)

# ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§
def linear_beta_schedule(timesteps, start=0.0001, end=0.02):
    return torch.linspace(start, end, timesteps)

# Diffusion í•™ìŠµ ê³¼ì •
def diffusion_loss(model, x_0, t, noise=None):
    if noise is None:
        noise = torch.randn_like(x_0)
    
    # Forward processë¡œ ë…¸ì´ì¦ˆ ì¶”ê°€
    betas = linear_beta_schedule(1000)
    alphas = 1. - betas
    alphas_cumprod = torch.cumprod(alphas, dim=0)
    
    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)
    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)
    
    # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì´ë¯¸ì§€
    x_t = sqrt_alphas_cumprod[t] * x_0 + sqrt_one_minus_alphas_cumprod[t] * noise
    
    # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë…¸ì´ì¦ˆ
    predicted_noise = model(x_t, t)
    
    # ì‹¤ì œ ë…¸ì´ì¦ˆì™€ì˜ ì°¨ì´
    loss = nn.MSELoss()(predicted_noise, noise)
    return loss

diffusion_model = DiffusionModel()
print(f"Diffusion Model íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in diffusion_model.parameters()):,}")
# Diffusion Model íŒŒë¼ë¯¸í„° ìˆ˜: 327,169
```

> **Diffusion Modelì˜ í˜ì‹ **: 2020ë…„ ì´í›„ ê¸°ì¡´ GANì„ ë›°ì–´ë„˜ëŠ” í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë©°, Stable Diffusion, DALL-E 2 ë“±ì˜ ê¸°ìˆ ì  í† ëŒ€ê°€ ë˜ì—ˆë‹¤. {: .prompt-tip}

## ğŸ’¼ ìƒì„± ëª¨ë¸ì˜ ì‹¤ìƒí™œ í™œìš© ì‚¬ë¡€

### ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ 

**í™”ì§ˆ ê°œì„  (Super Resolution)**

- ì €í™”ì§ˆ ì˜ìƒì„ ê³ í™”ì§ˆë¡œ ë³€í™˜
- TVì˜ AI ì—…ìŠ¤ì¼€ì¼ë§ ê¸°ëŠ¥
- ì˜¤ë˜ëœ ì‚¬ì§„ ë³µêµ¬ ë° ì»¬ëŸ¬ë¼ì´ì œì´ì…˜

```python
# í™”ì§ˆ ê°œì„  ëª¨ë¸ ì˜ˆì‹œ (ESRGAN ìŠ¤íƒ€ì¼)
import torch.nn.functional as F

class SuperResolutionGAN(nn.Module):
    def __init__(self, scale_factor=4):
        super().__init__()
        self.scale_factor = scale_factor
        
        # Feature extraction
        self.conv1 = nn.Conv2d(3, 64, 9, padding=4)
        
        # Residual blocks
        self.res_blocks = nn.Sequential(*[
            self._make_residual_block(64) for _ in range(16)
        ])
        
        # Upsampling
        self.upconv1 = nn.Conv2d(64, 256, 3, padding=1)
        self.upconv2 = nn.Conv2d(64, 256, 3, padding=1)
        self.conv_out = nn.Conv2d(64, 3, 9, padding=4)
    
    def _make_residual_block(self, channels):
        return nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.PReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels)
        )
    
    def forward(self, x):
        feat = F.relu(self.conv1(x))
        res = self.res_blocks(feat)
        
        # Pixel shuffle for upsampling
        up1 = F.pixel_shuffle(F.relu(self.upconv1(res)), 2)
        up2 = F.pixel_shuffle(F.relu(self.upconv2(up1)), 2)
        
        return torch.tanh(self.conv_out(up2))
```

### ê°œì¸í™” ì„œë¹„ìŠ¤

**AI í”„ë¡œí•„ ìƒì„±**

- ëª‡ ì¥ì˜ ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ í”„ë¡œí•„ ìƒì„±
- 5ì¼ ë§Œì— 100ì–µ ìˆ˜ìµì„ ì˜¬ë¦° ë¹„ì¦ˆë‹ˆìŠ¤ ì‚¬ë¡€

**ê°€ìƒ ì‹œì°© (Virtual Try-On)**

- ì˜·ì„ ì‹¤ì œë¡œ ì…ì§€ ì•Šê³ ë„ ì°©ìš© ëª¨ìŠµ í™•ì¸
- ë¬¼ë¦¬ì  ë³€í˜•ê³¼ ê·¸ë¦¼ì íš¨ê³¼ê¹Œì§€ ê³ ë ¤

### ì½˜í…ì¸  ìƒì„± ë„êµ¬

**í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±**

- **Midjourney**: "Space opera theater" í…ìŠ¤íŠ¸ë¡œ ë¯¸ìˆ  ëŒ€íšŒ 1ë“± ìˆ˜ìƒì‘ ìƒì„±
- **DALL-E 2**: ì°½ì˜ì ì´ê³  ì˜ˆìˆ ì ì¸ ì´ë¯¸ì§€ ìƒì„±
- **Stable Diffusion**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê°œì¸ë„ ì‰½ê²Œ ì ‘ê·¼ ê°€ëŠ¥

**ë™ì˜ìƒ ìƒì„± ë° ë³€í™˜**

- **RunwayML Gen-2**: í…ìŠ¤íŠ¸ë¡œ ë™ì˜ìƒ ìƒì„±
- **ë™ì‘ ì „ì´**: ì •ì  ì‚¬ì§„ì„ ì¶¤ì¶”ëŠ” ë™ì˜ìƒìœ¼ë¡œ ë³€í™˜
- **SadTalker**: í•œ ì¥ì˜ ì‚¬ì§„ê³¼ ìŒì„±ìœ¼ë¡œ ë§í•˜ëŠ” ì˜ìƒ ìƒì„±

```python
# í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
from diffusers import StableDiffusionPipeline
import torch

# Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)

# GPU ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
if torch.cuda.is_available():
    pipe = pipe.to("cuda")

# í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
prompt = "A cute cat wearing a wizard hat, digital art, highly detailed"
image = pipe(prompt).images[0]

# ì´ë¯¸ì§€ ì €ì¥
image.save("generated_cat_wizard.png")
print("ì´ë¯¸ì§€ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
```

### ìŒì„± ë° ë©€í‹°ë¯¸ë””ì–´

**AI ë”ë¹™ (ElevenLabs)**

- ë¹„ë””ì˜¤ì˜ ìŒì„±ì„ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ê³  ì…ëª¨ì–‘ê¹Œì§€ ë™ê¸°í™”
- ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ê¹Œì§€ ìƒì„±í•˜ëŠ” ê°€ìƒ ì•„ë°”íƒ€

**AI ìŒì•… ìƒì„±**

- ê¸°ì¡´ ê°€ìˆ˜ì˜ ëª©ì†Œë¦¬ì™€ ìŠ¤íƒ€ì¼ì„ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ê³¡ ìƒì„±
- ëª©ì†Œë¦¬ ë³€í™˜ ê¸°ìˆ ë¡œ í†¤, í”¼ì¹˜, ìŠµê´€ê¹Œì§€ ì¬í˜„

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: ë‹¤ì–‘í•œ ìƒì„± ëª¨ë¸ í™œìš© ì‚¬ë¡€ ì¸í¬ê·¸ë˜í”½]

> **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**: ìƒì„± ëª¨ë¸ì€ ë‹¨ìˆœí•œ ê¸°ìˆ  ë°ëª¨ë¥¼ ë„˜ì–´ ì‹¤ì œ ìˆ˜ìµì„ ì°½ì¶œí•˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ë¡œ ì„±ì¥í–ˆë‹¤. AI í”„ë¡œí•„ ì„œë¹„ìŠ¤ê°€ 5ì¼ ë§Œì— 100ì–µì„ ë²Œì–´ë“¤ì¸ ì‚¬ë¡€ëŠ” ì´ ê¸°ìˆ ì˜ ìƒì—…ì  ê°€ì¹˜ë¥¼ ë³´ì—¬ì¤€ë‹¤. {: .prompt-warning}

## ğŸ“Š ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì • (Maximum Likelihood Estimation)

### ì–¸ì–´ì  í‘œí˜„

ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •(MLE)ì€ ê´€ì¸¡ëœ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ë°©ë²•ì´ë‹¤. **"ì´ ë°ì´í„°ê°€ ê´€ì¸¡ë  ê°€ëŠ¥ì„±(Likelihood)ì„ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì"**ëŠ” ì§ê´€ì ì¸ ì•„ì´ë””ì–´ì— ê¸°ë°˜í•œë‹¤.

### ìˆ˜í•™ì  í‘œí˜„

ì£¼ì–´ì§„ ë°ì´í„° $\mathcal{D} = {x^{(1)}, x^{(2)}, ..., x^{(n)}}$ì— ëŒ€í•´ ê°€ëŠ¥ë„(Likelihood)ëŠ”:

$$L(\theta) = \prod_{i=1}^{n} p(x^{(i)}|\theta)$$

ê³„ì‚°ì˜ í¸ì˜ë¥¼ ìœ„í•´ **ë¡œê·¸ ê°€ëŠ¥ë„(Log-Likelihood)**ë¥¼ ì‚¬ìš©:

$$\log L(\theta) = \sum_{i=1}^{n} \log p(x^{(i)}|\theta)$$

ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •ì€ ì´ë¥¼ ìµœëŒ€í™”í•˜ëŠ” $\theta$ë¥¼ ì°¾ëŠ” ê²ƒ:

$$\hat{\theta}_{MLE} = \arg\max_\theta \log L(\theta)$$

### ë™ì „ ë˜ì§€ê¸° ì˜ˆì‹œ

ë™ì „ì„ 10ë²ˆ ë˜ì ¸ì„œ ì•ë©´ 7ë²ˆ, ë’·ë©´ 3ë²ˆì´ ë‚˜ì™”ì„ ë•Œ ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥  $\theta$ë¥¼ ì¶”ì •í•´ë³´ì.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize_scalar

# ë™ì „ ë˜ì§€ê¸° ë°ì´í„° (1: ì•ë©´, 0: ë’·ë©´)
coin_flips = [1, 1, 0, 1, 1, 1, 0, 1, 0, 1]  # ì•ë©´ 7ê°œ, ë’·ë©´ 3ê°œ
heads = sum(coin_flips)
tails = len(coin_flips) - heads

print(f"ì•ë©´: {heads}ê°œ, ë’·ë©´: {tails}ê°œ")

# ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜ ì •ì˜
def log_likelihood(theta, heads, tails):
    if theta <= 0 or theta >= 1:
        return -np.inf
    return heads * np.log(theta) + tails * np.log(1 - theta)

# ë‹¤ì–‘í•œ theta ê°’ì— ëŒ€í•œ ë¡œê·¸ ê°€ëŠ¥ë„ ê³„ì‚°
theta_values = np.linspace(0.01, 0.99, 100)
log_likelihoods = [log_likelihood(theta, heads, tails) for theta in theta_values]

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.plot(theta_values, log_likelihoods, 'b-', linewidth=2)
plt.axvline(x=0.7, color='red', linestyle='--', label='Î¸ = 0.7 (ì§ê´€ì  ë‹µ)')
plt.xlabel('Î¸ (ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ )')
plt.ylabel('ë¡œê·¸ ê°€ëŠ¥ë„')
plt.title('ë™ì „ ë˜ì§€ê¸° ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# í•´ì„ì  í•´ (MLE)
mle_theta = heads / len(coin_flips)
print(f"MLE ì¶”ì •ê°’: Î¸ = {mle_theta}")

# ì•ë©´: 7ê°œ, ë’·ë©´: 3ê°œ
# MLE ì¶”ì •ê°’: Î¸ = 0.7
```

> **ì§ê´€ê³¼ ìˆ˜í•™ì˜ ì¼ì¹˜**: ë™ì „ì„ 10ë²ˆ ë˜ì ¸ ì•ë©´ì´ 7ë²ˆ ë‚˜ì™”ë‹¤ë©´, ì§ê´€ì ìœ¼ë¡œ ì•ë©´ í™•ë¥ ì„ 70%ë¡œ ì¶”ì •í•  ê²ƒì´ë‹¤. MLEëŠ” ì´ëŸ¬í•œ ì§ê´€ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì •ë‹¹í™”í•´ì¤€ë‹¤. {: .prompt-tip}

## ğŸ“ KL Divergenceì™€ ë¶„í¬ ê°„ ê±°ë¦¬

### KL Divergence ì •ì˜

**ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°(Kullback-Leibler Divergence)**ëŠ” ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œë‹¤:

$$D_{KL}(P | Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$$

**íŠ¹ì„±:**

- í•­ìƒ 0 ì´ìƒì˜ ê°’
- $P = Q$ì¼ ë•Œë§Œ 0
- ëŒ€ì¹­ì„±ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŒ ($D_{KL}(P | Q) \neq D_{KL}(Q | P)$)

### KL Divergenceì™€ MLEì˜ ê´€ê³„

ìƒì„± ëª¨ë¸ í•™ìŠµì—ì„œ KL Divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒê³¼ MLEëŠ” ë™ì¹˜ë‹¤:

$$\min_\theta D_{KL}(P_{data} | P_\theta) = \min_\theta \left[ -\mathbb{E}_{x \sim P_{data}}[\log P_\theta(x)] + \text{const} \right]$$

$$= \max_\theta \mathbb{E}_{x \sim P_{data}}[\log P_\theta(x)] = \max_\theta \log L(\theta)$$

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import rel_entr

# ë‘ ë¶„í¬ ì •ì˜
x = np.arange(0, 10)
p = np.array([0.1, 0.15, 0.2, 0.25, 0.15, 0.1, 0.03, 0.01, 0.005, 0.005])  # ì‹¤ì œ ë¶„í¬
q1 = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])  # ê· ë“± ë¶„í¬
q2 = np.array([0.08, 0.12, 0.18, 0.22, 0.18, 0.12, 0.05, 0.03, 0.01, 0.01])  # ê°œì„ ëœ ë¶„í¬

# KL Divergence ê³„ì‚°
kl_div1 = np.sum(rel_entr(p, q1))
kl_div2 = np.sum(rel_entr(p, q2))

print(f"KL(P || Q1): {kl_div1:.4f}")
print(f"KL(P || Q2): {kl_div2:.4f}")

# ì‹œê°í™”
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))

ax1.bar(x, p, alpha=0.7, label='ì‹¤ì œ ë¶„í¬ P')
ax1.set_title('ì‹¤ì œ ë¶„í¬ P')
ax1.set_ylim(0, 0.3)

ax2.bar(x, q1, alpha=0.7, color='orange', label='ëª¨ë¸ ë¶„í¬ Q1 (ê· ë“±)')
ax2.set_title(f'ëª¨ë¸ Q1\nKL divergence: {kl_div1:.4f}')
ax2.set_ylim(0, 0.3)

ax3.bar(x, q2, alpha=0.7, color='green', label='ëª¨ë¸ ë¶„í¬ Q2 (ê°œì„ )')
ax3.set_title(f'ëª¨ë¸ Q2\nKL divergence: {kl_div2:.4f}')
ax3.set_ylim(0, 0.3)

plt.tight_layout()
plt.show()

# KL(P || Q1): 0.3443
# KL(P || Q2): 0.0261
```

### ì‹¤ì œ ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì œ

ì‹¤ì œ ìƒì„± ëª¨ë¸ í•™ìŠµì—ì„œëŠ” **ë°ì´í„°ì˜ ì§„ì§œ ë¶„í¬ $P_{data}$ë¥¼ ëª¨ë¥¸ë‹¤**ëŠ” ê·¼ë³¸ì ì¸ ë¬¸ì œê°€ ìˆë‹¤. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê±´ ë°”ë¡œ ê·¸ ë¶„í¬ì¸ë°, KL divergence ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” ê·¸ ë¶„í¬ë¥¼ ì•Œì•„ì•¼ í•˜ëŠ” ëª¨ìˆœì  ìƒí™©ì´ë‹¤.

**í•´ê²°ì±…:**

- **ê²½í—˜ì  ë¶„í¬(Empirical Distribution)** ì‚¬ìš©
- **ë³€ë¶„ ì¶”ë¡ (Variational Inference)**
- **ì ëŒ€ì  í•™ìŠµ(Adversarial Training)**
- **ì ìˆ˜ ë§¤ì¹­(Score Matching)**

> **ì´ë¡ ê³¼ ì‹¤ì œì˜ ê°„ê·¹**: MLEì™€ KL divergenceëŠ” ìƒì„± ëª¨ë¸ì˜ ì´ë¡ ì  ê¸°ë°˜ì„ ì œê³µí•˜ì§€ë§Œ, ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì§„ì§œ ë°ì´í„° ë¶„í¬ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ ê·¼ì‚¬ ë°©ë²•ì´ í•„ìš”í•˜ë‹¤. {: .prompt-warning}

## ğŸ”® ìƒì„± ëª¨ë¸ì˜ ë¯¸ë˜ì™€ ë„ì „ê³¼ì œ

### í˜„ì¬ì˜ í•œê³„ì 

**í’ˆì§ˆ í‰ê°€ì˜ ì–´ë ¤ì›€**

- ê°ê´€ì  í‰ê°€ ì§€í‘œ ë¶€ì¡±
- ì‚¬ëŒë§ˆë‹¤ ë‹¤ë¥¸ í’ˆì§ˆ ê¸°ì¤€
- FID, IS ë“±ì˜ ì§€í‘œë„ ì™„ì „í•˜ì§€ ì•ŠìŒ

**ê³„ì‚° ë¹„ìš©ê³¼ íš¨ìœ¨ì„±**

- ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ë†’ì€ í•™ìŠµ ë¹„ìš©
- ì‹¤ì‹œê°„ ìƒì„±ì˜ ì–´ë ¤ì›€
- ì—ë„ˆì§€ ì†Œë¹„ ë¬¸ì œ

**ì œì–´ ê°€ëŠ¥ì„±ê³¼ ì¼ê´€ì„±**

- ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì •í™•íˆ ìƒì„±í•˜ê¸° ì–´ë ¤ì›€
- ê¸´ ì‹œí€€ìŠ¤ì—ì„œì˜ ì¼ê´€ì„± ìœ ì§€ ë¬¸ì œ

### ë¯¸ë˜ ë°œì „ ë°©í–¥

**ë©€í‹°ëª¨ë‹¬ í†µí•©**

- í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ë¥¼ ì•„ìš°ë¥´ëŠ” í†µí•© ëª¨ë¸
- 3D ëª¨ë¸ê³¼ ê°€ìƒ í˜„ì‹¤ ì½˜í…ì¸  ìƒì„±

**ê°œì¸í™”ì™€ ë§ì¶¤í˜• ìƒì„±**

- ê°œì¸ì˜ ì·¨í–¥ê³¼ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•œ ìƒì„±
- ì ì€ ë°ì´í„°ë¡œë„ ë¹ ë¥¸ ì ì‘ ê°€ëŠ¥í•œ ëª¨ë¸

**ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©**

- ì‹¤ì‹œê°„ í”¼ë“œë°±ì„ í†µí•œ ë°˜ë³µì  ìƒì„±
- ì‚¬ìš©ìì™€ì˜ í˜‘ì—…ì  ì°½ì‘ ë„êµ¬

```python
# ë¯¸ë˜ì˜ ë©€í‹°ëª¨ë‹¬ ìƒì„± ëª¨ë¸ ê°œë… (ì˜ì‚¬ì½”ë“œ)
class MultiModalGenerativeModel(nn.Module):
    def __init__(self):
        super().__init__()
        # í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ì¸ì½”ë”
        self.text_encoder = TextEncoder()
        self.image_encoder = ImageEncoder()
        self.audio_encoder = AudioEncoder()
        
        # í†µí•© ì ì¬ ê³µê°„
        self.unified_latent_space = UnifiedLatentSpace()
        
        # ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹° ë””ì½”ë”
        self.image_decoder = ImageDecoder()
        self.video_decoder = VideoDecoder()
        self.audio_decoder = AudioDecoder()
        self.3d_decoder = ThreeDDecoder()
    
    def generate(self, prompt, modality='image', style=None, user_preference=None):
        # í”„ë¡¬í”„íŠ¸ë¥¼ ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©
        latent = self.encode_prompt(prompt)
        
        # ì‚¬ìš©ì ì„ í˜¸ë„ì™€ ìŠ¤íƒ€ì¼ ë°˜ì˜
        if user_preference:
            latent = self.apply_user_preference(latent, user_preference)
        
        # ì›í•˜ëŠ” ëª¨ë‹¬ë¦¬í‹°ë¡œ ìƒì„±
        if modality == 'image':
            return self.image_decoder(latent)
        elif modality == 'video':
            return self.video_decoder(latent)
        elif modality == '3d':
            return self.3d_decoder(latent)
        # ... ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ë“¤
```

> **ìƒì„± ëª¨ë¸ì˜ ë¯¸ë˜**: ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ìƒì„±ì„ ë„˜ì–´ ê°œì¸í™”ëœ ë©€í‹°ëª¨ë‹¬ ì°½ì‘ ë„êµ¬ë¡œ ë°œì „í•  ê²ƒì´ë‹¤. ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ê³ í’ˆì§ˆ ì½˜í…ì¸ ë¥¼ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì‹œëŒ€ê°€ ë‹¤ê°€ì˜¤ê³  ìˆë‹¤. {: .prompt-tip}

## ğŸ“ˆ ìƒì„± ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì‹¤ì „ íŒ

### ë°ì´í„° ì¤€ë¹„

**ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ êµ¬ì¶•**

- ì¼ê´€ëœ í•´ìƒë„ì™€ í’ˆì§ˆ
- ë‹¤ì–‘ì„±ê³¼ ê· í˜•ì„± í™•ë³´
- ì ì ˆí•œ ì „ì²˜ë¦¬ì™€ ì •ê·œí™”

**ë°ì´í„° ì¦ê°• ê¸°ë²•**

- íšŒì „, í¬ê¸° ë³€ê²½, ìƒ‰ìƒ ì¡°ì •
- ë…¸ì´ì¦ˆ ì¶”ê°€ì™€ ë¸”ëŸ¬ ì²˜ë¦¬
- ë„ë©”ì¸ë³„ íŠ¹í™” ì¦ê°• ê¸°ë²•

```python
import torchvision.transforms as transforms

# ìƒì„± ëª¨ë¸ìš© ë°ì´í„° ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
])

# ë°ì´í„°ë¡œë” ì„¤ì •
from torch.utils.data import DataLoader

dataloader = DataLoader(
    dataset,
    batch_size=64,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)
```

### ëª¨ë¸ í›ˆë ¨ ì „ëµ

**ì ì§„ì  í•™ìŠµ (Progressive Training)**

- ë‚®ì€ í•´ìƒë„ë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
- ì•ˆì •ì ì¸ í•™ìŠµê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

**í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§**

- Cosine annealingì´ë‚˜ step decay ì‚¬ìš©
- Warmup ë‹¨ê³„ë¡œ ì•ˆì •ì ì¸ ì´ˆê¸° í•™ìŠµ

**ì •ê·œí™” ê¸°ë²•**

- Batch Normalization ëŒ€ì‹  Layer Normalization ê³ ë ¤
- Spectral Normalizationìœ¼ë¡œ í•™ìŠµ ì•ˆì •ì„± ê°œì„ 

```python
# ìƒì„± ëª¨ë¸ í›ˆë ¨ ë£¨í”„ ì˜ˆì‹œ
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
scheduler_G = CosineAnnealingLR(optimizer_G, T_max=100)
scheduler_D = CosineAnnealingLR(optimizer_D, T_max=100)

# í›ˆë ¨ ë£¨í”„
for epoch in range(num_epochs):
    for batch_idx, real_imgs in enumerate(dataloader):
        # Discriminator í›ˆë ¨
        optimizer_D.zero_grad()
        
        # ì‹¤ì œ ì´ë¯¸ì§€ì— ëŒ€í•œ ì†ì‹¤
        real_validity = discriminator(real_imgs)
        real_loss = adversarial_loss(real_validity, torch.ones_like(real_validity))
        
        # ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ì†ì‹¤
        z = torch.randn(batch_size, latent_dim)
        fake_imgs = generator(z)
        fake_validity = discriminator(fake_imgs.detach())
        fake_loss = adversarial_loss(fake_validity, torch.zeros_like(fake_validity))
        
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        optimizer_D.step()
        
        # Generator í›ˆë ¨
        optimizer_G.zero_grad()
        
        fake_validity = discriminator(fake_imgs)
        g_loss = adversarial_loss(fake_validity, torch.ones_like(fake_validity))
        
        g_loss.backward()
        optimizer_G.step()
        
        if batch_idx % 100 == 0:
            print(f'Epoch [{epoch}/{num_epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}')
    
    scheduler_G.step()
    scheduler_D.step()
```

ìƒì„± ëª¨ë¸ì€ í˜„ì¬ AI ë¶„ì•¼ì—ì„œ ê°€ì¥ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆëŠ” ì˜ì—­ ì¤‘ í•˜ë‚˜ë‹¤. ì´ë¡ ì  ê¸°ë°˜ë¶€í„° ì‹¤ì œ ì‘ìš©ê¹Œì§€ í­ë„“ì€ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì—¬ëŸ¬ë¶„ë„ ì°½ì˜ì ì¸ AI í”„ë¡œë•íŠ¸ë¥¼ ë§Œë“¤ì–´ë³´ê¸¸ ê¸°ëŒ€í•œë‹¤.