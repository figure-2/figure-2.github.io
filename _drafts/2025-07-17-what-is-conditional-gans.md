---
title: ì¡°ê±´ë¶€ gan
date: 2025-07-17 13:26:00 +0900
categories: [ ]
tags: [ "ê¸‰ë°œì§„ê±°ë¶ì´" ]
toc: true
comments: false
mermaid: true
math: true
---
# ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

- torch==2.0.0+
- torchvision==0.15.0+
- numpy==1.24.0+
- matplotlib==3.7.0+
- Pillow==9.5.0+
- clip-by-openai==1.0

## ğŸš€ TL;DR

- **ì¡°ê±´ë¶€ GANs**ëŠ” ë‹¨ìˆœíˆ ëœë¤ ë…¸ì´ì¦ˆì—ì„œ ìƒì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **íŠ¹ì • ì¡°ê±´**ì„ ì…ë ¥ë°›ì•„ **ì›í•˜ëŠ” ì˜ë¯¸ì˜ ë°ì´í„°**ë¥¼ ìƒì„±í•˜ëŠ” ìƒì„± ëª¨ë¸
- **Pix2Pix**ëŠ” í˜ì–´ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ëŒ€ ì´ë¯¸ì§€ ë³€í™˜ì˜ ì´ˆê¸° ì„±ê³µ ì‚¬ë¡€ë¡œ **UNet êµ¬ì¡°**ì™€ **ì ëŒ€ì  í•™ìŠµ**ì„ ê²°í•©
- **CycleGAN**ì€ í˜ì–´ ë°ì´í„° ì—†ì´ë„ **Cycle Consistency** ì›ë¦¬ë¥¼ í†µí•´ ë‘ ë„ë©”ì¸ ê°„ ë³€í™˜ì„ ê°€ëŠ¥í•˜ê²Œ í•¨
- **ACGAN**, **StarGAN** ë“±ì€ ë¶„ë¥˜ ì†ì‹¤ê³¼ ë‹¤ì¤‘ ë„ë©”ì¸ ë³€í™˜ì„ í†µí•´ ì¡°ê±´ë¶€ ìƒì„±ì˜ í’ˆì§ˆê³¼ ë²”ìœ„ë¥¼ í™•ì¥
- **í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±**ì€ ìì—°ì–´ ì„¤ëª…ìœ¼ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ìˆ ë¡œ ë†’ì€ ììœ ë„ë¥¼ ì œê³µí•˜ì§€ë§Œ í•™ìŠµ ë‚œì´ë„ê°€ ë†’ìŒ
- **GigaGAN**ì€ ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ í”¼ë¼ë¯¸ë“œ êµ¬ì¡°ë¥¼ í†µí•´ ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ê³¼ ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ë™ì‹œì— ë‹¬ì„±

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [Conditional GANs Tutorial](https://github.com/yuiyeong/notebooks/blob/main/deep_learning/conditional_gans.ipynb)

## ğŸ¯ ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸(Conditional Generation)ì´ë€?

ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ì€ **íŠ¹ì • ì¡°ê±´(condition)**ì„ ì…ë ¥ë°›ì•„ ê·¸ ì¡°ê±´ì— ë§ëŠ” **ì›í•˜ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ ë°ì´í„°**ë¥¼ ìƒì„±í•˜ëŠ” ìƒì„± ëª¨ë¸ì´ë‹¤.

ê¸°ì¡´ì˜ ìƒì„± ëª¨ë¸ì´ ë‹¨ìˆœíˆ ë°ì´í„° ë¶„í¬ë§Œ í•™ìŠµí•˜ì—¬ ë¬´ì‘ìœ„ë¡œ ìƒì„±ë¬¼ì„ ë§Œë“¤ì–´ëƒˆë‹¤ë©´, ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ì€ ì‚¬ìš©ìê°€ **ìƒì„± ê²°ê³¼ë¥¼ ì œì–´**í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.

> ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ì˜ í•µì‹¬ì€ "ë‚´ê°€ ì›í•˜ëŠ” ê²ƒì„ ë§Œë“¤ì–´ ë‹¬ë¼"ê³  ëª…ë ¹í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "ìˆ«ì 7ì„ ê·¸ë ¤ì¤˜", "ë§ì„ ì–¼ë£©ë§ë¡œ ë°”ê¿”ì¤˜", "ë¶„í™ìƒ‰ ê½ƒì´ ìˆëŠ” í’ê²½ì„ ê·¸ë ¤ì¤˜"ì™€ ê°™ì€ êµ¬ì²´ì ì¸ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•  ìˆ˜ ìˆë‹¤. {: .prompt-tip}

### ì™œ ì¡°ê±´ë¶€ ìƒì„±ì´ ì¤‘ìš”í•œê°€?

- **ë°ì´í„° ì¦ê°•**: íŠ¹ì • í´ë˜ìŠ¤ì˜ ë°ì´í„°ê°€ ë¶€ì¡±í•  ë•Œ í•´ë‹¹ í´ë˜ìŠ¤ë§Œ ê³¨ë¼ì„œ ìƒì„±í•  ìˆ˜ ìˆì–´ íŒë³„ ëª¨ë¸ í•™ìŠµì— í™œìš©
- **ì´ë¯¸ì§€ í¸ì§‘**: ì‚¬ìš©ìì˜ ì˜ë„ì— ë§ê²Œ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ë³€í™˜
- **ì°½ì‘ ë„êµ¬**: í…ìŠ¤íŠ¸ ì„¤ëª…ë§Œìœ¼ë¡œ ì›í•˜ëŠ” ì´ë¯¸ì§€ë‚˜ ì½˜í…ì¸  ìƒì„±
- **ì‹¤ìš©ì  ì‘ìš©**: ìŠ¤ì¼€ì¹˜ë¥¼ ì‹¤ì‚¬ë¡œ ë³€í™˜, í‘ë°± ì´ë¯¸ì§€ë¥¼ ì»¬ëŸ¬ë¡œ ë³€í™˜ ë“±

## ğŸ—ï¸ ì¡°ê±´ë¶€ GANì˜ ê¸°ë³¸ êµ¬ì¡°

ì¡°ê±´ë¶€ GAN(Conditional GAN)ì€ ê¸°ì¡´ GANì— ì¡°ê±´ ì •ë³´ë¥¼ ì¶”ê°€í•œ ëª¨ë¸ì´ë‹¤.

### ìˆ˜í•™ì  í‘œí˜„

ê¸°ì¡´ GANì˜ ëª©ì  í•¨ìˆ˜ì— ì¡°ê±´ **y**ê°€ ì¶”ê°€ëœë‹¤:

$$ \min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x|y)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z|y)))] $$

ì—¬ê¸°ì„œ **y**ëŠ” ë²”ì£¼, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ì¡°ê±´ì´ ë  ìˆ˜ ìˆë‹¤.

### ê¸°ë³¸ êµ¬í˜„ ë°©ì‹

ê°€ì¥ ê°„ë‹¨í•œ ì¡°ê±´ë¶€ GANì€ ì¡°ê±´ì„ **ì›-í•« ë²¡í„°**ë‚˜ **ì„ë² ë”© ë²¡í„°**ë¡œ ë³€í™˜í•˜ì—¬ ì…ë ¥ì— ì—°ê²°(concatenate)í•˜ëŠ” ë°©ì‹ì´ë‹¤.

```python
import torch
import torch.nn as nn

class ConditionalGenerator(nn.Module):
    def __init__(self, noise_dim=100, num_classes=10, embed_dim=100):
        super(ConditionalGenerator, self).__init__()
        
        # ì¡°ê±´(í´ë˜ìŠ¤)ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        self.label_embedding = nn.Embedding(num_classes, embed_dim)
        
        # ë…¸ì´ì¦ˆì™€ ì¡°ê±´ì„ ê²°í•©í•œ ì…ë ¥ ì°¨ì›
        input_dim = noise_dim + embed_dim
        
        self.model = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512), 
            nn.ReLU(),
            nn.Linear(512, 784),  # 28x28 ì´ë¯¸ì§€
            nn.Tanh()
        )
    
    def forward(self, noise, labels):
        # ë¼ë²¨ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        embedded_labels = self.label_embedding(labels)
        
        # ë…¸ì´ì¦ˆì™€ ì¡°ê±´ì„ ê²°í•©
        combined_input = torch.cat([noise, embedded_labels], dim=1)
        
        return self.model(combined_input)

class ConditionalDiscriminator(nn.Module):
    def __init__(self, num_classes=10, embed_dim=100):
        super(ConditionalDiscriminator, self).__init__()
        
        self.label_embedding = nn.Embedding(num_classes, embed_dim)
        
        # ì´ë¯¸ì§€ì™€ ì¡°ê±´ì„ ê²°í•©í•œ ì…ë ¥ ì°¨ì›
        input_dim = 784 + embed_dim
        
        self.model = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(), 
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, images, labels):
        # ì´ë¯¸ì§€ë¥¼ í‰ë©´í™”
        flattened_images = images.view(images.size(0), -1)
        
        # ë¼ë²¨ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        embedded_labels = self.label_embedding(labels)
        
        # ì´ë¯¸ì§€ì™€ ì¡°ê±´ì„ ê²°í•©
        combined_input = torch.cat([flattened_images, embedded_labels], dim=1)
        
        return self.model(combined_input)

# ì‚¬ìš© ì˜ˆì‹œ
generator = ConditionalGenerator()
discriminator = ConditionalDiscriminator()

# ë°°ì¹˜ í¬ê¸° 32, ë…¸ì´ì¦ˆ ì°¨ì› 100
noise = torch.randn(32, 100)
labels = torch.randint(0, 10, (32,))  # 0-9 í´ë˜ìŠ¤

# ì¡°ê±´ë¶€ ìƒì„±
fake_images = generator(noise, labels)
print(f"ìƒì„±ëœ ì´ë¯¸ì§€ í¬ê¸°: {fake_images.shape}")  # torch.Size([32, 784])

# íŒë³„
real_images = torch.randn(32, 784)
real_output = discriminator(real_images, labels)
fake_output = discriminator(fake_images.detach(), labels)
```

## ğŸ­ ê³ ê¸‰ ì¡°ê±´ë¶€ GAN: ACGAN

ACGAN(Auxiliary Classifier GAN)ì€ ì¡°ê±´ë¶€ GANì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ ëª¨ë¸ì´ë‹¤.

### ACGANì˜ í•µì‹¬ ì•„ì´ë””ì–´

ê¸°ì¡´ ì¡°ê±´ë¶€ GANì—ì„œ íŒë³„ìëŠ” "ì§„ì§œ/ê°€ì§œ" + "ì¡°ê±´ ì¼ì¹˜ ì—¬ë¶€"ë¥¼ ë™ì‹œì— íŒë‹¨í–ˆì§€ë§Œ, ACGANì—ì„œëŠ” **ë¶„ë¥˜ ë¬¸ì œë¥¼ ë³„ë„ë¡œ í•´ê²°**í•œë‹¤.

```python
class ACGANDiscriminator(nn.Module):
    def __init__(self, num_classes=10):
        super(ACGANDiscriminator, self).__init__()
        
        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´
        self.features = nn.Sequential(
            nn.Conv2d(1, 64, 4, 2, 1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),
        )
        
        # ì§„ì§œ/ê°€ì§œ íŒë³„ í—¤ë“œ
        self.discriminator_head = nn.Sequential(
            nn.Linear(256 * 3 * 3, 1),
            nn.Sigmoid()
        )
        
        # í´ë˜ìŠ¤ ë¶„ë¥˜ í—¤ë“œ
        self.classifier_head = nn.Sequential(
            nn.Linear(256 * 3 * 3, num_classes),
            nn.Softmax(dim=1)
        )
    
    def forward(self, x):
        features = self.features(x)
        features = features.view(features.size(0), -1)
        
        # ë‘ ê°œì˜ ì¶œë ¥: ì§„ì§œ/ê°€ì§œ, í´ë˜ìŠ¤ í™•ë¥ 
        validity = self.discriminator_head(features)
        class_pred = self.classifier_head(features)
        
        return validity, class_pred

# ACGAN ì†ì‹¤ í•¨ìˆ˜
def acgan_loss(real_images, real_labels, fake_images, fake_labels, discriminator):
    # ì‹¤ì œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ê³¼ ë¶„ë¥˜
    real_validity, real_class_pred = discriminator(real_images)
    
    # ìƒì„± ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ê³¼ ë¶„ë¥˜  
    fake_validity, fake_class_pred = discriminator(fake_images)
    
    # ì ëŒ€ì  ì†ì‹¤
    adversarial_loss = nn.BCELoss()
    real_loss = adversarial_loss(real_validity, torch.ones_like(real_validity))
    fake_loss = adversarial_loss(fake_validity, torch.zeros_like(fake_validity))
    
    # ë¶„ë¥˜ ì†ì‹¤
    classification_loss = nn.CrossEntropyLoss()
    real_class_loss = classification_loss(real_class_pred, real_labels)
    fake_class_loss = classification_loss(fake_class_pred, fake_labels)
    
    # ì´ ì†ì‹¤
    d_loss = (real_loss + fake_loss) / 2 + (real_class_loss + fake_class_loss) / 2
    
    return d_loss
```

> ACGANì˜ í•µì‹¬ì€ íŒë³„ìê°€ "ì´ ì´ë¯¸ì§€ê°€ ì§„ì§œì¸ê°€?"ì™€ "ì´ ì´ë¯¸ì§€ëŠ” ì–´ë–¤ í´ë˜ìŠ¤ì¸ê°€?"ë¥¼ **ë™ì‹œì— í•™ìŠµ**í•¨ìœ¼ë¡œì¨ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ë” ì˜ ì´í•´í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ë‹¤. {: .prompt-tip}

## ğŸ–¼ï¸ Pix2Pix: ì´ë¯¸ì§€ ëŒ€ ì´ë¯¸ì§€ ë³€í™˜ì˜ ì‹œì‘

Pix2PixëŠ” 2017ë…„ì— ë°œí‘œëœ **í˜ì–´ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ë³€í™˜**ì˜ ëŒ€í‘œì ì¸ ì—°êµ¬ë‹¤.

### UNet êµ¬ì¡°ì™€ ìŠ¤í‚µ ì—°ê²°

Pix2Pixì˜ í•µì‹¬ì€ **UNet êµ¬ì¡°**ë¥¼ ìƒì„±ìë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.

[ì‹œê°ì  í‘œí˜„ ë„£ê¸° - UNet êµ¬ì¡°ë„]

```python
import torch.nn.functional as F

class UNetGenerator(nn.Module):
    def __init__(self, input_channels=3, output_channels=3):
        super(UNetGenerator, self).__init__()
        
        # ì¸ì½”ë” (ë‹¤ìš´ìƒ˜í”Œë§)
        self.down1 = self.down_block(input_channels, 64, normalize=False)
        self.down2 = self.down_block(64, 128)
        self.down3 = self.down_block(128, 256)
        self.down4 = self.down_block(256, 512)
        self.down5 = self.down_block(512, 512)
        
        # ë””ì½”ë” (ì—…ìƒ˜í”Œë§)
        self.up1 = self.up_block(512, 512, dropout=True)
        self.up2 = self.up_block(1024, 256, dropout=True)  # ìŠ¤í‚µ ì—°ê²°ë¡œ ì±„ë„ ìˆ˜ 2ë°°
        self.up3 = self.up_block(512, 128)
        self.up4 = self.up_block(256, 64)
        
        # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´
        self.final = nn.Sequential(
            nn.ConvTranspose2d(128, output_channels, 4, 2, 1),
            nn.Tanh()
        )
    
    def down_block(self, in_channels, out_channels, normalize=True):
        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1)]
        if normalize:
            layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.LeakyReLU(0.2))
        return nn.Sequential(*layers)
    
    def up_block(self, in_channels, out_channels, dropout=False):
        layers = [
            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        ]
        if dropout:
            layers.append(nn.Dropout(0.5))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # ì¸ì½”ë” - ìŠ¤í‚µ ì—°ê²°ì„ ìœ„í•´ ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        d1 = self.down1(x)      # [B, 64, H/2, W/2]
        d2 = self.down2(d1)     # [B, 128, H/4, W/4]  
        d3 = self.down3(d2)     # [B, 256, H/8, W/8]
        d4 = self.down4(d3)     # [B, 512, H/16, W/16]
        d5 = self.down5(d4)     # [B, 512, H/32, W/32]
        
        # ë””ì½”ë” - ìŠ¤í‚µ ì—°ê²° ì ìš©
        u1 = self.up1(d5)                              # [B, 512, H/16, W/16]
        u2 = self.up2(torch.cat([u1, d4], dim=1))      # [B, 256, H/8, W/8]
        u3 = self.up3(torch.cat([u2, d3], dim=1))      # [B, 128, H/4, W/4]
        u4 = self.up4(torch.cat([u3, d2], dim=1))      # [B, 64, H/2, W/2]
        
        output = self.final(torch.cat([u4, d1], dim=1)) # [B, 3, H, W]
        
        return output

# PatchGAN íŒë³„ì
class PatchGANDiscriminator(nn.Module):
    def __init__(self, input_channels=6):  # ì…ë ¥ + ì¶œë ¥ ì´ë¯¸ì§€ ì—°ê²°
        super(PatchGANDiscriminator, self).__init__()
        
        self.model = nn.Sequential(
            nn.Conv2d(input_channels, 64, 4, 2, 1),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(128, 256, 4, 2, 1), 
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(256, 1, 4, 1, 1)  # íŒ¨ì¹˜ë³„ íŒë³„ ê²°ê³¼
        )
    
    def forward(self, x, y):
        # ì…ë ¥ê³¼ ì¶œë ¥ ì´ë¯¸ì§€ë¥¼ ì±„ë„ ë°©í–¥ìœ¼ë¡œ ì—°ê²°
        combined = torch.cat([x, y], dim=1)
        return self.model(combined)

# Pix2Pix ì†ì‹¤ í•¨ìˆ˜
def pix2pix_loss(real_A, real_B, fake_B, discriminator, lambda_l1=100):
    # L1 ì†ì‹¤ (í”½ì…€ ë ˆë²¨ ë³µì›)
    l1_loss = nn.L1Loss()
    l1 = l1_loss(fake_B, real_B)
    
    # ì ëŒ€ì  ì†ì‹¤
    adversarial_loss = nn.BCEWithLogitsLoss()
    
    # ì‹¤ì œ í˜ì–´ì— ëŒ€í•œ íŒë³„
    real_output = discriminator(real_A, real_B)
    real_loss = adversarial_loss(real_output, torch.ones_like(real_output))
    
    # ìƒì„± í˜ì–´ì— ëŒ€í•œ íŒë³„
    fake_output = discriminator(real_A, fake_B)
    fake_loss = adversarial_loss(fake_output, torch.zeros_like(fake_output))
    
    # ìƒì„±ì ì†ì‹¤ (íŒë³„ìë¥¼ ì†ì´ë ¤ëŠ” ì†ì‹¤)
    gen_adversarial_loss = adversarial_loss(fake_output, torch.ones_like(fake_output))
    
    # ì´ ìƒì„±ì ì†ì‹¤
    gen_loss = gen_adversarial_loss + lambda_l1 * l1
    
    # íŒë³„ì ì†ì‹¤
    disc_loss = (real_loss + fake_loss) / 2
    
    return gen_loss, disc_loss, l1
```

### Pix2Pixì˜ í•µì‹¬ íŠ¹ì§•

- **UNet ìŠ¤í‚µ ì—°ê²°**: ê³ í•´ìƒë„ ë””í…Œì¼ ë³´ì¡´
- **PatchGAN íŒë³„ì**: ì „ì²´ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ íŒë³„í•˜ì—¬ ì§€ì—­ì  ë””í…Œì¼ í–¥ìƒ
- **L1 + ì ëŒ€ì  ì†ì‹¤**: í”½ì…€ ë ˆë²¨ ì •í™•ë„ì™€ ìì—°ìŠ¤ëŸ¬ì›€ì„ ë™ì‹œì— ì¶”êµ¬

> Pix2PixëŠ” **í˜ì–´ ë°ì´í„°ê°€ í•„ìš”**í•˜ë‹¤ëŠ” í•œê³„ê°€ ìˆì§€ë§Œ, ìŠ¤ì¼€ì¹˜â†’ì‹¤ì‚¬, ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µâ†’ì‚¬ì§„ ë“±ì˜ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ë‹¤. {: .prompt-tip}

## ğŸ”„ CycleGAN: í˜ì–´ ë°ì´í„° ì—†ëŠ” ë„ë©”ì¸ ë³€í™˜

CycleGANì€ 2017ë…„ì— ë°œí‘œëœ ì—°êµ¬ë¡œ, **í˜ì–´ ë°ì´í„° ì—†ì´ë„** ë‘ ë„ë©”ì¸ ê°„ ë³€í™˜ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤.

### Cycle Consistencyì˜ í•µì‹¬ ì•„ì´ë””ì–´

**"A â†’ B â†’ Aë¡œ ë³€í™˜í–ˆì„ ë•Œ ì›ë³¸ê³¼ ê°™ì•„ì•¼ í•œë‹¤"**

[ì‹œê°ì  í‘œí˜„ ë„£ê¸° - CycleGAN êµ¬ì¡°ë„ì™€ Cycle Consistency ì„¤ëª…]

### ìˆ˜í•™ì  í‘œí˜„

CycleGANì˜ ëª©ì  í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

$$ \mathcal{L}(G, F, D_X, D_Y) = \mathcal{L}_{GAN}(G, D_Y, X, Y) + \mathcal{L}_{GAN}(F, D_X, Y, X) + \lambda \mathcal{L}_{cyc}(G, F) $$

ì—¬ê¸°ì„œ Cycle Consistency ì†ì‹¤ì€:

$$ \mathcal{L}_{cyc}(G, F) = \mathbb{E}_{x \sim p_{data}(x)}[||F(G(x)) - x||_1] + \mathbb{E}_{y \sim p_{data}(y)}[||G(F(y)) - y||_1] $$

```python
class CycleGAN(nn.Module):
    def __init__(self):
        super(CycleGAN, self).__init__()
        
        # ë‘ ê°œì˜ ìƒì„±ì: Xâ†’Y, Yâ†’X
        self.G_AB = Generator()  # Aì—ì„œ Bë¡œ (ì˜ˆ: ë§ â†’ ì–¼ë£©ë§)
        self.G_BA = Generator()  # Bì—ì„œ Aë¡œ (ì˜ˆ: ì–¼ë£©ë§ â†’ ë§)
        
        # ë‘ ê°œì˜ íŒë³„ì: A ë„ë©”ì¸, B ë„ë©”ì¸
        self.D_A = Discriminator()  # A ë„ë©”ì¸ íŒë³„ì
        self.D_B = Discriminator()  # B ë„ë©”ì¸ íŒë³„ì
    
    def forward(self, real_A, real_B):
        # A â†’ B â†’ A ì‚¬ì´í´
        fake_B = self.G_AB(real_A)
        recovered_A = self.G_BA(fake_B)
        
        # B â†’ A â†’ B ì‚¬ì´í´  
        fake_A = self.G_BA(real_B)
        recovered_B = self.G_AB(fake_A)
        
        return fake_A, fake_B, recovered_A, recovered_B

def cyclegan_loss(real_A, real_B, fake_A, fake_B, recovered_A, recovered_B, 
                  D_A, D_B, lambda_cycle=10.0, lambda_identity=0.5):
    
    # ì ëŒ€ì  ì†ì‹¤
    adversarial_loss = nn.MSELoss()
    
    # D_Aì— ëŒ€í•œ ì†ì‹¤ (ì‹¤ì œ A vs ìƒì„±ëœ A)
    real_A_pred = D_A(real_A)
    fake_A_pred = D_A(fake_A.detach())
    
    loss_D_A = (adversarial_loss(real_A_pred, torch.ones_like(real_A_pred)) + 
                adversarial_loss(fake_A_pred, torch.zeros_like(fake_A_pred))) / 2
    
    # D_Bì— ëŒ€í•œ ì†ì‹¤ (ì‹¤ì œ B vs ìƒì„±ëœ B)
    real_B_pred = D_B(real_B)
    fake_B_pred = D_B(fake_B.detach())
    
    loss_D_B = (adversarial_loss(real_B_pred, torch.ones_like(real_B_pred)) + 
                adversarial_loss(fake_B_pred, torch.zeros_like(fake_B_pred))) / 2
    
    # ìƒì„±ìì˜ ì ëŒ€ì  ì†ì‹¤
    loss_G_A = adversarial_loss(D_A(fake_A), torch.ones_like(D_A(fake_A)))
    loss_G_B = adversarial_loss(D_B(fake_B), torch.ones_like(D_B(fake_B)))
    
    # Cycle Consistency ì†ì‹¤
    cycle_loss = nn.L1Loss()
    loss_cycle_A = cycle_loss(recovered_A, real_A)
    loss_cycle_B = cycle_loss(recovered_B, real_B)
    
    # Identity ì†ì‹¤ (ì„ íƒì )
    identity_loss = nn.L1Loss()
    loss_identity_A = identity_loss(G_BA(real_A), real_A)
    loss_identity_B = identity_loss(G_AB(real_B), real_B)
    
    # ì´ ìƒì„±ì ì†ì‹¤
    loss_G = (loss_G_A + loss_G_B + 
              lambda_cycle * (loss_cycle_A + loss_cycle_B) +
              lambda_identity * (loss_identity_A + loss_identity_B))
    
    # ì´ íŒë³„ì ì†ì‹¤
    loss_D = loss_D_A + loss_D_B
    
    return loss_G, loss_D

# í•™ìŠµ ì˜ˆì‹œ
def train_cyclegan(dataloader_A, dataloader_B, num_epochs=200):
    cyclegan = CycleGAN()
    
    optimizer_G = torch.optim.Adam(
        list(cyclegan.G_AB.parameters()) + list(cyclegan.G_BA.parameters()),
        lr=0.0002, betas=(0.5, 0.999)
    )
    optimizer_D = torch.optim.Adam(
        list(cyclegan.D_A.parameters()) + list(cyclegan.D_B.parameters()),
        lr=0.0002, betas=(0.5, 0.999)
    )
    
    for epoch in range(num_epochs):
        for batch_A, batch_B in zip(dataloader_A, dataloader_B):
            real_A, real_B = batch_A, batch_B
            
            # Forward pass
            fake_A, fake_B, recovered_A, recovered_B = cyclegan(real_A, real_B)
            
            # ì†ì‹¤ ê³„ì‚°
            loss_G, loss_D = cyclegan_loss(
                real_A, real_B, fake_A, fake_B, recovered_A, recovered_B,
                cyclegan.D_A, cyclegan.D_B
            )
            
            # ìƒì„±ì ì—…ë°ì´íŠ¸
            optimizer_G.zero_grad()
            loss_G.backward()
            optimizer_G.step()
            
            # íŒë³„ì ì—…ë°ì´íŠ¸
            optimizer_D.zero_grad()
            loss_D.backward()
            optimizer_D.step()
            
        print(f"Epoch {epoch}: G_loss={loss_G:.4f}, D_loss={loss_D:.4f}")
```

### CycleGANì˜ í™œìš© ì‚¬ë¡€

- **ìŠ¤íƒ€ì¼ ë³€í™˜**: ì‚¬ì§„ â†” ê·¸ë¦¼, ì—¬ë¦„ â†” ê²¨ìš¸ í’ê²½
- **ê°ì²´ ë³€í™˜**: ë§ â†” ì–¼ë£©ë§, ì‚¬ê³¼ â†” ì˜¤ë Œì§€
- **ë„ë©”ì¸ ì ì‘**: ì‹œë®¬ë ˆì´ì…˜ ì´ë¯¸ì§€ â†’ ì‹¤ì œ ì´ë¯¸ì§€

> CycleGANì˜ í˜ì‹ ì€ **ëŒ€ì¹­ì„± ê°€ì •**ì— ìˆë‹¤. "Aë¥¼ Bë¡œ ë°”ê¾¼ í›„ ë‹¤ì‹œ Aë¡œ ë°”ê¿¨ì„ ë•Œ ì›ë³¸ê³¼ ê°™ì•„ì•¼ í•œë‹¤"ëŠ” ì§ê´€ì ì¸ ì œì•½ìœ¼ë¡œ í˜ì–´ ë°ì´í„° ì—†ì´ë„ ë³€í™˜ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í–ˆë‹¤. {: .prompt-tip}

## â­ StarGAN: ë‹¤ì¤‘ ë„ë©”ì¸ ë³€í™˜

StarGANì€ 2018ë…„ì— ë°œí‘œëœ ì—°êµ¬ë¡œ, **ì—¬ëŸ¬ ë„ë©”ì¸ ê°„ ë³€í™˜**ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í–ˆë‹¤.

### ë‹¤ì¤‘ ë„ë©”ì¸ ë³€í™˜ì˜ í•„ìš”ì„±

ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ Nê°œ ë„ë©”ì¸ ê°„ ë³€í™˜ì„ í•˜ë ¤ë©´ NÃ—(N-1)ê°œì˜ ëª¨ë¸ì´ í•„ìš”í•˜ì§€ë§Œ, StarGANì€ **ë‹¨ì¼ ëª¨ë¸**ë¡œ ì²˜ë¦¬í•œë‹¤.

```python
class StarGANGenerator(nn.Module):
    def __init__(self, conv_dim=64, num_domains=5):
        super(StarGANGenerator, self).__init__()
        
        # ë‹¤ìš´ìƒ˜í”Œë§ ë¸”ë¡
        self.down_layers = nn.ModuleList([
            nn.Conv2d(3 + num_domains, conv_dim, 7, 1, 3),  # ì´ë¯¸ì§€ + ë„ë©”ì¸ ë ˆì´ë¸”
            nn.InstanceNorm2d(conv_dim),
            nn.ReLU(),
            
            nn.Conv2d(conv_dim, conv_dim*2, 4, 2, 1),
            nn.InstanceNorm2d(conv_dim*2),
            nn.ReLU(),
            
            nn.Conv2d(conv_dim*2, conv_dim*4, 4, 2, 1),
            nn.InstanceNorm2d(conv_dim*4), 
            nn.ReLU()
        ])
        
        # ì”ì°¨ ë¸”ë¡ë“¤
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(conv_dim*4) for _ in range(6)
        ])
        
        # ì—…ìƒ˜í”Œë§ ë¸”ë¡
        self.up_layers = nn.ModuleList([
            nn.ConvTranspose2d(conv_dim*4, conv_dim*2, 4, 2, 1),
            nn.InstanceNorm2d(conv_dim*2),
            nn.ReLU(),
            
            nn.ConvTranspose2d(conv_dim*2, conv_dim, 4, 2, 1),
            nn.InstanceNorm2d(conv_dim),
            nn.ReLU(),
            
            nn.Conv2d(conv_dim, 3, 7, 1, 3),
            nn.Tanh()
        ])
    
    def forward(self, x, target_domain):
        # íƒ€ê²Ÿ ë„ë©”ì¸ ë ˆì´ë¸”ì„ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ í™•ì¥
        target_domain = target_domain.view(target_domain.size(0), target_domain.size(1), 1, 1)
        target_domain = target_domain.repeat(1, 1, x.size(2), x.size(3))
        
        # ì´ë¯¸ì§€ì™€ ë„ë©”ì¸ ë ˆì´ë¸” ê²°í•©
        x = torch.cat([x, target_domain], dim=1)
        
        # ì¸ì½”ë”
        for layer in self.down_layers:
            x = layer(x)
        
        # ì”ì°¨ ë¸”ë¡
        for block in self.residual_blocks:
            x = block(x)
        
        # ë””ì½”ë”  
        for layer in self.up_layers:
            x = layer(x)
            
        return x

class StarGANDiscriminator(nn.Module):
    def __init__(self, conv_dim=64, num_domains=5):
        super(StarGANDiscriminator, self).__init__()
        
        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´
        self.main = nn.Sequential(
            nn.Conv2d(3, conv_dim, 4, 2, 1),
            nn.LeakyReLU(0.01),
            
            nn.Conv2d(conv_dim, conv_dim*2, 4, 2, 1),
            nn.LeakyReLU(0.01),
            
            nn.Conv2d(conv_dim*2, conv_dim*4, 4, 2, 1),
            nn.LeakyReLU(0.01),
            
            nn.Conv2d(conv_dim*4, conv_dim*8, 4, 2, 1),
            nn.LeakyReLU(0.01),
            
            nn.Conv2d(conv_dim*8, conv_dim*16, 4, 2, 1),
            nn.LeakyReLU(0.01)
        )
        
        # ì§„ì§œ/ê°€ì§œ íŒë³„ í—¤ë“œ
        self.dis_head = nn.Conv2d(conv_dim*16, 1, 3, 1, 1)
        
        # ë„ë©”ì¸ ë¶„ë¥˜ í—¤ë“œ
        self.cls_head = nn.Conv2d(conv_dim*16, num_domains, 2, 1, 0)
    
    def forward(self, x):
        features = self.main(x)
        
        # ì§„ì§œ/ê°€ì§œ ì ìˆ˜
        validity = self.dis_head(features)
        
        # ë„ë©”ì¸ ë¶„ë¥˜ ì ìˆ˜
        domain_pred = self.cls_head(features)
        domain_pred = domain_pred.view(domain_pred.size(0), -1)
        
        return validity, domain_pred

# StarGAN ì†ì‹¤ í•¨ìˆ˜
def stargan_loss(real_images, real_domains, target_domains, generator, discriminator,
                lambda_cls=1.0, lambda_rec=10.0):
    
    # ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±
    fake_images = generator(real_images, target_domains)
    
    # ë³µì› ì´ë¯¸ì§€ ìƒì„± (ì‚¬ì´í´ ì¼ê´€ì„±)
    reconstructed_images = generator(fake_images, real_domains)
    
    # íŒë³„ì ì˜ˆì¸¡
    real_validity, real_domain_pred = discriminator(real_images)
    fake_validity, fake_domain_pred = discriminator(fake_images.detach())
    
    # ì ëŒ€ì  ì†ì‹¤
    adversarial_loss = nn.MSELoss()
    d_loss_real = adversarial_loss(real_validity, torch.ones_like(real_validity))
    d_loss_fake = adversarial_loss(fake_validity, torch.zeros_like(fake_validity))
    
    # ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤ (ì‹¤ì œ ì´ë¯¸ì§€)
    classification_loss = nn.CrossEntropyLoss()
    d_loss_cls = classification_loss(real_domain_pred, real_domains)
    
    # íŒë³„ì ì´ ì†ì‹¤
    d_loss = d_loss_real + d_loss_fake + lambda_cls * d_loss_cls
    
    # ìƒì„±ì ì ëŒ€ì  ì†ì‹¤
    fake_validity_for_g, fake_domain_pred_for_g = discriminator(fake_images)
    g_loss_fake = adversarial_loss(fake_validity_for_g, torch.ones_like(fake_validity_for_g))
    
    # ìƒì„±ì ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤
    g_loss_cls = classification_loss(fake_domain_pred_for_g, target_domains)
    
    # ë³µì› ì†ì‹¤ (ì‚¬ì´í´ ì¼ê´€ì„±)
    reconstruction_loss = nn.L1Loss()
    g_loss_rec = reconstruction_loss(reconstructed_images, real_images)
    
    # ìƒì„±ì ì´ ì†ì‹¤
    g_loss = g_loss_fake + lambda_cls * g_loss_cls + lambda_rec * g_loss_rec
    
    return g_loss, d_loss

# ì‚¬ìš© ì˜ˆì‹œ
def train_stargan():
    # 5ê°œ ë„ë©”ì¸: ì„±ë³„, ë‚˜ì´, ë¨¸ë¦¬ìƒ‰ ë“±
    num_domains = 5
    
    generator = StarGANGenerator(num_domains=num_domains)
    discriminator = StarGANDiscriminator(num_domains=num_domains)
    
    # ì‹¤ì œ ì´ë¯¸ì§€ì™€ ë„ë©”ì¸ ë ˆì´ë¸”
    real_images = torch.randn(16, 3, 128, 128)
    real_domains = torch.randint(0, num_domains, (16,))
    target_domains = torch.randint(0, num_domains, (16,))
    
    # íƒ€ê²Ÿ ë„ë©”ì¸ì„ ì›-í•« ë²¡í„°ë¡œ ë³€í™˜
    target_domains_onehot = torch.zeros(16, num_domains)
    target_domains_onehot.scatter_(1, target_domains.unsqueeze(1), 1)
    
    real_domains_onehot = torch.zeros(16, num_domains)
    real_domains_onehot.scatter_(1, real_domains.unsqueeze(1), 1)
    
    # ì†ì‹¤ ê³„ì‚°
    g_loss, d_loss = stargan_loss(
        real_images, real_domains, target_domains,
        generator, discriminator
    )
    
    print(f"Generator Loss: {g_loss:.4f}")
    print(f"Discriminator Loss: {d_loss:.4f}")
```

### StarGANì˜ ì¥ì 

- **ëª¨ë¸ íš¨ìœ¨ì„±**: Nê°œ ë„ë©”ì¸ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì²˜ë¦¬
- **ë‹¤ì–‘í•œ ì†ì„± ì œì–´**: ë‚˜ì´, ì„±ë³„, ë¨¸ë¦¬ìƒ‰, í‘œì • ë“± ë™ì‹œ ë³€ê²½ ê°€ëŠ¥
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ë„ë©”ì¸ ì¶”ê°€ê°€ ìš©ì´

## ğŸ’¬ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±: ìì—°ì–´ë¡œ ê·¸ë¦¼ ê·¸ë¦¬ê¸°

í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì€ **ê°€ì¥ ë†’ì€ ììœ ë„**ë¥¼ ì œê³µí•˜ì§€ë§Œ **í•™ìŠµì´ ê°€ì¥ ì–´ë ¤ìš´** ì¡°ê±´ë¶€ ìƒì„± ì‘ì—…ì´ë‹¤.

### í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì˜ ì–´ë ¤ì›€

- **ìˆœì°¨ì  vs ë™ì‹œì **: í…ìŠ¤íŠ¸ëŠ” ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±ë˜ì–´ ì´ì „ ì»¨í…ìŠ¤íŠ¸ í™œìš© ê°€ëŠ¥í•˜ì§€ë§Œ, ì´ë¯¸ì§€ëŠ” ëª¨ë“  í”½ì…€ì„ ë™ì‹œì— ìƒì„±í•´ì•¼ í•¨
- **ëª¨ë‹¬ë¦¬í‹° ì°¨ì´**: ì–¸ì–´ì™€ ì‹œê°ì˜ í‘œí˜„ ë°©ì‹ì´ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¦„
- **ë³µì¡í•œ ì˜ë¯¸ë¡ **: "í„¸ì´ ë¶€ìŠ¬ë¶€ìŠ¬í•œ ê·€ì—¬ìš´ ê°•ì•„ì§€"ì™€ ê°™ì€ ì¶”ìƒì  í‘œí˜„ì„ ì‹œê°ì ìœ¼ë¡œ êµ¬í˜„

### ì´ˆê¸° GAN ê¸°ë°˜ ì ‘ê·¼ë²•

```python
class TextToImageGAN(nn.Module):
    def __init__(self, text_embedding_dim=256, noise_dim=100):
        super(TextToImageGAN, self).__init__()
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” (ë¯¸ë¦¬ í•™ìŠµëœ ì„ë² ë”© ì‚¬ìš© ê°€ëŠ¥)
        self.text_encoder = nn.Sequential(
            nn.Linear(text_embedding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128)
        )
        
        # ìƒì„±ì
        self.generator = nn.Sequential(
            nn.Linear(noise_dim + 128, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(), 
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, 3*64*64),  # 64x64 RGB ì´ë¯¸ì§€
            nn.Tanh()
        )
        
        # íŒë³„ì (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ)
        self.discriminator = nn.Sequential(
            nn.Linear(3*64*64 + 128, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, text_embeddings, noise):
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_features = self.text_encoder(text_embeddings)
        
        # ë…¸ì´ì¦ˆì™€ í…ìŠ¤íŠ¸ íŠ¹ì§• ê²°í•©
        combined_input = torch.cat([noise, text_features], dim=1)
        
        # ì´ë¯¸ì§€ ìƒì„±
        generated_images = self.generator(combined_input)
        generated_images = generated_images.view(-1, 3, 64, 64)
        
        return generated_images, text_features
    
    def discriminate(self, images, text_features):
        # ì´ë¯¸ì§€ë¥¼ í‰ë©´í™”
        flattened_images = images.view(images.size(0), -1)
        
        # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ íŠ¹ì§• ê²°í•©
        combined = torch.cat([flattened_images, text_features], dim=1)
        
        return self.discriminator(combined)

# í–¥ìƒëœ ì†ì‹¤ í•¨ìˆ˜ (ì˜ëª»ëœ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í˜ì–´ í™œìš©)
def text_to_image_loss(real_images, real_texts, wrong_texts, model):
    batch_size = real_images.size(0)
    noise = torch.randn(batch_size, 100)
    
    # ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
    fake_images, real_text_features = model(real_texts, noise)
    
    # ì˜ëª»ëœ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
    wrong_text_features = model.text_encoder(wrong_texts)
    
    # íŒë³„ì ì˜ˆì¸¡
    real_real_pred = model.discriminate(real_images, real_text_features)      # ì‹¤ì œ ì´ë¯¸ì§€ + ì‹¤ì œ í…ìŠ¤íŠ¸
    real_wrong_pred = model.discriminate(real_images, wrong_text_features)    # ì‹¤ì œ ì´ë¯¸ì§€ + ì˜ëª»ëœ í…ìŠ¤íŠ¸  
    fake_real_pred = model.discriminate(fake_images, real_text_features)      # ìƒì„± ì´ë¯¸ì§€ + ì‹¤ì œ í…ìŠ¤íŠ¸
    
    # ì†ì‹¤ ê³„ì‚°
    bce_loss = nn.BCELoss()
    
    # íŒë³„ì ì†ì‹¤
    d_loss_real_real = bce_loss(real_real_pred, torch.ones_like(real_real_pred))
    d_loss_real_wrong = bce_loss(real_wrong_pred, torch.zeros_like(real_wrong_pred))  
    d_loss_fake_real = bce_loss(fake_real_pred, torch.zeros_like(fake_real_pred))
    
    d_loss = (d_loss_real_real + d_loss_real_wrong + d_loss_fake_real) / 3
    
    # ìƒì„±ì ì†ì‹¤ (íŒë³„ìë¥¼ ì†ì´ë ¤ëŠ” ì†ì‹¤)
    g_loss = bce_loss(fake_real_pred, torch.ones_like(fake_real_pred))
    
    return g_loss, d_loss

# í…ìŠ¤íŠ¸ ë³´ê°„ë²• (Interpolation) ì ìš©
def text_interpolation_training(model, text1, text2, alpha=0.5):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ ë³´ê°„ì„ í†µí•œ ë°ì´í„° ì¦ê°•"""
    
    # í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ
    text1_features = model.text_encoder(text1)
    text2_features = model.text_encoder(text2)
    
    # ì„ í˜• ë³´ê°„
    interpolated_features = alpha * text1_features + (1 - alpha) * text2_features
    
    # ë³´ê°„ëœ íŠ¹ì§•ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±
    noise = torch.randn(text1.size(0), 100)
    combined_input = torch.cat([noise, interpolated_features], dim=1)
    interpolated_images = model.generator(combined_input)
    interpolated_images = interpolated_images.view(-1, 3, 64, 64)
    
    return interpolated_images, interpolated_features
```

## ğŸš€ GigaGAN: ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±

GigaGANì€ 2023ë…„ì— ë°œí‘œëœ ì—°êµ¬ë¡œ, **ëŒ€ê·œëª¨ ëª¨ë¸**ê³¼ **ê³„ì¸µì  ìƒì„±**ì„ í†µí•´ ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì„ ë‹¬ì„±í–ˆë‹¤.

### GigaGANì˜ í•µì‹¬ êµ¬ì¡°

[ì‹œê°ì  í‘œí˜„ ë„£ê¸° - GigaGAN ì „ì²´ êµ¬ì¡°ë„]

```python
class GigaGANGenerator(nn.Module):
    def __init__(self, text_embedding_dim=512, style_dim=512):
        super(GigaGANGenerator, self).__init__()
        
        # í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì‹± (CLIP ì¸ì½”ë” í™œìš©)
        self.text_encoder = nn.Linear(text_embedding_dim, 256)
        
        # ìŠ¤íƒ€ì¼ ë„¤íŠ¸ì›Œí¬ (StyleGAN ë°©ì‹)
        self.style_network = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, style_dim)
        )
        
        # ê³„ì¸µì  ìƒì„±ì (ì €í•´ìƒë„ â†’ ê³ í•´ìƒë„)
        self.base_generator = BaseGenerator(style_dim, 256)  # 64x64
        self.super_resolution = SuperResolutionNet(256)      # 64x64 â†’ 512x512
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ ë ˆì´ì–´ë“¤
        self.cross_attention_layers = nn.ModuleList([
            CrossAttentionBlock(256, 256) for _ in range(6)
        ])
    
    def forward(self, text_embeddings, noise):
        batch_size = text_embeddings.size(0)
        
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬
        text_features = self.text_encoder(text_embeddings)  # [B, 256]
        
        # ìŠ¤íƒ€ì¼ ë²¡í„° ìƒì„±
        style_vector = self.style_network(noise)  # [B, 512]
        
        # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± (64x64)
        base_image = self.base_generator(style_vector)  # [B, 3, 64, 64]
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ì„ í†µí•œ í…ìŠ¤íŠ¸ ë°˜ì˜
        features = base_image
        for attention_layer in self.cross_attention_layers:
            features = attention_layer(features, text_features)
        
        # ê³ í•´ìƒë„ë¡œ ì—…ìŠ¤ì¼€ì¼
        high_res_image = self.super_resolution(features)  # [B, 3, 512, 512]
        
        return high_res_image

class CrossAttentionBlock(nn.Module):
    def __init__(self, visual_dim, text_dim, num_heads=8):
        super(CrossAttentionBlock, self).__init__()
        
        self.visual_dim = visual_dim
        self.text_dim = text_dim
        self.num_heads = num_heads
        self.head_dim = visual_dim // num_heads
        
        # Query: ì‹œê°ì  íŠ¹ì§•, Key/Value: í…ìŠ¤íŠ¸ íŠ¹ì§•
        self.q_linear = nn.Linear(visual_dim, visual_dim)
        self.k_linear = nn.Linear(text_dim, visual_dim)
        self.v_linear = nn.Linear(text_dim, visual_dim)
        
        self.out_linear = nn.Linear(visual_dim, visual_dim)
        self.norm = nn.LayerNorm(visual_dim)
        
    def forward(self, visual_features, text_features):
        B, C, H, W = visual_features.size()
        
        # ì‹œê°ì  íŠ¹ì§•ì„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ [B, H*W, C]
        visual_seq = visual_features.view(B, C, H*W).transpose(1, 2)
        
        # í…ìŠ¤íŠ¸ íŠ¹ì§• ì°¨ì› ë§ì¶”ê¸° [B, text_len, text_dim] â†’ [B, 1, text_dim]
        if text_features.dim() == 2:
            text_features = text_features.unsqueeze(1)
        
        # Q, K, V ê³„ì‚°
        Q = self.q_linear(visual_seq)    # [B, H*W, visual_dim]
        K = self.k_linear(text_features) # [B, text_len, visual_dim] 
        V = self.v_linear(text_features) # [B, text_len, visual_dim]
        
        # ë©€í‹°í—¤ë“œ ì–´í…ì…˜
        Q = Q.view(B, H*W, self.num_heads, self.head_dim).transpose(1, 2)  # [B, num_heads, H*W, head_dim]
        K = K.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)   # [B, num_heads, text_len, head_dim]
        V = V.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)   # [B, num_heads, text_len, head_dim]
        
        # ì–´í…ì…˜ ì ìˆ˜ ê³„ì‚°
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attention_weights = torch.softmax(attention_scores, dim=-1)
        
        # ì–´í…ì…˜ ì ìš©
        attended = torch.matmul(attention_weights, V)  # [B, num_heads, H*W, head_dim]
        attended = attended.transpose(1, 2).contiguous().view(B, H*W, self.visual_dim)
        
        # ì¶œë ¥ ë³€í™˜
        output = self.out_linear(attended)
        
        # ì”ì°¨ ì—°ê²° ë° ì •ê·œí™”
        output = self.norm(output + visual_seq)
        
        # ì›ë˜ í˜•íƒœë¡œ ë³µì›
        output = output.transpose(1, 2).view(B, C, H, W)
        
        return output

class HierarchicalDiscriminator(nn.Module):
    def __init__(self, text_dim=256):
        super(HierarchicalDiscriminator, self).__init__()
        
        # ë‹¤ë‹¨ê³„ íŒë³„ì (ê° í•´ìƒë„ë³„)
        self.discriminators = nn.ModuleList([
            PatchDiscriminator(3 + text_dim, 64),   # 64x64
            PatchDiscriminator(3 + text_dim, 128),  # 128x128  
            PatchDiscriminator(3 + text_dim, 256),  # 256x256
            PatchDiscriminator(3 + text_dim, 512),  # 512x512
        ])
        
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬
        self.text_encoder = nn.Linear(text_dim, text_dim)
        
    def forward(self, images, text_features):
        batch_size = images.size(0)
        outputs = []
        
        # í…ìŠ¤íŠ¸ íŠ¹ì§• ì²˜ë¦¬
        text_features = self.text_encoder(text_features)
        
        # ê° í•´ìƒë„ì—ì„œ íŒë³„
        current_images = images
        for i, discriminator in enumerate(self.discriminators):
            # í…ìŠ¤íŠ¸ íŠ¹ì§•ì„ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ í™•ì¥
            text_map = text_features.unsqueeze(-1).unsqueeze(-1)
            text_map = text_map.repeat(1, 1, current_images.size(2), current_images.size(3))
            
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê²°í•©
            combined = torch.cat([current_images, text_map], dim=1)
            
            # íŒë³„
            output = discriminator(combined)
            outputs.append(output)
            
            # ë‹¤ìŒ ë ˆë²¨ì„ ìœ„í•´ ë‹¤ìš´ìƒ˜í”Œë§
            if i < len(self.discriminators) - 1:
                current_images = F.interpolate(current_images, scale_factor=0.5, mode='bilinear')
        
        return outputs

# GigaGAN í•™ìŠµ í•¨ìˆ˜
def train_gigagan(dataloader, num_epochs=100):
    # ëª¨ë¸ ì´ˆê¸°í™”
    generator = GigaGANGenerator()
    discriminator = HierarchicalDiscriminator()
    
    # ì˜µí‹°ë§ˆì´ì €
    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.0, 0.99))
    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.0, 0.99))
    
    # ì†ì‹¤ í•¨ìˆ˜
    adversarial_loss = nn.BCEWithLogitsLoss()
    
    for epoch in range(num_epochs):
        for batch_idx, (real_images, text_embeddings) in enumerate(dataloader):
            batch_size = real_images.size(0)
            
            # ë…¸ì´ì¦ˆ ìƒì„±
            noise = torch.randn(batch_size, 512)
            
            # === íŒë³„ì í•™ìŠµ ===
            d_optimizer.zero_grad()
            
            # ì‹¤ì œ ì´ë¯¸ì§€ íŒë³„
            real_outputs = discriminator(real_images, text_embeddings)
            d_loss_real = sum([adversarial_loss(output, torch.ones_like(output)) 
                              for output in real_outputs]) / len(real_outputs)
            
            # ê°€ì§œ ì´ë¯¸ì§€ ìƒì„± ë° íŒë³„
            fake_images = generator(text_embeddings, noise)
            fake_outputs = discriminator(fake_images.detach(), text_embeddings)
            d_loss_fake = sum([adversarial_loss(output, torch.zeros_like(output)) 
                              for output in fake_outputs]) / len(fake_outputs)
            
            # íŒë³„ì ì†ì‹¤
            d_loss = (d_loss_real + d_loss_fake) / 2
            d_loss.backward()
            d_optimizer.step()
            
            # === ìƒì„±ì í•™ìŠµ ===
            g_optimizer.zero_grad()
            
            # ìƒì„±ì ì ëŒ€ì  ì†ì‹¤
            fake_outputs = discriminator(fake_images, text_embeddings)
            g_loss = sum([adversarial_loss(output, torch.ones_like(output)) 
                         for output in fake_outputs]) / len(fake_outputs)
            
            g_loss.backward()
            g_optimizer.step()
            
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}: G_loss={g_loss:.4f}, D_loss={d_loss:.4f}")
```

### GigaGANì˜ í˜ì‹ ì 

- **ê³„ì¸µì  ìƒì„±**: ì €í•´ìƒë„ì—ì„œ ì‹œì‘í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ê³ í•´ìƒë„ë¡œ ì—…ìŠ¤ì¼€ì¼
- **í¬ë¡œìŠ¤ ì–´í…ì…˜**: í…ìŠ¤íŠ¸ì™€ ì‹œê°ì  íŠ¹ì§• ê°„ì˜ ì •êµí•œ ìƒí˜¸ì‘ìš©
- **ë¹ ë¥¸ ì¶”ë¡ **: ë””í“¨ì „ ëª¨ë¸ê³¼ ë‹¬ë¦¬ **ë‹¨ì¼ í¬ì›Œë“œ íŒ¨ìŠ¤**ë¡œ ìƒì„±
- **ìŠ¤íƒ€ì¼ ì œì–´**: ì „ì—­ì  ìŠ¤íƒ€ì¼ê³¼ ì§€ì—­ì  ë””í…Œì¼ì„ ë¶„ë¦¬í•˜ì—¬ ì œì–´

> GigaGANì€ GANì˜ **ë¹ ë¥¸ ìƒì„± ì†ë„**ì™€ ìµœì‹  **ëŒ€ê·œëª¨ ëª¨ë¸**ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬, ë””í“¨ì „ ëª¨ë¸ì— ë§ì„œëŠ” ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì„ ë‹¬ì„±í–ˆë‹¤. {: .prompt-tip}

## ğŸ¨ ì‹¤ì œ í™œìš© ì‚¬ë¡€ì™€ í•œê³„ì 

### ì¡°ê±´ë¶€ GANsì˜ ì‹¤ì œ í™œìš©

- **ì½˜í…ì¸  ì°½ì‘**: ê²Œì„, ì˜í™”, ê´‘ê³  ë“±ì—ì„œ ì»¨ì…‰ ì•„íŠ¸ ìƒì„±
- **íŒ¨ì…˜ ë””ìì¸**: ì˜ë¥˜ ë””ìì¸ í”„ë¡œí† íƒ€ì´í•‘
- **ê±´ì¶• ì‹œê°í™”**: ì„¤ê³„ë„ë¥¼ ì‹¤ì œ ê±´ë¬¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
- **ì˜ë£Œ ì´ë¯¸ì§•**: ì„œë¡œ ë‹¤ë¥¸ ì˜ë£Œ ì˜ìƒ ëª¨ë‹¬ë¦¬í‹° ê°„ ë³€í™˜
- **ë°ì´í„° ì¦ê°•**: ë¶€ì¡±í•œ ë°ì´í„° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìƒì„±

### í˜„ì¬ì˜ í•œê³„ì 

- **ë°ì´í„° ì˜ì¡´ì„±**: ê³ í’ˆì§ˆ í˜ì–´ ë°ì´í„°ë‚˜ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í•„ìš”
- **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ë„ë©”ì¸ì— ìµœì í™”ë˜ë©´ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ ì„±ëŠ¥ ì €í•˜
- **ëª¨ë“œ ë¶•ê´´**: ë‹¤ì–‘ì„± ë¶€ì¡±ìœ¼ë¡œ ë¹„ìŠ·í•œ ê²°ê³¼ë§Œ ìƒì„±í•˜ëŠ” ê²½ìš°
- **ì œì–´ì˜ í•œê³„**: ë¯¸ì„¸í•œ ë””í…Œì¼ì´ë‚˜ ë³µì¡í•œ ê³µê°„ ê´€ê³„ ì œì–´ ì–´ë ¤ì›€

### ë¯¸ë˜ ì „ë§

- **ë©€í‹°ëª¨ë‹¬ í•™ìŠµ**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ë¥¼ í†µí•©í•œ ì¡°ê±´ë¶€ ìƒì„±
- **ì‹¤ì‹œê°„ í¸ì§‘**: ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì„ í†µí•œ ì‹¤ì‹œê°„ ì´ë¯¸ì§€ í¸ì§‘
- **ê°œì¸í™”**: ê°œë³„ ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ í•™ìŠµí•œ ë§ì¶¤í˜• ìƒì„±
- **ìœ¤ë¦¬ì  ê³ ë ¤**: ë”¥í˜ì´í¬ ë°©ì§€ì™€ ì•ˆì „í•œ AI ìƒì„± ì½˜í…ì¸ 

## ğŸ” ë§ˆë¬´ë¦¬

ì¡°ê±´ë¶€ GANsëŠ” **"ë‚´ê°€ ì›í•˜ëŠ” ê²ƒì„ ë§Œë“¤ì–´ ë‹¬ë¼"**ëŠ” ì¸ê°„ì˜ ê·¼ë³¸ì  ìš•êµ¬ë¥¼ AIë¡œ êµ¬í˜„í•œ ê¸°ìˆ ì´ë‹¤. Pix2Pixì˜ í˜ì–´ ë°ì´í„° ë°©ì‹ì—ì„œ ì‹œì‘í•˜ì—¬, CycleGANì˜ ì‚¬ì´í´ ì¼ê´€ì„±, StarGANì˜ ë‹¤ì¤‘ ë„ë©”ì¸ ë³€í™˜, ê·¸ë¦¬ê³  ìµœê·¼ì˜ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ê¹Œì§€, ì¡°ê±´ë¶€ ìƒì„±ì€ ì§€ì†ì ìœ¼ë¡œ ë°œì „í•´ì™”ë‹¤.

ë¹„ë¡ ë””í“¨ì „ ëª¨ë¸ì´ ìƒì„± í’ˆì§ˆì—ì„œ ì•ì„œë‚˜ê°€ê³  ìˆì§€ë§Œ, GANì˜ **ë¹ ë¥¸ ìƒì„± ì†ë„**ì™€ **ì§ê´€ì ì¸ í•™ìŠµ ê³¼ì •**ì€ ì—¬ì „íˆ ë§ì€ ì‹¤ìš©ì  ê°€ì¹˜ë¥¼ ì œê³µí•œë‹¤. íŠ¹íˆ ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë‚˜ ë¦¬ì†ŒìŠ¤ê°€ ì œí•œëœ í™˜ê²½ì—ì„œëŠ” ì¡°ê±´ë¶€ GANsê°€ ê³„ì†í•´ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.

> ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ì˜ ì§„ì •í•œ ê°€ì¹˜ëŠ” ë‹¨ìˆœíˆ "ê·¸ëŸ´ë“¯í•œ" ì´ë¯¸ì§€ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì¸ê°„ì˜ **ì°½ì˜ì  ì˜ë„ë¥¼ AIê°€ ì´í•´í•˜ê³  êµ¬í˜„**í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ë° ìˆë‹¤. ì´ëŠ” AIê°€ ë‹¨ìˆœí•œ ë„êµ¬ë¥¼ ë„˜ì–´ **ì°½ì‘ íŒŒíŠ¸ë„ˆ**ë¡œ ë°œì „í•˜ëŠ” ì¤‘ìš”í•œ ë‹¨ê³„ì´ë‹¤. {: .prompt-tip}