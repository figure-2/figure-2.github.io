---
title: ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì˜ í‰ê°€ ì§€í‘œ ì‚´í´ë³´ê¸°
date: 2025-07-15 20:44:00 +0900
categories: 
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

- torch==2.0.0+
- torchvision==0.15.0+
- numpy==1.21.0+
- scipy==1.7.0+
- PIL==8.3.0+
- clip-by-openai==1.0

## ğŸš€ TL;DR

- **ìƒì„± ëª¨ë¸ í‰ê°€**ëŠ” ì •ë‹µì´ ì—†ì–´ íŒë³„ ëª¨ë¸ë³´ë‹¤ **í›¨ì”¬ ë³µì¡í•˜ê³  ì–´ë ¤ìš´ ë¬¸ì œ**
- **í’ˆì§ˆ(Quality)**ê³¼ **ë‹¤ì–‘ì„±(Diversity)** ë‘ ê´€ì ì—ì„œ í‰ê°€í•´ì•¼ í•˜ë©°, ì´ ë‘˜ì€ ë³´í†µ **íŠ¸ë ˆì´ë“œì˜¤í”„ ê´€ê³„**
- **Inception Score(IS)**ëŠ” í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— ê³ ë ¤í•˜ì§€ë§Œ **ImageNet í´ë˜ìŠ¤ì—ë§Œ ì ìš© ê°€ëŠ¥**í•˜ê³  **adversarial attackì— ì·¨ì•½**
- **FID(FrÃ©chet Inception Distance)**ëŠ” ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„± ë°ì´í„°ì˜ **íŠ¹ì„± ë¶„í¬ ê°„ ê±°ë¦¬**ë¥¼ ì¸¡ì •í•˜ì—¬ **ê°€ì¥ ë„ë¦¬ ì‚¬ìš©**ë˜ëŠ” ì§€í‘œ
- **Precision/Recall**ì€ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ **ê°ê° ë¶„ë¦¬í•´ì„œ ì¸¡ì •** ê°€ëŠ¥í•˜ì§€ë§Œ **outlierì— ë¯¼ê°**í•˜ê³  **ê³„ì‚°ëŸ‰ì´ í¼**
- **ì¡°ê±´ë¶€ ìƒì„±**ì—ì„œëŠ” **LPIPS**(ë‹¤ì–‘ì„±), **CLIP Score**(í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì¼ì¹˜ë„) ë“± **ì¶”ê°€ ì§€í‘œ** í•„ìš”
- ì‹¤ì œë¡œëŠ” **ë‹¨ì¼ ì§€í‘œë¡œ ì™„ë²½í•œ í‰ê°€ ë¶ˆê°€ëŠ¥**í•˜ì—¬ **ì—¬ëŸ¬ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ í™œìš©**í•´ì•¼ í•¨

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [Image Generation Evaluation Metrics](https://github.com/yuiyeong/notebooks/blob/main/deep_learning/image_generation_metrics.ipynb)

## ğŸ¯ ìƒì„± ëª¨ë¸ í‰ê°€ì˜ í•„ìš”ì„±ê³¼ ì–´ë ¤ì›€

### íŒë³„ ëª¨ë¸ vs ìƒì„± ëª¨ë¸ì˜ í‰ê°€ ì°¨ì´

**íŒë³„ ëª¨ë¸(Discriminative Model)**ì€ ëª…í™•í•œ ì •ë‹µì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— í‰ê°€ê°€ ìƒëŒ€ì ìœ¼ë¡œ ê°„ë‹¨í•˜ë‹¤. ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ì •í™•ë„(Accuracy), ì •ë°€ë„(Precision), ì¬í˜„ìœ¨(Recall) ë“±ì„, íšŒê·€ ë¬¸ì œì—ì„œëŠ” MSE, MAE ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```python
# íŒë³„ ëª¨ë¸ì˜ ê°„ë‹¨í•œ í‰ê°€ ì˜ˆì‹œ
from sklearn.metrics import accuracy_score, precision_score, recall_score

# ë¶„ë¥˜ ëª¨ë¸ í‰ê°€
y_true = [0, 1, 1, 0, 1]
y_pred = [0, 1, 0, 0, 1]

accuracy = accuracy_score(y_true, y_pred)  # 0.8
precision = precision_score(y_true, y_pred)  # 1.0 
recall = recall_score(y_true, y_pred)  # 0.67

print(f"ì •í™•ë„: {accuracy}, ì •ë°€ë„: {precision}, ì¬í˜„ìœ¨: {recall}")
# ì •í™•ë„: 0.8, ì •ë°€ë„: 1.0, ì¬í˜„ìœ¨: 0.6666666666666666
```

í•˜ì§€ë§Œ **ìƒì„± ëª¨ë¸(Generative Model)**ì€ ë‹¤ìŒê³¼ ê°™ì€ ê·¼ë³¸ì ì¸ ì–´ë ¤ì›€ì´ ìˆë‹¤:

- **ì •ë‹µì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ**: ìƒˆë¡œìš´ ìƒ˜í”Œì„ ìƒì„±í•˜ë¯€ë¡œ í•™ìŠµ ë°ì´í„°ì™€ ì§ì ‘ ë¹„êµ ë¶ˆê°€ëŠ¥
- **ì•”ê¸° vs í•™ìŠµ êµ¬ë¶„ ì–´ë ¤ì›€**: í•™ìŠµ ë°ì´í„°ì™€ ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ ë‹¨ìˆœ ì•”ê¸°ì¼ ê°€ëŠ¥ì„±
- **ì£¼ê´€ì  í‰ê°€ ì˜ì¡´**: ì‚¬ëŒì˜ í‰ê°€ê°€ í•„ìš”í•˜ì§€ë§Œ ì£¼ê´€ì„±ê³¼ ë¹„ìš© ë¬¸ì œ ì¡´ì¬

> ìƒì„± ëª¨ë¸ì˜ ëª©í‘œëŠ” **ë°ì´í„° ë¶„í¬ í•™ìŠµ**ì´ì§€ **ë°ì´í„° ì•”ê¸°**ê°€ ì•„ë‹ˆë¯€ë¡œ, ë‹¨ìˆœíˆ í•™ìŠµ ë°ì´í„°ì™€ì˜ ìœ ì‚¬ë„ë§Œìœ¼ë¡œëŠ” í‰ê°€í•  ìˆ˜ ì—†ë‹¤. {: .prompt-tip}

### í‰ê°€ì˜ ë‘ ì¶•: í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±

ìƒì„± ëª¨ë¸ì„ í‰ê°€í•  ë•ŒëŠ” í¬ê²Œ **ë‘ ê°€ì§€ ê´€ì **ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤:

- **í’ˆì§ˆ(Quality/Fidelity)**: ì–¼ë§ˆë‚˜ ì‹¤ì œ ë°ì´í„°ì²˜ëŸ¼ ê·¸ëŸ´ë“¯í•˜ê²Œ ìƒì„±í–ˆëŠ”ê°€?
- **ë‹¤ì–‘ì„±(Diversity)**: ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ìƒì„±í•  ìˆ˜ ìˆëŠ”ê°€?

```mermaid
graph TD
    A[ìƒì„± ëª¨ë¸ í‰ê°€] --> B[í’ˆì§ˆ Quality]
    A --> C[ë‹¤ì–‘ì„± Diversity]
    B --> D[ì„ ëª…ë„]
    B --> E[ë””í…Œì¼ í‘œí˜„]
    B --> F[ì‹¤ì‚¬ì„±]
    C --> G[ë¶„í¬ ì»¤ë²„ë¦¬ì§€]
    C --> H[ìƒ˜í”Œ ë‹¤ì–‘ì„±]
    C --> I[ëª¨ë“œ ë¶•ê´´ ë°©ì§€]
```

> í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì€ ë³´í†µ **íŠ¸ë ˆì´ë“œì˜¤í”„ ê´€ê³„**ì— ìˆë‹¤. í’ˆì§ˆì„ ë†’ì´ë ¤ë©´ ë‹¤ì–‘ì„±ì´ ê°ì†Œí•˜ê³ , ë‹¤ì–‘ì„±ì„ ë†’ì´ë ¤ë©´ í’ˆì§ˆì´ ë–¨ì–´ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤. {: .prompt-warning}

## ğŸ“Š Inception Score (IS): í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì˜ ì²« ë²ˆì§¸ ì‹œë„

### ê°œë…ê³¼ ë™ì‘ ì›ë¦¬

**Inception Score**ëŠ” ì‚¬ì „ í›ˆë ¨ëœ **Inception-v3 ë¶„ë¥˜ê¸°**ë¥¼ í™œìš©í•˜ì—¬ ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— í‰ê°€í•˜ëŠ” ì§€í‘œë‹¤.

í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

- **ë†’ì€ í’ˆì§ˆ**: ë¶„ë¥˜ê¸°ê°€ ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ ë†’ì€ í™•ì‹ ìœ¼ë¡œ íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ â†’ **ë‚®ì€ ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼**
- **ë†’ì€ ë‹¤ì–‘ì„±**: ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ì´ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ì— ê³ ë¥´ê²Œ ë¶„í¬ â†’ **ë†’ì€ ì£¼ë³€ ì—”íŠ¸ë¡œí”¼**

### ìˆ˜í•™ì  í‘œí˜„

**ì—”íŠ¸ë¡œí”¼(Entropy)**ëŠ” ë¶ˆí™•ì‹¤ì„±ì´ë‚˜ ë¬´ì§ˆì„œë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë‹¤:

$$ H(X) = -\sum_{i} p(x_i) \log p(x_i) $$

Inception ScoreëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤:

$$ \text{IS} = \exp\left(\mathbb{E}_{x}\left[D_{KL}(p(y|x) \parallel p(y))\right]\right) $$

ì—¬ê¸°ì„œ:

- $$ p(y|x) $$: ì´ë¯¸ì§€ xê°€ ì£¼ì–´ì¡Œì„ ë•Œ í´ë˜ìŠ¤ yì˜ ì¡°ê±´ë¶€ í™•ë¥  (í’ˆì§ˆ ì¸¡ì •)
- $$ p(y) $$: ëª¨ë“  ìƒì„± ì´ë¯¸ì§€ì— ëŒ€í•œ í´ë˜ìŠ¤ì˜ ì£¼ë³€ í™•ë¥  (ë‹¤ì–‘ì„± ì¸¡ì •)
- $$ D_{KL} $$: KL Divergence

### êµ¬í˜„ ì˜ˆì‹œ

```python
import torch
import torch.nn.functional as F
from torchvision import models, transforms
import numpy as np

def calculate_inception_score(images, batch_size=32, splits=10):
    """
    Inception Score ê³„ì‚°
    
    Args:
        images: ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ (torch.Tensor)
        batch_size: ë°°ì¹˜ í¬ê¸°
        splits: í‰ê· ì„ ë‚´ê¸° ìœ„í•œ ë¶„í•  ìˆ˜
    """
    # ì‚¬ì „ í›ˆë ¨ëœ Inception-v3 ëª¨ë¸ ë¡œë“œ
    inception_model = models.inception_v3(pretrained=True, transform_input=False)
    inception_model.eval()
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    preprocess = transforms.Compose([
        transforms.Resize(299),
        transforms.CenterCrop(299),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    def get_predictions(images):
        """ë¶„ë¥˜ê¸° ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°"""
        predictions = []
        
        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size]
            batch = torch.stack([preprocess(img) for img in batch])
            
            with torch.no_grad():
                outputs = inception_model(batch)
                predictions.append(F.softmax(outputs, dim=1))
        
        return torch.cat(predictions, dim=0)
    
    # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
    preds = get_predictions(images)
    
    # Inception Score ê³„ì‚°
    scores = []
    
    for i in range(splits):
        part = preds[i * len(preds) // splits:(i + 1) * len(preds) // splits]
        
        # ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼ (í’ˆì§ˆ)
        kl_div = part * (torch.log(part) - torch.log(torch.mean(part, dim=0, keepdim=True)))
        kl_div = torch.mean(torch.sum(kl_div, dim=1))
        
        scores.append(torch.exp(kl_div))
    
    return torch.mean(torch.stack(scores)), torch.std(torch.stack(scores))

# ì‚¬ìš© ì˜ˆì‹œ
# generated_images = torch.randn(1000, 3, 299, 299)  # ìƒì„±ëœ ì´ë¯¸ì§€
# is_mean, is_std = calculate_inception_score(generated_images)
# print(f"Inception Score: {is_mean:.2f} Â± {is_std:.2f}")
```

### ì¥ì ê³¼ í•œê³„ì 

**ì¥ì :**

- í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— ê³ ë ¤
- êµ¬í˜„ì´ ìƒëŒ€ì ìœ¼ë¡œ ê°„ë‹¨
- ë„ë¦¬ ì‚¬ìš©ë˜ì–´ ë¹„êµ ê¸°ì¤€ìœ¼ë¡œ í™œìš©

**í•œê³„ì :**

- **ImageNet í´ë˜ìŠ¤ì—ë§Œ ì ìš© ê°€ëŠ¥**: ì–¼êµ´, í’ê²½ ë“±ì€ í‰ê°€ ë¶ˆê°€
- **Adversarial attackì— ì·¨ì•½**: ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ë„ ë†’ì€ ì ìˆ˜ ê°€ëŠ¥
- **Mode collapse íƒì§€ ì‹¤íŒ¨**: ê° í´ë˜ìŠ¤ë§ˆë‹¤ í•œ ì¥ì”©ë§Œ ìƒì„±í•´ë„ ë†’ì€ ì ìˆ˜

> ISëŠ” ImageNetê³¼ ê°™ì€ ê°ì²´ ë¶„ë¥˜ ë°ì´í„°ì…‹ì—ì„œë§Œ ì˜ë¯¸ê°€ ìˆìœ¼ë©°, ì–¼êµ´ì´ë‚˜ í’ê²½ ì´ë¯¸ì§€ ìƒì„±ì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ëŠ” ì¹˜ëª…ì  í•œê³„ê°€ ìˆë‹¤. {: .prompt-warning}

## ğŸ¨ FrÃ©chet Inception Distance (FID): í˜„ì¬ í‘œì¤€ ì§€í‘œ

### ê°œë…ê³¼ ë™ì‘ ì›ë¦¬

**FID(FrÃ©chet Inception Distance)**ëŠ” Inception Scoreì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ ì§€í‘œë¡œ, **ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„± ë°ì´í„°ì˜ íŠ¹ì„± ë¶„í¬ ê°„ ê±°ë¦¬**ë¥¼ ì¸¡ì •í•œë‹¤.

í•µì‹¬ ì•„ì´ë””ì–´:

- ì‚¬ì „ í›ˆë ¨ëœ Inception-v3ì˜ **íŠ¹ì„± ë²¡í„°(feature vector)** í™œìš© (í™•ë¥  ë²¡í„° ëŒ€ì‹ )
- ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„± ë°ì´í„°ì˜ **íŠ¹ì„± ë¶„í¬**ë¥¼ ê°ê° ê°€ìš°ì‹œì•ˆìœ¼ë¡œ ê°€ì •
- ë‘ ê°€ìš°ì‹œì•ˆ ë¶„í¬ ê°„ì˜ **FrÃ©chet distance** ê³„ì‚°

### ìˆ˜í•™ì  í‘œí˜„

ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„± ë°ì´í„°ì˜ íŠ¹ì„±ì´ ê°ê° ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •:

$$ \mathcal{N}(\mu_r, \Sigma_r), \quad \mathcal{N}(\mu_g, \Sigma_g) $$

FIDëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤:

$$ \text{FID} = |\mu_r - \mu_g|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}) $$

ì—¬ê¸°ì„œ:

- $$ \mu_r, \mu_g $$: ì‹¤ì œ/ìƒì„± ë°ì´í„° íŠ¹ì„±ì˜ í‰ê· 
- $$ \Sigma_r, \Sigma_g $$: ì‹¤ì œ/ìƒì„± ë°ì´í„° íŠ¹ì„±ì˜ ê³µë¶„ì‚° í–‰ë ¬
- $$ \text{Tr} $$: í–‰ë ¬ì˜ ëŒ€ê°í•©(trace)

### êµ¬í˜„ ì˜ˆì‹œ

```python
import torch
import numpy as np
from scipy.linalg import sqrtm
from torchvision import models, transforms

def calculate_fid(real_images, generated_images, batch_size=50):
    """
    FID(FrÃ©chet Inception Distance) ê³„ì‚°
    
    Args:
        real_images: ì‹¤ì œ ì´ë¯¸ì§€ë“¤
        generated_images: ìƒì„±ëœ ì´ë¯¸ì§€ë“¤
        batch_size: ë°°ì¹˜ í¬ê¸°
    """
    
    # Inception-v3 ëª¨ë¸ ë¡œë“œ (ë¶„ë¥˜ì¸µ ì œê±°)
    inception = models.inception_v3(pretrained=True, transform_input=False)
    inception.fc = torch.nn.Identity()  # ë¶„ë¥˜ì¸µ ì œê±°
    inception.eval()
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    preprocess = transforms.Compose([
        transforms.Resize(299),
        transforms.CenterCrop(299),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    def extract_features(images):
        """íŠ¹ì„± ë²¡í„° ì¶”ì¶œ"""
        features = []
        
        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size]
            batch = torch.stack([preprocess(img) for img in batch])
            
            with torch.no_grad():
                # Inception-v3ì˜ ë§ˆì§€ë§‰ pooling layer ì¶œë ¥ (2048ì°¨ì›)
                feat = inception(batch)
                features.append(feat)
        
        return torch.cat(features, dim=0).numpy()
    
    # íŠ¹ì„± ì¶”ì¶œ
    real_features = extract_features(real_images)
    generated_features = extract_features(generated_images)
    
    # í‰ê· ê³¼ ê³µë¶„ì‚° ê³„ì‚°
    mu_real = np.mean(real_features, axis=0)
    mu_gen = np.mean(generated_features, axis=0)
    
    sigma_real = np.cov(real_features, rowvar=False)
    sigma_gen = np.cov(generated_features, rowvar=False)
    
    # FID ê³„ì‚°
    mu_diff = mu_real - mu_gen
    
    # ê³µë¶„ì‚° í–‰ë ¬ì˜ ì œê³±ê·¼ ê³„ì‚°
    covmean = sqrtm(sigma_real.dot(sigma_gen))
    
    # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ í—ˆìˆ˜ë¶€ ì œê±°
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    
    fid = mu_diff.dot(mu_diff) + np.trace(sigma_real + sigma_gen - 2 * covmean)
    
    return fid

# ì‚¬ìš© ì˜ˆì‹œ
# real_imgs = torch.randn(1000, 3, 256, 256)
# gen_imgs = torch.randn(1000, 3, 256, 256)
# fid_score = calculate_fid(real_imgs, gen_imgs)
# print(f"FID Score: {fid_score:.2f}")
```

### ì¥ì ê³¼ í•œê³„ì 

**ì¥ì :**

- **ImageNetì— êµ­í•œë˜ì§€ ì•ŠìŒ**: ëª¨ë“  ì¢…ë¥˜ì˜ ì´ë¯¸ì§€ í‰ê°€ ê°€ëŠ¥
- **Adversarial attackì— ê°•ê±´**: íŠ¹ì„± ë²¡í„° ì‚¬ìš©ìœ¼ë¡œ ë…¸ì´ì¦ˆì— ëœ ë¯¼ê°
- **ì‹¤ì œ ë°ì´í„° í™œìš©**: ìƒì„± ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ë¥¼ ëª¨ë‘ ê³ ë ¤
- **ì‚¬ëŒì˜ ì¸ì‹ê³¼ ìœ ì‚¬**: ì‹œê°ì  í’ˆì§ˆê³¼ ìƒê´€ê´€ê³„ ë†’ìŒ

**í•œê³„ì :**

- **í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± ë¶„ë¦¬ ë¶ˆê°€**: ë‹¨ì¼ ì ìˆ˜ë¡œë§Œ ì œê³µ
- **ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜ í•„ìš”**: í†µê³„ì  ì•ˆì •ì„±ì„ ìœ„í•´ ë§ì€ ìƒ˜í”Œ í•„ìš”
- **ê³„ì‚° ë¹„ìš©**: ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ íŠ¹ì„± ì¶”ì¶œ í•„ìš”

> FIDëŠ” í˜„ì¬ **ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ”** ìƒì„± ëª¨ë¸ í‰ê°€ ì§€í‘œë¡œ, ëŒ€ë¶€ë¶„ì˜ ë…¼ë¬¸ì—ì„œ í‘œì¤€ ì§€í‘œë¡œ ì±„íƒí•˜ê³  ìˆë‹¤. {: .prompt-tip}

## âš–ï¸ Precisionê³¼ Recall: í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì˜ ë¶„ë¦¬ ì¸¡ì •

### ìƒì„± ëª¨ë¸ì—ì„œì˜ Precisionê³¼ Recall

íŒë³„ ëª¨ë¸ì˜ Precision/Recall ê°œë…ì„ ìƒì„± ëª¨ë¸ì— ì ìš©í•œ ì§€í‘œë‹¤. **í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ ê°ê° ë¶„ë¦¬í•´ì„œ ì¸¡ì •**í•  ìˆ˜ ìˆë‹¤ëŠ” í° ì¥ì ì´ ìˆë‹¤.

**ê°œë…ì  ì •ì˜:**

- **Precision (ì •ë°€ë„)**: ìƒì„±ëœ ìƒ˜í”Œ ì¤‘ ì‹¤ì œ ë°ì´í„° ë¶„í¬ì— ì†í•˜ëŠ” ë¹„ìœ¨ â†’ **í’ˆì§ˆ ì¸¡ì •**
- **Recall (ì¬í˜„ìœ¨)**: ì‹¤ì œ ë°ì´í„° ë¶„í¬ ì¤‘ ìƒì„±ëœ ìƒ˜í”Œë¡œ ì»¤ë²„ë˜ëŠ” ë¹„ìœ¨ â†’ **ë‹¤ì–‘ì„± ì¸¡ì •**

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°]

### ê·¼ë°©(Neighborhood) ê¸°ë°˜ ê³„ì‚°

Precisionê³¼ Recall ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” **ê·¼ë°©(neighborhood)** ê°œë…ì„ ì •ì˜í•´ì•¼ í•œë‹¤:

1. ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ ë‹¤ë¥¸ ìƒ˜í”Œì„ ì°¾ìŒ
2. ê·¸ ê±°ë¦¬ë¥¼ ë°˜ì§€ë¦„ìœ¼ë¡œ í•˜ëŠ” ì›ì„ ê·¸ë¦¼
3. ì´ ì›ì´ í•´ë‹¹ ìƒ˜í”Œì˜ ê·¼ë°©ì´ ë¨

### êµ¬í˜„ ì˜ˆì‹œ

```python
import torch
import numpy as np
from sklearn.metrics import pairwise_distances
from scipy.spatial.distance import cdist

def calculate_precision_recall(real_features, generated_features, k=3):
    """
    ìƒì„± ëª¨ë¸ì˜ Precisionê³¼ Recall ê³„ì‚°
    
    Args:
        real_features: ì‹¤ì œ ë°ì´í„°ì˜ íŠ¹ì„± ë²¡í„°
        generated_features: ìƒì„± ë°ì´í„°ì˜ íŠ¹ì„± ë²¡í„°  
        k: kë²ˆì§¸ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì„ ê·¼ë°© ë°˜ì§€ë¦„ìœ¼ë¡œ ì‚¬ìš©
    """
    
    def compute_pairwise_distances(X, Y=None):
        """ë‘ ë°ì´í„°ì…‹ ê°„ ê±°ë¦¬ ê³„ì‚°"""
        if Y is None:
            Y = X
        return cdist(X, Y, metric='euclidean')
    
    def get_kth_nearest_distance(X, k):
        """kë²ˆì§¸ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚°"""
        distances = compute_pairwise_distances(X, X)
        # ìê¸° ìì‹  ì œì™¸í•˜ê³  kë²ˆì§¸ ê±°ë¦¬
        kth_distances = np.partition(distances, k, axis=1)[:, k]
        return kth_distances
    
    # ì‹¤ì œ ë°ì´í„°ì˜ kë²ˆì§¸ ê·¼ë°© ë°˜ì§€ë¦„ ê³„ì‚°
    real_kth_distances = get_kth_nearest_distance(real_features, k)
    
    # ìƒì„± ë°ì´í„°ì˜ kë²ˆì§¸ ê·¼ë°© ë°˜ì§€ë¦„ ê³„ì‚°  
    gen_kth_distances = get_kth_nearest_distance(generated_features, k)
    
    # Precision ê³„ì‚°: ìƒì„±ëœ ìƒ˜í”Œ ì¤‘ ì‹¤ì œ ë°ì´í„° ê·¼ë°©ì— ì†í•˜ëŠ” ë¹„ìœ¨
    real_gen_distances = compute_pairwise_distances(real_features, generated_features)
    precision_count = 0
    
    for i, gen_point in enumerate(generated_features):
        # ê°€ì¥ ê°€ê¹Œìš´ ì‹¤ì œ ë°ì´í„° ì  ì°¾ê¸°
        min_real_idx = np.argmin(real_gen_distances[:, i])
        min_distance = real_gen_distances[min_real_idx, i]
        
        # ìƒì„± ì ì´ ì‹¤ì œ ë°ì´í„°ì˜ ê·¼ë°©ì— ì†í•˜ëŠ”ì§€ í™•ì¸
        if min_distance <= real_kth_distances[min_real_idx]:
            precision_count += 1
    
    precision = precision_count / len(generated_features)
    
    # Recall ê³„ì‚°: ì‹¤ì œ ë°ì´í„° ì¤‘ ìƒì„± ë°ì´í„° ê·¼ë°©ì— ì†í•˜ëŠ” ë¹„ìœ¨
    gen_real_distances = compute_pairwise_distances(generated_features, real_features)
    recall_count = 0
    
    for i, real_point in enumerate(real_features):
        # ê°€ì¥ ê°€ê¹Œìš´ ìƒì„± ë°ì´í„° ì  ì°¾ê¸°
        min_gen_idx = np.argmin(gen_real_distances[:, i])
        min_distance = gen_real_distances[min_gen_idx, i]
        
        # ì‹¤ì œ ì ì´ ìƒì„± ë°ì´í„°ì˜ ê·¼ë°©ì— ì†í•˜ëŠ”ì§€ í™•ì¸
        if min_distance <= gen_kth_distances[min_gen_idx]:
            recall_count += 1
    
    recall = recall_count / len(real_features)
    
    return precision, recall

# ì‹œê°í™”ë¥¼ ìœ„í•œ 2D ì˜ˆì‹œ
def visualize_precision_recall():
    """Precision/Recall ê°œë… ì‹œê°í™”"""
    import matplotlib.pyplot as plt
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    real_data = np.random.multivariate_normal([2, 2], [[1, 0], [0, 1]], 100)
    gen_data = np.random.multivariate_normal([2.5, 2.5], [[1.5, 0], [0, 1.5]], 80)
    
    plt.figure(figsize=(12, 5))
    
    # ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„± ë°ì´í„° í”Œë¡¯
    plt.subplot(1, 2, 1)
    plt.scatter(real_data[:, 0], real_data[:, 1], 
               alpha=0.6, c='blue', label='Real Data')
    plt.scatter(gen_data[:, 0], gen_data[:, 1], 
               alpha=0.6, c='red', label='Generated Data')
    plt.legend()
    plt.title('Real vs Generated Data')
    plt.grid(True, alpha=0.3)
    
    # Precision/Recall ê³„ì‚° ë° ì‹œê°í™”
    precision, recall = calculate_precision_recall(real_data, gen_data)
    
    plt.subplot(1, 2, 2)
    plt.bar(['Precision', 'Recall'], [precision, recall], 
           color=['orange', 'green'], alpha=0.7)
    plt.ylim(0, 1)
    plt.title(f'Precision: {precision:.3f}, Recall: {recall:.3f}')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# visualize_precision_recall()
```

### ì¥ì ê³¼ í•œê³„ì 

**ì¥ì :**

- **í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± ë¶„ë¦¬ ì¸¡ì •**: ê°ê°ì„ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€ ê°€ëŠ¥
- **ì§ê´€ì  í•´ì„**: íŒë³„ ëª¨ë¸ì˜ Precision/Recallê³¼ ìœ ì‚¬í•œ í•´ì„
- **ì„¸ë°€í•œ ë¶„ì„**: ëª¨ë¸ì˜ ê°•ì•½ì  íŒŒì•…ì— ìœ ìš©

**í•œê³„ì :**

- **Outlierì— ë¯¼ê°**: ì†Œìˆ˜ì˜ ì´ìƒì¹˜ê°€ ì ìˆ˜ë¥¼ í¬ê²Œ ì¢Œìš°
- **ìƒ˜í”Œë§ì— ì˜ì¡´**: ë™ì¼í•œ ë¶„í¬ë¼ë„ ìƒ˜í”Œë§ì— ë”°ë¼ ì ìˆ˜ ë³€ë™
- **ë†’ì€ ê³„ì‚° ë¹„ìš©**: ëª¨ë“  ìŒì— ëŒ€í•œ ê±°ë¦¬ ê³„ì‚° í•„ìš”
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ì„±**: kê°’ì— ë”°ë¼ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§

> Precisionê³¼ Recallì€ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì˜ **íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ëª…í™•íˆ ë³´ì—¬ì£¼ëŠ”** ìœ ì¼í•œ ì§€í‘œë¡œ, ëª¨ë¸ ê°œì„  ë°©í–¥ì„ ê²°ì •í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•˜ë‹¤. {: .prompt-tip}

## ğŸ­ ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ì˜ í‰ê°€ ì§€í‘œ

### ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ì´ë€?

**ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸(Conditional Generative Model)**ì€ íŠ¹ì • ì¡°ê±´(í´ë˜ìŠ¤, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“±)ì´ ì£¼ì–´ì¡Œì„ ë•Œ ê·¸ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

```python
# ì¡°ê±´ë¶€ ìƒì„± ì˜ˆì‹œ
def conditional_generation_example():
    """ì¡°ê±´ë¶€ ìƒì„±ì˜ ê°œë… ì„¤ëª…"""
    
    # ì¼ë°˜ ìƒì„± ëª¨ë¸: ëœë¤ ìƒ˜í”Œ
    # generated_image = model.generate()  # ë¬´ì—‡ì´ ë‚˜ì˜¬ì§€ ëª¨ë¦„
    
    # ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸: ì¡°ê±´ ì§€ì •
    # digit_3 = model.generate(condition="digit_3")  # ìˆ«ì 3 ìƒì„±
    # cat_image = model.generate(condition="cat")     # ê³ ì–‘ì´ ì´ë¯¸ì§€ ìƒì„±
    # zebra_from_horse = model.generate(condition=horse_image, target="zebra")
    
    print("ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ì˜ ì¥ì :")
    print("1. ìƒì„± ê²°ê³¼ ì œì–´ ê°€ëŠ¥")
    print("2. íŠ¹ì • ìš©ë„ì— ë§ëŠ” ë°ì´í„° ìƒì„±")
    print("3. ë°ì´í„° ë³€í™˜ ì‘ì—… ìˆ˜í–‰ ê°€ëŠ¥")

conditional_generation_example()
```

### ê¸°ì¡´ ì§€í‘œì˜ í•œê³„

FID, IS, Precision/Recall ë“± ê¸°ì¡´ ì§€í‘œë“¤ì€ **ì¡°ê±´ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ì§€ ëª»í•œë‹¤**ëŠ” ì¹˜ëª…ì  í•œê³„ê°€ ìˆë‹¤.

> ì˜ˆë¥¼ ë“¤ì–´, "ìˆ«ì 3ì„ ìƒì„±í•˜ë¼"ëŠ” ì¡°ê±´ì„ ì£¼ì—ˆëŠ”ë° ëª¨ë¸ì´ ê³„ì† ë‹¤ë¥¸ ìˆ«ìë¥¼ ìƒì„±í•´ë„ ê¸°ì¡´ ì§€í‘œë“¤ì€ ì´ë¥¼ **í˜ë„í‹°ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤**. {: .prompt-warning}

### Intra-FID: í´ë˜ìŠ¤ë³„ FID

**Intra-FID**ëŠ” ê¸°ì¡´ FIDë¥¼ í´ë˜ìŠ¤ë³„ë¡œ ë‚˜ëˆ„ì–´ ê³„ì‚°í•œ í›„ í‰ê· ì„ ë‚´ëŠ” ë°©ì‹ì´ë‹¤.

```python
def calculate_intra_fid(real_images_by_class, generated_images_by_class):
    """
    í´ë˜ìŠ¤ë³„ FID ê³„ì‚° í›„ í‰ê· 
    
    Args:
        real_images_by_class: {class_id: [images]} í˜•íƒœì˜ ì‹¤ì œ ì´ë¯¸ì§€
        generated_images_by_class: {class_id: [images]} í˜•íƒœì˜ ìƒì„± ì´ë¯¸ì§€
    """
    
    class_fids = []
    
    for class_id in real_images_by_class.keys():
        if class_id in generated_images_by_class:
            real_imgs = real_images_by_class[class_id]
            gen_imgs = generated_images_by_class[class_id]
            
            # í•´ë‹¹ í´ë˜ìŠ¤ì— ëŒ€í•´ì„œë§Œ FID ê³„ì‚°
            class_fid = calculate_fid(real_imgs, gen_imgs)
            class_fids.append(class_fid)
            
            print(f"Class {class_id} FID: {class_fid:.2f}")
    
    intra_fid = np.mean(class_fids)
    print(f"Intra-FID (í‰ê· ): {intra_fid:.2f}")
    
    return intra_fid

# ì‚¬ìš© ì˜ˆì‹œ (ì˜ì‚¬ ì½”ë“œ)
# real_by_class = {0: [digit0_images], 1: [digit1_images], ...}
# gen_by_class = {0: [generated_digit0], 1: [generated_digit1], ...}
# intra_fid = calculate_intra_fid(real_by_class, gen_by_class)
```

### Classification Accuracy: ë¶„ë¥˜ê¸° ê¸°ë°˜ í‰ê°€

ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ **ì‚¬ì „ í›ˆë ¨ëœ ë¶„ë¥˜ê¸°**ì— ì…ë ¥í•˜ì—¬ ì˜ë„í•œ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë˜ëŠ” ë¹„ìœ¨ì„ ì¸¡ì •í•œë‹¤.

```python
def calculate_classification_accuracy(generated_images, target_labels, classifier):
    """
    ìƒì„±ëœ ì´ë¯¸ì§€ì˜ ë¶„ë¥˜ ì •í™•ë„ ê³„ì‚°
    
    Args:
        generated_images: ìƒì„±ëœ ì´ë¯¸ì§€ë“¤
        target_labels: ìƒì„± ì˜ë„í•œ ë¼ë²¨ë“¤
        classifier: ì‚¬ì „ í›ˆë ¨ëœ ë¶„ë¥˜ê¸°
    """
    
    classifier.eval()
    correct = 0
    total = len(generated_images)
    
    with torch.no_grad():
        for img, target in zip(generated_images, target_labels):
            # ë¶„ë¥˜ê¸° ì˜ˆì¸¡
            output = classifier(img.unsqueeze(0))
            predicted = output.argmax(dim=1)
            
            if predicted.item() == target:
                correct += 1
    
    accuracy = correct / total
    print(f"Classification Accuracy: {accuracy:.3f}")
    
    return accuracy

# í–¥ìƒëœ ë°©ë²•: ìƒì„± í›„ ë¶„ë¥˜ê¸° í•™ìŠµ
def enhanced_classification_evaluation(generator, real_data, num_classes):
    """
    ìƒì„± í›„ ë¶„ë¥˜ê¸° í•™ìŠµ ë°©ì‹
    
    1. ìƒì„±ê¸°ë¡œ ì¡°ê±´ë¶€ ì´ë¯¸ì§€ ìƒì„±
    2. ìƒì„±ëœ ì´ë¯¸ì§€ë¡œ ë¶„ë¥˜ê¸° í•™ìŠµ  
    3. í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë¶„ë¥˜ê¸° ì„±ëŠ¥ í‰ê°€
    """
    
    # 1. ê° í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ìƒì„±
    generated_data = []
    generated_labels = []
    
    for class_id in range(num_classes):
        class_images = generator.generate(condition=class_id, num_samples=1000)
        generated_data.extend(class_images)
        generated_labels.extend([class_id] * 1000)
    
    # 2. ìƒì„±ëœ ë°ì´í„°ë¡œ ë¶„ë¥˜ê¸° í•™ìŠµ
    classifier = train_classifier(generated_data, generated_labels)
    
    # 3. ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
    test_accuracy = evaluate_classifier(classifier, real_data['test'])
    
    print(f"Enhanced Classification Score: {test_accuracy:.3f}")
    
    return test_accuracy
```

**í•œê³„ì :**

- **ë¶„ë¥˜ê¸° ì„±ëŠ¥ì— ì˜ì¡´**: ë¶„ë¥˜ê¸°ê°€ ì¢‹ì§€ ì•Šìœ¼ë©´ í‰ê°€ ìì²´ê°€ ë¶€ì •í™•
- **ê²½ê³„ ì¡°ì‘ ê°€ëŠ¥**: ë¶„ë¥˜ ê²½ê³„ë§Œ ë„˜ìœ¼ë©´ ë˜ë¯€ë¡œ í’ˆì§ˆì´ ë‚®ì•„ë„ ë†’ì€ ì ìˆ˜
- **ë‹¤ì–‘ì„± ë¬´ì‹œ**: ë™ì¼í•œ ì´ë¯¸ì§€ë§Œ ë°˜ë³µ ìƒì„±í•´ë„ ë†’ì€ ì ìˆ˜

## ğŸ–¼ï¸ LPIPS: í•™ìŠµëœ ì§€ê°ì  ìœ ì‚¬ë„

### ê°œë…ê³¼ í™œìš©

**LPIPS(Learned Perceptual Image Patch Similarity)**ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ë„¤íŠ¸ì›Œí¬ì˜ **íŠ¹ì„± ê³µê°„ì—ì„œ ì´ë¯¸ì§€ ê°„ ìœ ì‚¬ë„**ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œë‹¤.

ì£¼ìš” í™œìš©ì²˜:

- **ì´ë¯¸ì§€ ë³€í™˜ ì‘ì—…**ì—ì„œ ì›ë³¸ê³¼ ë³€í™˜ëœ ì´ë¯¸ì§€ ê°„ì˜ **ë‹¤ì–‘ì„± ì¸¡ì •**
- **Style Transfer**, **Image-to-Image Translation** ë“±ì˜ í‰ê°€

### êµ¬í˜„ ì˜ˆì‹œ

```python
import torch
import torch.nn as nn
from torchvision import models

class LPIPS(nn.Module):
    """LPIPS ìœ ì‚¬ë„ ê³„ì‚° ëª¨ë“ˆ"""
    
    def __init__(self, net='vgg', version='0.1'):
        super(LPIPS, self).__init__()
        
        # ì‚¬ì „ í›ˆë ¨ëœ ë„¤íŠ¸ì›Œí¬ ë¡œë“œ (VGG, AlexNet ë“±)
        if net == 'vgg':
            self.net = models.vgg16(pretrained=True).features
        elif net == 'alex':
            self.net = models.alexnet(pretrained=True).features
        
        # ë„¤íŠ¸ì›Œí¬ ë™ê²°
        for param in self.net.parameters():
            param.requires_grad = False
            
        self.net.eval()
    
    def extract_features(self, x, layers=[4, 9, 16, 23, 30]):
        """ì—¬ëŸ¬ ë ˆì´ì–´ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        for i, layer in enumerate(self.net):
            x = layer(x)
            if i in layers:
                features.append(x)
        
        return features
    
    def forward(self, img1, img2):
        """ë‘ ì´ë¯¸ì§€ ê°„ LPIPS ê±°ë¦¬ ê³„ì‚°"""
        
        # íŠ¹ì„± ì¶”ì¶œ
        feats1 = self.extract_features(img1)
        feats2 = self.extract_features(img2)
        
        # ê° ë ˆì´ì–´ë³„ ê±°ë¦¬ ê³„ì‚°
        distances = []
        
        for f1, f2 in zip(feats1, feats2):
            # L2 ì •ê·œí™”
            f1_norm = f1 / (torch.norm(f1, dim=1, keepdim=True) + 1e-10)
            f2_norm = f2 / (torch.norm(f2, dim=1, keepdim=True) + 1e-10)
            
            # ê³µê°„ì  í‰ê· 
            dist = torch.mean((f1_norm - f2_norm) ** 2, dim=[2, 3])
            distances.append(dist)
        
        # ê°€ì¤‘ í‰ê·  (í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        total_distance = sum(distances) / len(distances)
        
        return torch.mean(total_distance)

def evaluate_diversity_with_lpips(original_images, generated_images):
    """LPIPSë¥¼ ì‚¬ìš©í•œ ë‹¤ì–‘ì„± í‰ê°€"""
    
    lpips_metric = LPIPS(net='vgg')
    distances = []
    
    for orig, gen in zip(original_images, generated_images):
        # ì›ë³¸ê³¼ ìƒì„± ì´ë¯¸ì§€ ê°„ ê±°ë¦¬
        distance = lpips_metric(orig.unsqueeze(0), gen.unsqueeze(0))
        distances.append(distance.item())
    
    avg_distance = np.mean(distances)
    
    print(f"í‰ê·  LPIPS ê±°ë¦¬: {avg_distance:.4f}")
    print("ê±°ë¦¬ê°€ í´ìˆ˜ë¡ ë” ë‹¤ì–‘í•œ ë³€í™˜ì„ ì˜ë¯¸")
    
    return avg_distance

# ì‚¬ìš© ì˜ˆì‹œ
# horse_images = [...]  # ë§ ì´ë¯¸ì§€ë“¤
# zebra_images = [...]  # ë³€í™˜ëœ ì–¼ë£©ë§ ì´ë¯¸ì§€ë“¤
# diversity_score = evaluate_diversity_with_lpips(horse_images, zebra_images)
```

### í™œìš© ì˜ˆì‹œ: Style Transfer í‰ê°€

```python
def evaluate_style_transfer():
    """Style Transfer ê²°ê³¼ í‰ê°€ ì˜ˆì‹œ"""
    
    # ìŠ¤íƒ€ì¼ ì „ì´ ì „í›„ ë¹„êµ
    content_images = [...]  # ì›ë³¸ ì»¨í…ì¸  ì´ë¯¸ì§€
    styled_images = [...]   # ìŠ¤íƒ€ì¼ ì „ì´ëœ ì´ë¯¸ì§€
    
    lpips_scores = []
    
    for content, styled in zip(content_images, styled_images):
        score = lpips_metric(content, styled)
        lpips_scores.append(score.item())
    
    avg_lpips = np.mean(lpips_scores)
    
    print(f"Style Transfer LPIPS: {avg_lpips:.4f}")
    
    if avg_lpips < 0.3:
        print("ë³€í™˜ì´ ì•½í•¨ - ì›ë³¸ê³¼ ë„ˆë¬´ ìœ ì‚¬")
    elif avg_lpips > 0.7:
        print("ë³€í™˜ì´ ê³¼í•¨ - ì›ë³¸ ì •ë³´ ì†ì‹¤ ìœ„í—˜")
    else:
        print("ì ì ˆí•œ ìˆ˜ì¤€ì˜ ìŠ¤íƒ€ì¼ ë³€í™˜")
    
    return avg_lpips
```

## ğŸ¯ CLIP Score: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì¼ì¹˜ë„ í‰ê°€

### CLIP ëª¨ë¸ ì´í•´

**CLIP(Contrastive Language-Image Pre-training)**ì€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë™ì¼í•œ **ì„ë² ë”© ê³µê°„**ì—ì„œ í•™ìŠµí•œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ë‹¤.

í•µì‹¬ ì•„ì´ë””ì–´:

- í…ìŠ¤íŠ¸ ì„¤ëª…ê³¼ í•´ë‹¹ ì´ë¯¸ì§€ì˜ ì„ë² ë”©ì„ **ê°€ê¹ê²Œ** í•™ìŠµ
- ê´€ë ¨ ì—†ëŠ” í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìŒì˜ ì„ë² ë”©ì„ **ë©€ê²Œ** í•™ìŠµ

### CLIP Score ê³„ì‚°

**CLIP Score**ëŠ” ìƒì„±ëœ ì´ë¯¸ì§€ê°€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì¡°ê±´ì„ ì–¼ë§ˆë‚˜ ì˜ ë°˜ì˜í•˜ëŠ”ì§€ ì¸¡ì •í•œë‹¤.

```python
import clip
import torch
from PIL import Image

def calculate_clip_score(images, texts, device='cuda'):
    """
    CLIP Score ê³„ì‚°
    
    Args:
        images: ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ (PIL Image ë˜ëŠ” tensor)
        texts: í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        device: ê³„ì‚° ì¥ì¹˜
    """
    
    # CLIP ëª¨ë¸ ë¡œë“œ
    model, preprocess = clip.load("ViT-B/32", device=device)
    
    clip_scores = []
    
    for image, text in zip(images, texts):
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if isinstance(image, Image.Image):
            image_input = preprocess(image).unsqueeze(0).to(device)
        else:
            image_input = preprocess(image).to(device)
        
        # í…ìŠ¤íŠ¸ í† í°í™”
        text_input = clip.tokenize([text]).to(device)
        
        with torch.no_grad():
            # íŠ¹ì„± ì¶”ì¶œ
            image_features = model.encode_image(image_input)
            text_features = model.encode_text(text_input)
            
            # ì •ê·œí™”
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = torch.cosine_similarity(image_features, text_features)
            clip_scores.append(similarity.item())
    
    avg_clip_score = np.mean(clip_scores)
    
    print(f"í‰ê·  CLIP Score: {avg_clip_score:.4f}")
    print("ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì¼ì¹˜ë„ê°€ ë†’ìŒ")
    
    return avg_clip_score, clip_scores

def evaluate_text_to_image_generation():
    """í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ í‰ê°€ ì˜ˆì‹œ"""
    
    # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì™€ ìƒì„±ëœ ì´ë¯¸ì§€
    prompts = [
        "a red car driving on a highway",
        "a cat sitting on a windowsill", 
        "a beautiful sunset over the ocean",
        "a person playing guitar in a park"
    ]
    
    # ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ì—ì„œ ìƒì„±)
    generated_images = [...]  # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€ì‘í•˜ëŠ” ìƒì„± ì´ë¯¸ì§€
    
    # CLIP Score ê³„ì‚°
    avg_score, individual_scores = calculate_clip_score(generated_images, prompts)
    
    # ê°œë³„ ê²°ê³¼ ë¶„ì„
    for i, (prompt, score) in enumerate(zip(prompts, individual_scores)):
        print(f"í”„ë¡¬í”„íŠ¸ {i+1}: '{prompt}'")
        print(f"CLIP Score: {score:.4f}")
        
        if score > 0.3:
            print("âœ… í…ìŠ¤íŠ¸ì™€ ì˜ ì¼ì¹˜")
        elif score > 0.2:
            print("âš ï¸ ë¶€ë¶„ì  ì¼ì¹˜")
        else:
            print("âŒ í…ìŠ¤íŠ¸ì™€ ë¶ˆì¼ì¹˜")
        print()
    
    return avg_score

# evaluate_text_to_image_generation()
```

### ë‹¤ì–‘í•œ CLIP Score ë³€í˜•

```python
def directional_clip_score(source_images, target_images, source_texts, target_texts):
    """
    ë°©í–¥ì„± CLIP Score - ì´ë¯¸ì§€ í¸ì§‘ í‰ê°€ìš©
    
    í¸ì§‘ ì „í›„ ì´ë¯¸ì§€ì˜ ë³€í™”ê°€ í…ìŠ¤íŠ¸ ë³€í™”ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •
    """
    
    model, preprocess = clip.load("ViT-B/32")
    
    # ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì˜ íŠ¹ì„± ì¶”ì¶œ
    source_img_features = []
    target_img_features = []
    source_text_features = []
    target_text_features = []
    
    for src_img, tgt_img, src_txt, tgt_txt in zip(
        source_images, target_images, source_texts, target_texts):
        
        # ì´ë¯¸ì§€ íŠ¹ì„±
        src_img_feat = model.encode_image(preprocess(src_img).unsqueeze(0))
        tgt_img_feat = model.encode_image(preprocess(tgt_img).unsqueeze(0))
        
        # í…ìŠ¤íŠ¸ íŠ¹ì„±  
        src_txt_feat = model.encode_text(clip.tokenize([src_txt]))
        tgt_txt_feat = model.encode_text(clip.tokenize([tgt_txt]))
        
        source_img_features.append(src_img_feat)
        target_img_features.append(tgt_img_feat)
        source_text_features.append(src_txt_feat)
        target_text_features.append(tgt_txt_feat)
    
    # ë°©í–¥ì„± ê³„ì‚°
    img_direction = torch.cat(target_img_features) - torch.cat(source_img_features)
    text_direction = torch.cat(target_text_features) - torch.cat(source_text_features)
    
    # ì •ê·œí™”
    img_direction = img_direction / img_direction.norm(dim=-1, keepdim=True)
    text_direction = text_direction / text_direction.norm(dim=-1, keepdim=True)
    
    # ë°©í–¥ì„± ì¼ì¹˜ë„
    directional_similarity = torch.cosine_similarity(img_direction, text_direction)
    
    return directional_similarity.mean().item()

# ì‚¬ìš© ì˜ˆì‹œ: ì´ë¯¸ì§€ í¸ì§‘ í‰ê°€
# before_images = [...]  # í¸ì§‘ ì „ ì´ë¯¸ì§€
# after_images = [...]   # í¸ì§‘ í›„ ì´ë¯¸ì§€  
# before_texts = ["a person with brown hair", ...]
# after_texts = ["a person with blonde hair", ...]
# 
# directional_score = directional_clip_score(
#     before_images, after_images, before_texts, after_texts
# )
```

## ğŸ“ˆ í‰ê°€ ì§€í‘œ ì„ íƒ ê°€ì´ë“œë¼ì¸

### ìƒì„± ëª¨ë¸ ìœ í˜•ë³„ ì¶”ì²œ ì§€í‘œ

```mermaid
flowchart TD
    A[ìƒì„± ëª¨ë¸ í‰ê°€] --> B{ëª¨ë¸ ìœ í˜•}
    
    B --> C[ë¬´ì¡°ê±´ ìƒì„±]
    B --> D[ì¡°ê±´ë¶€ ìƒì„±]
    
    C --> E[FID + IS<br/>Precision/Recall]
    
    D --> F{ì¡°ê±´ ìœ í˜•}
    F --> G[í´ë˜ìŠ¤ ê¸°ë°˜]
    F --> H[í…ìŠ¤íŠ¸ ê¸°ë°˜]
    F --> I[ì´ë¯¸ì§€ ê¸°ë°˜]
    
    G --> J[Intra-FID<br/>Classification Accuracy]
    H --> K[CLIP Score<br/>FID]
    I --> L[LPIPS<br/>FID]
```

### ì§€í‘œë³„ íŠ¹ì„± ìš”ì•½

|ì§€í‘œ|í’ˆì§ˆ|ë‹¤ì–‘ì„±|ì¡°ê±´ë¶€|ê³„ì‚°ë¹„ìš©|ì ìš©ë²”ìœ„|
|---|---|---|---|---|---|
|**Inception Score**|âœ…|âœ…|âŒ|ì¤‘ê°„|ImageNetë§Œ|
|**FID**|âœ…|âœ…|âŒ|ì¤‘ê°„|ëª¨ë“  ì´ë¯¸ì§€|
|**Precision**|âœ…|âŒ|âŒ|ë†’ìŒ|ëª¨ë“  ì´ë¯¸ì§€|
|**Recall**|âŒ|âœ…|âŒ|ë†’ìŒ|ëª¨ë“  ì´ë¯¸ì§€|
|**Intra-FID**|âœ…|âœ…|âœ…|ì¤‘ê°„|í´ë˜ìŠ¤ ì¡°ê±´|
|**Classification Acc**|âŒ|âŒ|âœ…|ë‚®ìŒ|í´ë˜ìŠ¤ ì¡°ê±´|
|**LPIPS**|âŒ|âœ…|âŒ|ì¤‘ê°„|ì´ë¯¸ì§€ ìŒ|
|**CLIP Score**|âŒ|âŒ|âœ…|ì¤‘ê°„|í…ìŠ¤íŠ¸ ì¡°ê±´|

### ì‹¤ì „ í‰ê°€ íŒŒì´í”„ë¼ì¸

```python
class GenerativeModelEvaluator:
    """ìƒì„± ëª¨ë¸ ì¢…í•© í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model_type='unconditional'):
        self.model_type = model_type
        self.metrics = {}
    
    def evaluate_unconditional(self, real_images, generated_images):
        """ë¬´ì¡°ê±´ ìƒì„± ëª¨ë¸ í‰ê°€"""
        
        print("=== ë¬´ì¡°ê±´ ìƒì„± ëª¨ë¸ í‰ê°€ ===")
        
        # 1. FID ê³„ì‚°
        fid_score = calculate_fid(real_images, generated_images)
        self.metrics['FID'] = fid_score
        print(f"FID: {fid_score:.2f}")
        
        # 2. Inception Score ê³„ì‚° (ImageNet ë°ì´í„°ì¸ ê²½ìš°)
        try:
            is_mean, is_std = calculate_inception_score(generated_images)
            self.metrics['IS_mean'] = is_mean
            self.metrics['IS_std'] = is_std
            print(f"Inception Score: {is_mean:.2f} Â± {is_std:.2f}")
        except:
            print("Inception Score: ê³„ì‚° ë¶ˆê°€ (ImageNet í´ë˜ìŠ¤ ì•„ë‹˜)")
        
        # 3. Precision/Recall ê³„ì‚°
        real_features = extract_inception_features(real_images)
        gen_features = extract_inception_features(generated_images)
        
        precision, recall = calculate_precision_recall(real_features, gen_features)
        self.metrics['Precision'] = precision
        self.metrics['Recall'] = recall
        
        print(f"Precision: {precision:.3f}")
        print(f"Recall: {recall:.3f}")
        
        # 4. ì¢…í•© í‰ê°€
        self._summarize_unconditional()
    
    def evaluate_conditional(self, real_data, generated_data, conditions):
        """ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ í‰ê°€"""
        
        print("=== ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸ í‰ê°€ ===")
        
        condition_type = self._detect_condition_type(conditions)
        
        if condition_type == 'class':
            self._evaluate_class_conditional(real_data, generated_data, conditions)
        elif condition_type == 'text':
            self._evaluate_text_conditional(generated_data, conditions)
        elif condition_type == 'image':
            self._evaluate_image_conditional(real_data, generated_data)
    
    def _evaluate_class_conditional(self, real_data, generated_data, class_labels):
        """í´ë˜ìŠ¤ ì¡°ê±´ë¶€ í‰ê°€"""
        
        # 1. Intra-FID
        intra_fid = calculate_intra_fid(real_data, generated_data)
        self.metrics['Intra_FID'] = intra_fid
        print(f"Intra-FID: {intra_fid:.2f}")
        
        # 2. Classification Accuracy  
        classifier = load_pretrained_classifier()
        accuracy = calculate_classification_accuracy(
            generated_data, class_labels, classifier
        )
        self.metrics['Classification_Accuracy'] = accuracy
        print(f"Classification Accuracy: {accuracy:.3f}")
    
    def _evaluate_text_conditional(self, generated_images, text_prompts):
        """í…ìŠ¤íŠ¸ ì¡°ê±´ë¶€ í‰ê°€"""
        
        # CLIP Score ê³„ì‚°
        avg_clip_score, _ = calculate_clip_score(generated_images, text_prompts)
        self.metrics['CLIP_Score'] = avg_clip_score
        print(f"CLIP Score: {avg_clip_score:.3f}")
    
    def _evaluate_image_conditional(self, source_images, generated_images):
        """ì´ë¯¸ì§€ ì¡°ê±´ë¶€ í‰ê°€ (Image-to-Image)"""
        
        # LPIPS ê³„ì‚°
        lpips_score = evaluate_diversity_with_lpips(source_images, generated_images)
        self.metrics['LPIPS'] = lpips_score
        print(f"LPIPS: {lpips_score:.4f}")
    
    def _summarize_unconditional(self):
        """ë¬´ì¡°ê±´ ìƒì„± ê²°ê³¼ í•´ì„"""
        
        fid = self.metrics['FID']
        precision = self.metrics['Precision']
        recall = self.metrics['Recall']
        
        print("\n=== í‰ê°€ ê²°ê³¼ í•´ì„ ===")
        
        if fid < 10:
            print("âœ… ë§¤ìš° ìš°ìˆ˜í•œ ì „ì²´ì  ì„±ëŠ¥")
        elif fid < 50:
            print("âœ… ì–‘í˜¸í•œ ì „ì²´ì  ì„±ëŠ¥")
        else:
            print("âš ï¸ ê°œì„  í•„ìš”í•œ ì„±ëŠ¥")
        
        if precision > 0.8 and recall > 0.8:
            print("âœ… í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± ëª¨ë‘ ìš°ìˆ˜")
        elif precision > 0.8:
            print("âœ… ë†’ì€ í’ˆì§ˆ, âš ï¸ ë‹¤ì–‘ì„± ë¶€ì¡±")
        elif recall > 0.8:
            print("âœ… ë†’ì€ ë‹¤ì–‘ì„±, âš ï¸ í’ˆì§ˆ ë¶€ì¡±")
        else:
            print("âš ï¸ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± ëª¨ë‘ ê°œì„  í•„ìš”")

# ì‚¬ìš© ì˜ˆì‹œ
def comprehensive_evaluation_example():
    """ì¢…í•© í‰ê°€ ì˜ˆì‹œ"""
    
    evaluator = GenerativeModelEvaluator()
    
    # ë°ì´í„° ë¡œë“œ (ì˜ì‚¬ ì½”ë“œ)
    real_images = load_real_images()
    generated_images = load_generated_images()
    
    # ë¬´ì¡°ê±´ ìƒì„± í‰ê°€
    evaluator.evaluate_unconditional(real_images, generated_images)
    
    # ì¡°ê±´ë¶€ ìƒì„± í‰ê°€ (í…ìŠ¤íŠ¸ ì¡°ê±´)
    text_prompts = ["a dog playing in the park", "a red car", ...]
    conditional_images = load_conditional_images()
    
    evaluator.evaluate_conditional(None, conditional_images, text_prompts)
    
    return evaluator.metrics

# comprehensive_evaluation_example()
```

## ğŸ¯ ì‹¤ë¬´ì—ì„œì˜ í‰ê°€ ì „ëµ

### ëª¨ë¸ ê°œë°œ ë‹¨ê³„ë³„ í‰ê°€

```python
def development_stage_evaluation():
    """ê°œë°œ ë‹¨ê³„ë³„ í‰ê°€ ì „ëµ"""
    
    stages = {
        "ì´ˆê¸° ê°œë°œ": {
            "ì£¼ìš” ì§€í‘œ": ["FID", "IS"],
            "ëª©ì ": "ê¸°ë³¸ ì„±ëŠ¥ í™•ì¸",
            "ë¹ˆë„": "ë§¤ ì—í¬í¬"
        },
        
        "ì¤‘ê°„ ê°œë°œ": {
            "ì£¼ìš” ì§€í‘œ": ["FID", "Precision", "Recall"],
            "ëª©ì ": "í’ˆì§ˆ-ë‹¤ì–‘ì„± íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„",
            "ë¹ˆë„": "ì£¼ìš” ì²´í¬í¬ì¸íŠ¸"
        },
        
        "ìµœì¢… í‰ê°€": {
            "ì£¼ìš” ì§€í‘œ": ["ì „ì²´ ì§€í‘œ", "Human Evaluation"],
            "ëª©ì ": "ì¢…í•© ì„±ëŠ¥ ê²€ì¦",
            "ë¹ˆë„": "ìµœì¢… ëª¨ë¸"
        }
    }
    
    for stage, info in stages.items():
        print(f"\n=== {stage} ===")
        for key, value in info.items():
            print(f"{key}: {value}")

development_stage_evaluation()
```

### í•œê³„ì  ì¸ì‹ê³¼ ëŒ€ì‘ ë°©ì•ˆ

> **ì¤‘ìš”**: ì–´ë–¤ ë‹¨ì¼ ì§€í‘œë„ ìƒì„± ëª¨ë¸ì„ ì™„ë²½í•˜ê²Œ í‰ê°€í•  ìˆ˜ ì—†ë‹¤. ê° ì§€í‘œëŠ” íŠ¹ì • ì¸¡ë©´ë§Œì„ ë°˜ì˜í•˜ë¯€ë¡œ **ì—¬ëŸ¬ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ í™œìš©**í•´ì•¼ í•œë‹¤. {: .prompt-warning}

**ê³µí†µ í•œê³„ì ë“¤:**

- **ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì˜ì¡´**: Inception, CLIP ë“±ì˜ í¸í–¥ì´ í‰ê°€ì— ì˜í–¥
- **ë°ì´í„°ì…‹ íŠ¹ì„± ë¯¼ê°**: ê°™ì€ ëª¨ë¸ë„ ë°ì´í„°ì…‹ì— ë”°ë¼ ì ìˆ˜ ë³€ë™
- **ê³„ì‚° ë¹„ìš©**: ëŒ€ê·œëª¨ í‰ê°€ ì‹œ ìƒë‹¹í•œ ì‹œê°„ê³¼ ìì› í•„ìš”
- **ì¸ê°„ ì¸ì‹ê³¼ì˜ ê´´ë¦¬**: ìˆ˜ì¹˜ì  ì§€í‘œê°€ í•­ìƒ ì¸ê°„ì˜ ì„ í˜¸ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ

**ëŒ€ì‘ ë°©ì•ˆ:**

1. **ë‹¤ì¤‘ ì§€í‘œ í™œìš©**: ìµœì†Œ 3ê°œ ì´ìƒ ì§€í‘œë¡œ ì¢…í•© í‰ê°€
2. **ë„ë©”ì¸ë³„ íŠ¹í™”**: ì–¼êµ´ì€ LPIPS, ìì—° ì´ë¯¸ì§€ëŠ” FID ë“±
3. **Human Evaluation ë³‘í–‰**: ì¤‘ìš”í•œ ê²°ì • ì‹œ ì‚¬ëŒ í‰ê°€ ì¶”ê°€
4. **ì§€ì†ì  ëª¨ë‹ˆí„°ë§**: ê°œë°œ ê³¼ì • ì „ë°˜ì— ê±¸ì¹œ ì§€í‘œ ì¶”ì 

ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì˜ í‰ê°€ëŠ” ì—¬ì „íˆ **í™œë°œí•œ ì—°êµ¬ ë¶„ì•¼**ì´ë©°, ìƒˆë¡œìš´ ì§€í‘œë“¤ì´ ê³„ì† ì œì•ˆë˜ê³  ìˆë‹¤. **ê° ì§€í‘œì˜ íŠ¹ì„±ê³¼ í•œê³„ë¥¼ ì •í™•íˆ ì´í•´**í•˜ê³  **ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ì¡°í•©**ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤.