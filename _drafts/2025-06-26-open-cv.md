---
title: OpenCV
date: 2025-06-26 15:38:00 +0900
categories: [ ]
tags: [ "ê¸‰ë°œì§„ê±°ë¶ì´" ]
toc: true
comments: false
mermaid: true
math: true
---
# Python OpenCV ì™„ë²½ ê°€ì´ë“œ: ì»´í“¨í„° ë¹„ì „ í•µì‹¬ í•¨ìˆ˜ì™€ ì‹¤ë¬´ í™œìš©ë²•

## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

- opencv-python==4.10.0
- numpy==1.26.4
- matplotlib==3.10.1
- Pillow==10.4.0

## ğŸš€ TL;DR

- **OpenCV**ëŠ” ì»´í“¨í„° ë¹„ì „ê³¼ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°€ì¥ ê°•ë ¥í•˜ê³  ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤
- **ì´ë¯¸ì§€ ì½ê¸°/ì €ì¥/í‘œì‹œ**ë¶€í„° **ìƒ‰ìƒ ë³€í™˜**, **ê¸°í•˜í•™ì  ë³€í™˜**, **í•„í„°ë§**, **ì—£ì§€ ê²€ì¶œ** ë“± í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•œë‹¤
- **cv2.imread()**, **cv2.resize()**, **cv2.cvtColor()**, **cv2.GaussianBlur()** ë“±ì´ ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ë“¤ì´ë‹¤
- **ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì²˜ë¦¬**, **ê°ì²´ ì¸ì‹**, **ì–¼êµ´ ê²€ì¶œ**, **OCR** ë“± ë‹¤ì–‘í•œ ì‹¤ë¬´ í”„ë¡œì íŠ¸ì— í™œìš©ëœë‹¤
- **NumPy ë°°ì—´** ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ì—¬ ë¹ ë¥¸ ì„±ëŠ¥ê³¼ ë‹¤ë¥¸ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ì˜ í˜¸í™˜ì„±ì„ ì œê³µí•œë‹¤

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [OpenCV ê¸°ì´ˆì™€ ì‹¤ë¬´ í™œìš©ë²•](https://github.com/yuiyeong/notebooks/blob/main/computer_vision/opencv_fundamentals.ipynb)

## ğŸ¯ OpenCVë€?

**OpenCV**(Open Source Computer Vision Library)ëŠ” ì»´í“¨í„° ë¹„ì „, ì´ë¯¸ì§€ ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤. 1999ë…„ ì¸í…”ì—ì„œ ê°œë°œì´ ì‹œì‘ë˜ì–´ í˜„ì¬ëŠ” ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì»´í“¨í„° ë¹„ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ìë¦¬ì¡ì•˜ë‹¤.

OpenCVëŠ” **ì‹¤ì‹œê°„ ì´ë¯¸ì§€ ì²˜ë¦¬**ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, C++ë¡œ ì‘ì„±ëœ ê³ ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ Pythonì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. ë§ˆì¹˜ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ ë‹¤ë£¨ëŠ” **ìŠ¤ìœ„ìŠ¤ ì•„ë¯¸ ë‚˜ì´í”„**ê°™ì€ ì¡´ì¬ë¼ê³  í•  ìˆ˜ ìˆë‹¤.

### OpenCVì˜ í•µì‹¬ íŠ¹ì§•

- **ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›**: C++, Python, Java ë“± ì—¬ëŸ¬ ì–¸ì–´ì—ì„œ ì‚¬ìš© ê°€ëŠ¥
- **í¬ë¡œìŠ¤ í”Œë«í¼**: Windows, Linux, macOS, Android, iOS ë“±ì—ì„œ ë™ì‘
- **ê´‘ë²”ìœ„í•œ ê¸°ëŠ¥**: ê¸°ë³¸ ì´ë¯¸ì§€ ì²˜ë¦¬ë¶€í„° ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ê¹Œì§€
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ì›¹ìº , ë™ì˜ìƒ ë“± ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì§€ì›
- **NumPy í†µí•©**: NumPy ë°°ì—´ê³¼ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë˜ì–´ ë‹¤ë¥¸ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì‰½ê²Œ ì—°ë™

```mermaid
mindmap
  root((OpenCV))
    (ì´ë¯¸ì§€ ì²˜ë¦¬)
      [ê¸°ë³¸ ì¡°ì‘]
        (ì½ê¸°/ì €ì¥)
        (í¬ê¸° ì¡°ì •)
        (ìë¥´ê¸°/íšŒì „)
      [í•„í„°ë§]
        (ë¸”ëŸ¬ ì²˜ë¦¬)
        (ë…¸ì´ì¦ˆ ì œê±°)
        (ìƒ¤í”„ë‹)
      [ìƒ‰ìƒ ë³€í™˜]
        (RGB/BGR)
        (HSV ë³€í™˜)
        (ê·¸ë ˆì´ìŠ¤ì¼€ì¼)
    (íŠ¹ì§• ê²€ì¶œ)
      [ì—£ì§€ ê²€ì¶œ]
        (Canny)
        (Sobel)
        (Laplacian)
      [ì½”ë„ˆ ê²€ì¶œ]
        (Harris)
        (FAST)
      [íŠ¹ì§•ì  ë§¤ì¹­]
        (SIFT/ORB)
        (í…œí”Œë¦¿ ë§¤ì¹­)
    (ê°ì²´ ì¸ì‹)
      [ë¶„ë¥˜ ë° ê²€ì¶œ]
        (ì–¼êµ´ ê²€ì¶œ)
        (í…ìŠ¤íŠ¸ ì¸ì‹)
        (ê°ì²´ ì¶”ì )
      [ë¨¸ì‹ ëŸ¬ë‹]
        (SVM)
        (ë”¥ëŸ¬ë‹ ëª¨ë¸)
```

> OpenCVëŠ” í•™ìˆ  ì—°êµ¬ë¶€í„° ìƒì—…ì  ì‘ìš©ê¹Œì§€ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ì»´í“¨í„° ë¹„ì „ì˜ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ììœ¨ì£¼í–‰ì°¨, ì˜ë£Œ ì˜ìƒ ë¶„ì„, ë³´ì•ˆ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•µì‹¬ ê¸°ìˆ ë¡œ í™œìš©ëœë‹¤. {: .prompt-tip}

## ğŸ“ ì´ë¯¸ì§€ ì½ê¸°, ì €ì¥, í‘œì‹œí•˜ê¸°

OpenCVì—ì„œ ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” ì‘ì—…ì€ ì´ë¯¸ì§€ë¥¼ ì½ê³ , ì²˜ë¦¬í•˜ê³ , ì €ì¥í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ëª¨ë“  ì»´í“¨í„° ë¹„ì „ í”„ë¡œì íŠ¸ì˜ ì¶œë°œì ì´ ëœë‹¤.

### cv2.imread() - ì´ë¯¸ì§€ ì½ê¸°

**cv2.imread()**ëŠ” íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì½ì–´ë“¤ì´ëŠ” í•¨ìˆ˜ë‹¤.

```python
cv2.imread(filename, flags)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **filename**: ì½ì„ ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ë¡œ (ë¬¸ìì—´)
- **flags**: ì´ë¯¸ì§€ë¥¼ ì½ëŠ” ë°©ë²•ì„ ì§€ì •í•˜ëŠ” í”Œë˜ê·¸
    - **cv2.IMREAD_COLOR** (ê¸°ë³¸ê°’, 1): ì»¬ëŸ¬ ì´ë¯¸ì§€ë¡œ ì½ê¸° (BGR í˜•ì‹)
    - **cv2.IMREAD_GRAYSCALE** (0): ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ì½ê¸°
    - **cv2.IMREAD_UNCHANGED** (-1): ì›ë³¸ ì´ë¯¸ì§€ ê·¸ëŒ€ë¡œ ì½ê¸° (ì•ŒíŒŒ ì±„ë„ í¬í•¨)

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ì»¬ëŸ¬ ì´ë¯¸ì§€ ì½ê¸° (ê¸°ë³¸ê°’)
img_color = cv2.imread('example.jpg')  
print(f"ì»¬ëŸ¬ ì´ë¯¸ì§€ shape: {img_color.shape}")  # ì¶œë ¥: (ë†’ì´, ë„ˆë¹„, 3)

# ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ì½ê¸°
img_gray = cv2.imread('example.jpg', cv2.IMREAD_GRAYSCALE)
print(f"ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ shape: {img_gray.shape}")  # ì¶œë ¥: (ë†’ì´, ë„ˆë¹„)

# ì´ë¯¸ì§€ ì½ê¸° ì‹¤íŒ¨ ì²˜ë¦¬
img = cv2.imread('nonexistent.jpg')
if img is None:
    print("ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
```

> OpenCVëŠ” ì´ë¯¸ì§€ë¥¼ **BGR(Blue-Green-Red)** ìˆœì„œë¡œ ì½ëŠ”ë‹¤. ì´ëŠ” ì¼ë°˜ì ì¸ **RGB** ìˆœì„œì™€ ë‹¤ë¥´ë¯€ë¡œ matplotlib ë“± ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í•¨ê»˜ ì‚¬ìš©í•  ë•Œ ì£¼ì˜í•´ì•¼ í•œë‹¤. {: .prompt-warning}

### cv2.imshow() - ì´ë¯¸ì§€ í‘œì‹œí•˜ê¸°

**cv2.imshow()**ëŠ” ì´ë¯¸ì§€ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ëŠ” í•¨ìˆ˜ë‹¤.

```python
cv2.imshow(winname, mat)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **winname**: ì°½ì˜ ì´ë¦„ (ë¬¸ìì—´)
- **mat**: í‘œì‹œí•  ì´ë¯¸ì§€ (NumPy ë°°ì—´)

```python
import cv2

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')

# ì´ë¯¸ì§€ í‘œì‹œ
cv2.imshow('Original Image', img)

# í‚¤ ì…ë ¥ ëŒ€ê¸° (ë°€ë¦¬ì´ˆ, 0ì€ ë¬´í•œ ëŒ€ê¸°)
cv2.waitKey(0)

# ëª¨ë“  ì°½ ë‹«ê¸°
cv2.destroyAllWindows()

# íŠ¹ì • ì°½ë§Œ ë‹«ê¸°
cv2.destroyWindow('Original Image')
```

### cv2.imwrite() - ì´ë¯¸ì§€ ì €ì¥í•˜ê¸°

**cv2.imwrite()**ëŠ” ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ë‹¤.

```python
cv2.imwrite(filename, img, params)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **filename**: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (í™•ì¥ìë¡œ í¬ë§· ê²°ì •)
- **img**: ì €ì¥í•  ì´ë¯¸ì§€ (NumPy ë°°ì—´)
- **params**: ì €ì¥ ì˜µì…˜ (ì„ íƒì‚¬í•­)

```python
import cv2

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('input.jpg')

# ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥
cv2.imwrite('output.jpg', img)  # JPEG í˜•ì‹
cv2.imwrite('output.png', img)  # PNG í˜•ì‹

# JPEG í’ˆì§ˆ ì„¤ì • (0-100, ë†’ì„ìˆ˜ë¡ ê³ í’ˆì§ˆ)
cv2.imwrite('high_quality.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 95])

# PNG ì••ì¶• ë ˆë²¨ ì„¤ì • (0-9, ë†’ì„ìˆ˜ë¡ ì‘ì€ íŒŒì¼ í¬ê¸°)
cv2.imwrite('compressed.png', img, [cv2.IMWRITE_PNG_COMPRESSION, 9])

print("ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ!")
```

### Matplotlibê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ê¸°

OpenCVì™€ matplotlibì„ í•¨ê»˜ ì‚¬ìš©í•  ë•ŒëŠ” **BGRì—ì„œ RGBë¡œ ìƒ‰ìƒ ìˆœì„œë¥¼ ë³€ê²½**í•´ì•¼ í•œë‹¤.

```python
import cv2
import matplotlib.pyplot as plt

# OpenCVë¡œ ì´ë¯¸ì§€ ì½ê¸° (BGR)
img_bgr = cv2.imread('example.jpg')

# BGRì„ RGBë¡œ ë³€í™˜
img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

# matplotlibìœ¼ë¡œ í‘œì‹œ
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.imshow(img_bgr)  # BGR ê·¸ëŒ€ë¡œ (ìƒ‰ìƒì´ ì´ìƒí•˜ê²Œ ë³´ì„)
plt.title('BGR (ì˜ëª»ëœ ìƒ‰ìƒ)')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(img_rgb)  # RGBë¡œ ë³€í™˜ëœ ì •ìƒ ìƒ‰ìƒ
plt.title('RGB (ì •ìƒ ìƒ‰ìƒ)')
plt.axis('off')

plt.tight_layout()
plt.show()
```

> ì‹¤ë¬´ì—ì„œëŠ” OpenCVë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê³  matplotlibìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ, BGRâ†”RGB ë³€í™˜ì€ ë§¤ìš° ì¤‘ìš”í•œ ê¸°ìˆ ì´ë‹¤. {: .prompt-tip}

## ğŸ¨ ì´ë¯¸ì§€ ê¸°ë³¸ ì†ì„±ê³¼ ì¡°ì‘

ì´ë¯¸ì§€ë¥¼ ë‹¤ë£¨ê¸° ì „ì— ì´ë¯¸ì§€ì˜ ê¸°ë³¸ ì†ì„±ì„ ì´í•´í•˜ê³  í”½ì…€ ë‹¨ìœ„ ì¡°ì‘ ë°©ë²•ì„ ì•Œì•„ì•¼ í•œë‹¤. OpenCVì—ì„œ ì´ë¯¸ì§€ëŠ” **NumPy ë°°ì—´**ë¡œ í‘œí˜„ë˜ë¯€ë¡œ NumPyì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤.

### ì´ë¯¸ì§€ ê¸°ë³¸ ì†ì„± í™•ì¸

```python
import cv2
import numpy as np

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')

# ê¸°ë³¸ ì†ì„± í™•ì¸
print(f"ì´ë¯¸ì§€ í˜•íƒœ(shape): {img.shape}")  # ì¶œë ¥: (ë†’ì´, ë„ˆë¹„, ì±„ë„)
print(f"ì´ë¯¸ì§€ í¬ê¸°(size): {img.size}")   # ì¶œë ¥: ì „ì²´ í”½ì…€ ìˆ˜
print(f"ë°ì´í„° íƒ€ì…(dtype): {img.dtype}") # ì¶œë ¥: uint8 (ë³´í†µ)

# ë†’ì´, ë„ˆë¹„, ì±„ë„ ê°œë³„ ì ‘ê·¼
height, width = img.shape[:2]
if len(img.shape) == 3:
    channels = img.shape[2]
    print(f"ë†’ì´: {height}, ë„ˆë¹„: {width}, ì±„ë„: {channels}")
else:
    print(f"ë†’ì´: {height}, ë„ˆë¹„: {width} (ê·¸ë ˆì´ìŠ¤ì¼€ì¼)")

# ì´ë¯¸ì§€ ì •ë³´ ìš”ì•½ í•¨ìˆ˜
def get_image_info(image, name="Image"):
    print(f"\n=== {name} ì •ë³´ ===")
    print(f"Shape: {image.shape}")
    print(f"Size: {image.size}")
    print(f"Data type: {image.dtype}")
    print(f"Min value: {np.min(image)}")
    print(f"Max value: {np.max(image)}")
    print(f"Mean value: {np.mean(image):.2f}")

get_image_info(img, "ì›ë³¸ ì´ë¯¸ì§€")
```

### í”½ì…€ê°’ ì ‘ê·¼ê³¼ ìˆ˜ì •

OpenCV ì´ë¯¸ì§€ëŠ” NumPy ë°°ì—´ì´ë¯€ë¡œ ì¸ë±ì‹±ì„ í†µí•´ í”½ì…€ê°’ì— ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

```python
import cv2
import numpy as np

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')

# íŠ¹ì • í”½ì…€ê°’ ì½ê¸° (y, x, channel ìˆœì„œ ì£¼ì˜!)
(b, g, r) = img[100, 50]  # (y=100, x=50) ìœ„ì¹˜ì˜ BGR ê°’
print(f"í”½ì…€ (50, 100) BGR ê°’: B={b}, G={g}, R={r}")

# ê°œë³„ ì±„ë„ ì ‘ê·¼
blue_value = img[100, 50, 0]   # Blue ì±„ë„
green_value = img[100, 50, 1]  # Green ì±„ë„  
red_value = img[100, 50, 2]    # Red ì±„ë„

# í”½ì…€ê°’ ìˆ˜ì •
img[100, 50] = [255, 0, 0]  # í•´ë‹¹ í”½ì…€ì„ íŒŒë€ìƒ‰ìœ¼ë¡œ ë³€ê²½

# ì˜ì—­ ë‹¨ìœ„ ìˆ˜ì •
img[100:200, 50:150] = [0, 255, 0]  # ì‚¬ê°í˜• ì˜ì—­ì„ ì´ˆë¡ìƒ‰ìœ¼ë¡œ ë³€ê²½

# ë” íš¨ìœ¨ì ì¸ í”½ì…€ ì ‘ê·¼ ë°©ë²• (item, itemset)
# ì½ê¸°
b_value = img.item(100, 50, 0)  # Blue ê°’ ì½ê¸°
print(f"Blue ê°’: {b_value}")

# ì“°ê¸°  
img.itemset((100, 50, 0), 255)  # Blue ê°’ì„ 255ë¡œ ì„¤ì •
```

### ì´ë¯¸ì§€ ì˜ì—­ ìë¥´ê¸°(ROI - Region of Interest)

**ROI(Region of Interest)**ëŠ” ì´ë¯¸ì§€ì—ì„œ ê´€ì‹¬ ìˆëŠ” íŠ¹ì • ì˜ì—­ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.

```python
import cv2
import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# ROI ì„¤ì • (y1:y2, x1:x2)
roi = img[100:400, 200:500]  # ë†’ì´ 100-400, ë„ˆë¹„ 200-500 ì˜ì—­
roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.imshow(img_rgb)
plt.title('ì›ë³¸ ì´ë¯¸ì§€')
# ROI ì˜ì—­ í‘œì‹œ
plt.plot([200, 500, 500, 200, 200], [100, 100, 400, 400, 100], 'r-', linewidth=2)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(roi_rgb)
plt.title('ì¶”ì¶œëœ ROI')
plt.axis('off')

plt.tight_layout()
plt.show()

print(f"ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {img.shape}")
print(f"ROI í¬ê¸°: {roi.shape}")
```

### ì´ë¯¸ì§€ ë³µì‚¬ì™€ ë¶„í• 

```python
import cv2
import numpy as np

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')

# ì´ë¯¸ì§€ ë³µì‚¬ (ì–•ì€ ë³µì‚¬ vs ê¹Šì€ ë³µì‚¬)
img_view = img  # ì–•ì€ ë³µì‚¬ (ê°™ì€ ë©”ëª¨ë¦¬ ì°¸ì¡°)
img_copy = img.copy()  # ê¹Šì€ ë³µì‚¬ (ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬)

# ì±„ë„ ë¶„í• 
b, g, r = cv2.split(img)  # BGR ì±„ë„ì„ ê°œë³„ë¡œ ë¶„ë¦¬
print(f"Blue ì±„ë„ shape: {b.shape}")
print(f"Green ì±„ë„ shape: {g.shape}")
print(f"Red ì±„ë„ shape: {r.shape}")

# ì±„ë„ ë³‘í•©
img_merged = cv2.merge([b, g, r])  # ë¶„í• ëœ ì±„ë„ì„ ë‹¤ì‹œ ë³‘í•©

# ê°œë³„ ì±„ë„ì„ 3ì±„ë„ ì´ë¯¸ì§€ë¡œ ë§Œë“¤ê¸°
b_3channel = cv2.merge([b, b, b])  # Blue ì±„ë„ë§Œìœ¼ë¡œ 3ì±„ë„ ì´ë¯¸ì§€ ìƒì„±
g_3channel = cv2.merge([g, g, g])  # Green ì±„ë„ë§Œìœ¼ë¡œ 3ì±„ë„ ì´ë¯¸ì§€ ìƒì„±
r_3channel = cv2.merge([r, r, r])  # Red ì±„ë„ë§Œìœ¼ë¡œ 3ì±„ë„ ì´ë¯¸ì§€ ìƒì„±

# íŠ¹ì • ì±„ë„ë§Œ 0ìœ¼ë¡œ ë§Œë“¤ê¸°
img_no_red = img.copy()
img_no_red[:, :, 2] = 0  # Red ì±„ë„ì„ 0ìœ¼ë¡œ ì„¤ì •

print("ì±„ë„ ë¶„í•  ë° ë³‘í•© ì™„ë£Œ!")
```

> ì´ë¯¸ì§€ ì¡°ì‘ ì‹œ **ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë³´ì¡´**í•˜ë ¤ë©´ ë°˜ë“œì‹œ **copy()**ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì›ë³¸ë„ í•¨ê»˜ ë³€ê²½ë  ìˆ˜ ìˆë‹¤. {: .prompt-warning}

## ğŸŒˆ ìƒ‰ìƒ ê³µê°„ ë³€í™˜

ìƒ‰ìƒ ê³µê°„ ë³€í™˜ì€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ë‹¤. ì„œë¡œ ë‹¤ë¥¸ ìƒ‰ìƒ ê³µê°„ì€ ê°ê° ê³ ìœ í•œ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆì–´, íŠ¹ì • ì‘ì—…ì— ë” ì í•©í•œ ìƒ‰ìƒ ê³µê°„ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬ì´ë‹¤.

### cv2.cvtColor() - ìƒ‰ìƒ ê³µê°„ ë³€í™˜ì˜ í•µì‹¬

**cv2.cvtColor()**ëŠ” í•˜ë‚˜ì˜ ìƒ‰ìƒ ê³µê°„ì—ì„œ ë‹¤ë¥¸ ìƒ‰ìƒ ê³µê°„ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë‹¤.

```python
cv2.cvtColor(src, code, dst, dstCn)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **src**: ì…ë ¥ ì´ë¯¸ì§€
- **code**: ë³€í™˜ íƒ€ì…ì„ ì§€ì •í•˜ëŠ” í”Œë˜ê·¸
- **dst**: ì¶œë ¥ ì´ë¯¸ì§€ (ì„ íƒì‚¬í•­)
- **dstCn**: ì¶œë ¥ ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜ (ì„ íƒì‚¬í•­)

### ì£¼ìš” ìƒ‰ìƒ ê³µê°„ê³¼ í™œìš©ë²•

```python
import cv2
import matplotlib.pyplot as plt
import numpy as np

# ì›ë³¸ ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# 1. BGR â†” RGB ë³€í™˜ (ê°€ì¥ ìì£¼ ì‚¬ìš©)
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)

# 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# 3. HSV ë³€í™˜ (ìƒ‰ìƒ ê¸°ë°˜ ê°ì²´ ê²€ì¶œì— ìœ ìš©)
img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

# 4. LAB ë³€í™˜ (ì¡°ëª…ì— ê°•ì¸í•œ ìƒ‰ìƒ ë¶„ì„)
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# 5. YUV ë³€í™˜ (ë¹„ë””ì˜¤ ì••ì¶•ì—ì„œ ì‚¬ìš©)
img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 10))

# ì›ë³¸
plt.subplot(2, 3, 1)
plt.imshow(img_rgb)
plt.title('ì›ë³¸ (RGB)')
plt.axis('off')

# ê·¸ë ˆì´ìŠ¤ì¼€ì¼
plt.subplot(2, 3, 2)
plt.imshow(img_gray, cmap='gray')
plt.title('ê·¸ë ˆì´ìŠ¤ì¼€ì¼')
plt.axis('off')

# HSV
plt.subplot(2, 3, 3)
plt.imshow(img_hsv)
plt.title('HSV')
plt.axis('off')

# LAB
plt.subplot(2, 3, 4)
plt.imshow(img_lab)
plt.title('LAB')
plt.axis('off')

# YUV
plt.subplot(2, 3, 5)
plt.imshow(img_yuv)
plt.title('YUV')
plt.axis('off')

plt.tight_layout()
plt.show()
```

### HSV ìƒ‰ìƒ ê³µê°„ì„ í™œìš©í•œ ìƒ‰ìƒ ê¸°ë°˜ ê°ì²´ ê²€ì¶œ

**HSV(Hue-Saturation-Value)**ëŠ” ìƒ‰ìƒ ê¸°ë°˜ ê°ì²´ ê²€ì¶œì— ë§¤ìš° ìœ ìš©í•œ ìƒ‰ìƒ ê³µê°„ì´ë‹¤.

```python
import cv2
import numpy as np

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

# íŠ¹ì • ìƒ‰ìƒ ë²”ìœ„ ì •ì˜ (íŒŒë€ìƒ‰ ê°ì²´ ê²€ì¶œ ì˜ˆì‹œ)
# HSVì—ì„œ ìƒ‰ìƒ(Hue) ë²”ìœ„: 0-179, ì±„ë„(Saturation): 0-255, ëª…ë„(Value): 0-255
lower_blue = np.array([100, 50, 50])   # íŒŒë€ìƒ‰ í•˜í•œê°’
upper_blue = np.array([130, 255, 255]) # íŒŒë€ìƒ‰ ìƒí•œê°’

# ìƒ‰ìƒ ë²”ìœ„ì— ë”°ë¥¸ ë§ˆìŠ¤í¬ ìƒì„±
mask = cv2.inRange(hsv, lower_blue, upper_blue)

# ë§ˆìŠ¤í¬ë¥¼ ì´ìš©í•œ ìƒ‰ìƒ ì¶”ì¶œ
result = cv2.bitwise_and(img, img, mask=mask)

# ê²°ê³¼ ì¶œë ¥
cv2.imshow('Original', img)
cv2.imshow('Mask', mask)
cv2.imshow('Result', result)
cv2.waitKey(0)
cv2.destroyAllWindows()

print(f"ê²€ì¶œëœ í”½ì…€ ìˆ˜: {np.sum(mask > 0)}")
```

### ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ìƒ‰ìƒ ë³€í™˜ íŒ¨í„´

```python
import cv2
import numpy as np

def color_space_demo(image_path):
    """ë‹¤ì–‘í•œ ìƒ‰ìƒ ê³µê°„ ë³€í™˜ ë°ëª¨"""
    
    # ì´ë¯¸ì§€ ì½ê¸°
    img = cv2.imread(image_path)
    
    # ìƒ‰ìƒ ê³µê°„ë³„ ë³€í™˜
    conversions = {
        'RGB': cv2.cvtColor(img, cv2.COLOR_BGR2RGB),
        'GRAY': cv2.cvtColor(img, cv2.COLOR_BGR2GRAY),
        'HSV': cv2.cvtColor(img, cv2.COLOR_BGR2HSV),
        'LAB': cv2.cvtColor(img, cv2.COLOR_BGR2LAB),
        'HLS': cv2.cvtColor(img, cv2.COLOR_BGR2HLS),
        'YUV': cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
    }
    
    return conversions

# ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° í•¨ìˆ˜
def calculate_color_histogram(img, color_space='BGR'):
    """ìƒ‰ìƒë³„ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°"""
    
    if color_space == 'BGR':
        colors = ['Blue', 'Green', 'Red']
        color_codes = ['b', 'g', 'r']
    elif color_space == 'HSV':
        colors = ['Hue', 'Saturation', 'Value']
        color_codes = ['c', 'm', 'y']
    
    plt.figure(figsize=(12, 4))
    
    for i, (color, code) in enumerate(zip(colors, color_codes)):
        hist = cv2.calcHist([img], [i], None, [256], [0, 256])
        plt.subplot(1, 3, i+1)
        plt.plot(hist, color=code)
        plt.title(f'{color} Histogram')
        plt.xlabel('Pixel Value')
        plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# ì‚¬ìš© ì˜ˆì‹œ
# img = cv2.imread('example.jpg')
# calculate_color_histogram(img, 'BGR')
```

> **HSV ìƒ‰ìƒ ê³µê°„**ì€ ì¡°ëª… ë³€í™”ì— ìƒëŒ€ì ìœ¼ë¡œ ê°•ì¸í•˜ì—¬ **ìƒ‰ìƒ ê¸°ë°˜ ê°ì²´ ì¶”ì **ì´ë‚˜ **ì¸ê³µì§€ëŠ¥ ë°ì´í„° ì „ì²˜ë¦¬**ì— ìì£¼ í™œìš©ëœë‹¤. {: .prompt-tip}

## ğŸ“ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •ê³¼ ê¸°í•˜í•™ì  ë³€í™˜

ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •ê³¼ ê¸°í•˜í•™ì  ë³€í™˜ì€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ê°€ì¥ ê¸°ë³¸ì ì´ë©´ì„œë„ ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ê³¼ì •ì´ë‹¤. íŠ¹íˆ **ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì…ë ¥ í¬ê¸°ë¥¼ ë§ì¶”ê±°ë‚˜**, **ë°ì´í„° ì¦ê°•(Data Augmentation)**ì„ ìœ„í•´ í•„ìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.

### cv2.resize() - ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •

**cv2.resize()**ëŠ” ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜ë¡œ, ë‹¤ì–‘í•œ ë³´ê°„ë²•ì„ ì§€ì›í•œë‹¤.

```python
cv2.resize(src, dsize, fx, fy, interpolation)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **src**: ì…ë ¥ ì´ë¯¸ì§€
- **dsize**: ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸° (width, height) íŠœí”Œ
- **fx, fy**: x, y ë°©í–¥ ìŠ¤ì¼€ì¼ íŒ©í„° (dsizeê°€ Noneì¼ ë•Œ ì‚¬ìš©)
- **interpolation**: ë³´ê°„ ë°©ë²•

### ë‹¤ì–‘í•œ ë³´ê°„ë²•ê³¼ í™œìš©

```python
import cv2
import matplotlib.pyplot as plt
import numpy as np

# ì›ë³¸ ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

print(f"ì›ë³¸ í¬ê¸°: {img.shape}")

# 1. ì ˆëŒ€ í¬ê¸°ë¡œ ì¡°ì •
resized_fixed = cv2.resize(img, (300, 200))  # (width, height)

# 2. ë¹„ìœ¨ë¡œ ì¡°ì •
resized_scale = cv2.resize(img, None, fx=0.5, fy=0.5)  # 50% í¬ê¸°

# 3. ë‹¤ì–‘í•œ ë³´ê°„ë²• ë¹„êµ
interpolations = {
    'NEAREST': cv2.INTER_NEAREST,      # ìµœê·¼ì ‘ ì´ì›ƒ (ë¹ ë¦„, í’ˆì§ˆ ë‚®ìŒ)
    'LINEAR': cv2.INTER_LINEAR,        # ì„ í˜• ë³´ê°„ (ê¸°ë³¸ê°’, ê· í˜•)
    'CUBIC': cv2.INTER_CUBIC,          # 3ì°¨ ë³´ê°„ (ëŠë¦¼, í’ˆì§ˆ ë†’ìŒ)
    'LANCZOS4': cv2.INTER_LANCZOS4     # Lanczos ë³´ê°„ (ë§¤ìš° ê³ í’ˆì§ˆ)
}

# ì‘ì€ ì´ë¯¸ì§€ë¥¼ í¬ê²Œ í™•ëŒ€í•´ì„œ ë³´ê°„ë²• ì°¨ì´ í™•ì¸
small_img = cv2.resize(img, (50, 50))
target_size = (200, 200)

plt.figure(figsize=(15, 8))

plt.subplot(2, 3, 1)
plt.imshow(cv2.cvtColor(small_img, cv2.COLOR_BGR2RGB))
plt.title('ì›ë³¸ ì‘ì€ ì´ë¯¸ì§€ (50x50)')
plt.axis('off')

for i, (name, method) in enumerate(interpolations.items(), 2):
    enlarged = cv2.resize(small_img, target_size, interpolation=method)
    enlarged_rgb = cv2.cvtColor(enlarged, cv2.COLOR_BGR2RGB)
    
    plt.subplot(2, 3, i)
    plt.imshow(enlarged_rgb)
    plt.title(f'{name} (200x200)')
    plt.axis('off')

plt.tight_layout()
plt.show()

# ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ ì‹¤ìš©ì ì¸ í¬ê¸° ì¡°ì • í•¨ìˆ˜
def smart_resize(image, target_width=None, target_height=None, maintain_aspect=True):
    """ì¢…íš¡ë¹„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ í¬ê¸° ì¡°ì •"""
    
    h, w = image.shape[:2]
    
    if maintain_aspect:
        if target_width is not None:
            # ë„ˆë¹„ ê¸°ì¤€ìœ¼ë¡œ ë¹„ìœ¨ ê³„ì‚°
            ratio = target_width / w
            new_h = int(h * ratio)
            return cv2.resize(image, (target_width, new_h))
        elif target_height is not None:
            # ë†’ì´ ê¸°ì¤€ìœ¼ë¡œ ë¹„ìœ¨ ê³„ì‚°
            ratio = target_height / h
            new_w = int(w * ratio)
            return cv2.resize(image, (new_w, target_height))
    else:
        # ë¹„ìœ¨ ë¬´ì‹œí•˜ê³  ê°•ì œ í¬ê¸° ì¡°ì •
        if target_width and target_height:
            return cv2.resize(image, (target_width, target_height))
    
    return image

# ì‚¬ìš© ì˜ˆì‹œ
resized_smart = smart_resize(img, target_width=400)
print(f"ìŠ¤ë§ˆíŠ¸ ë¦¬ì‚¬ì´ì¦ˆ ê²°ê³¼: {resized_smart.shape}")
```

### cv2.warpAffine() - ì•„í•€ ë³€í™˜

**ì•„í•€ ë³€í™˜(Affine Transformation)**ì€ ì„ í˜• ë³€í™˜ê³¼ í‰í–‰ ì´ë™ì„ ê²°í•©í•œ ë³€í™˜ìœ¼ë¡œ, ì§ì„ ì˜ í‰í–‰ì„±ì„ ìœ ì§€í•œë‹¤.

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
h, w = img.shape[:2]

# 1. í‰í–‰ ì´ë™ (Translation)
# ë³€í™˜ í–‰ë ¬: [[1, 0, tx], [0, 1, ty]]
tx, ty = 100, 50  # xì¶•ìœ¼ë¡œ 100, yì¶•ìœ¼ë¡œ 50 ì´ë™
M_translate = np.float32([[1, 0, tx], [0, 1, ty]])
translated = cv2.warpAffine(img, M_translate, (w, h))

# 2. íšŒì „ (Rotation)
center = (w//2, h//2)  # íšŒì „ ì¤‘ì‹¬ì 
angle = 45  # íšŒì „ ê°ë„ (ë„)
scale = 1.0  # ìŠ¤ì¼€ì¼ íŒ©í„°
M_rotate = cv2.getRotationMatrix2D(center, angle, scale)
rotated = cv2.warpAffine(img, M_rotate, (w, h))

# 3. ìŠ¤ì¼€ì¼ë§ê³¼ íšŒì „ ê²°í•©
M_scale_rotate = cv2.getRotationMatrix2D(center, 30, 0.8)  # 30ë„ íšŒì „, 0.8ë°° ì¶•ì†Œ
scaled_rotated = cv2.warpAffine(img, M_scale_rotate, (w, h))

# 4. ì‚¬ìš©ì ì •ì˜ ì•„í•€ ë³€í™˜
# ì„¸ ì ì˜ ë³€í™˜ì„ ì •ì˜í•˜ì—¬ ì•„í•€ ë³€í™˜ í–‰ë ¬ ê³„ì‚°
src_points = np.float32([[0, 0], [w-1, 0], [0, h-1]])
dst_points = np.float32([[0, h*0.13], [w*0.85, h*0.25], [w*0.15, h*0.7]])
M_custom = cv2.getAffineTransform(src_points, dst_points)
custom_transformed = cv2.warpAffine(img, M_custom, (w, h))

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 10))

images = [
    (img_rgb, 'ì›ë³¸'),
    (cv2.cvtColor(translated, cv2.COLOR_BGR2RGB), 'í‰í–‰ì´ë™'),
    (cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB), 'íšŒì „ 45Â°'),
    (cv2.cvtColor(scaled_rotated, cv2.COLOR_BGR2RGB), 'íšŒì „+ì¶•ì†Œ'),
    (cv2.cvtColor(custom_transformed, cv2.COLOR_BGR2RGB), 'ì‚¬ìš©ì ì •ì˜')
]

for i, (image, title) in enumerate(images):
    plt.subplot(2, 3, i+1)
    plt.imshow(image)
    plt.title(title)
    plt.axis('off')

plt.tight_layout()
plt.show()

print("ì•„í•€ ë³€í™˜ í–‰ë ¬:")
print("í‰í–‰ì´ë™:\n", M_translate)
print("íšŒì „:\n", M_rotate)
print("ì‚¬ìš©ì ì •ì˜:\n", M_custom)
```

### cv2.warpPerspective() - ì›ê·¼ ë³€í™˜

**ì›ê·¼ ë³€í™˜(Perspective Transformation)**ì€ 3D ê³µê°„ì—ì„œì˜ ê´€ì  ë³€í™”ë¥¼ 2D ì´ë¯¸ì§€ì—ì„œ ì‹œë®¬ë ˆì´ì…˜í•œë‹¤.

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
h, w = img.shape[:2]

# ì›ê·¼ ë³€í™˜ì„ ìœ„í•œ ë„¤ ì  ì •ì˜
# ì†ŒìŠ¤ í¬ì¸íŠ¸ (ì›ë³¸ ì´ë¯¸ì§€ì˜ ë„¤ ëª¨ì„œë¦¬)
src_points = np.float32([
    [0, 0],        # ì¢Œìƒë‹¨
    [w-1, 0],      # ìš°ìƒë‹¨  
    [0, h-1],      # ì¢Œí•˜ë‹¨
    [w-1, h-1]     # ìš°í•˜ë‹¨
])

# ëŒ€ìƒ í¬ì¸íŠ¸ (ë³€í™˜ í›„ ìœ„ì¹˜)
dst_points = np.float32([
    [w*0.2, h*0.1],   # ì¢Œìƒë‹¨ì„ ì•ˆìª½ìœ¼ë¡œ
    [w*0.8, h*0.2],   # ìš°ìƒë‹¨ì„ ì•ˆìª½ìœ¼ë¡œ
    [w*0.1, h*0.9],   # ì¢Œí•˜ë‹¨ì„ ì•ˆìª½ìœ¼ë¡œ  
    [w*0.9, h*0.8]    # ìš°í•˜ë‹¨ì„ ì•ˆìª½ìœ¼ë¡œ
])

# ì›ê·¼ ë³€í™˜ í–‰ë ¬ ê³„ì‚°
M_perspective = cv2.getPerspectiveTransform(src_points, dst_points)

# ì›ê·¼ ë³€í™˜ ì ìš©
perspective_transformed = cv2.warpPerspective(img, M_perspective, (w, h))

# ë¬¸ì„œ ìŠ¤ìº” íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜
# ì¢…ì´ê°€ ì‚´ì§ ê¸°ìš¸ì–´ì§„ íš¨ê³¼
doc_src = np.float32([[0, 0], [w-1, 0], [0, h-1], [w-1, h-1]])
doc_dst = np.float32([[w*0.1, h*0.2], [w*0.9, h*0.1], [w*0.2, h*0.8], [w*0.8, h*0.9]])
M_document = cv2.getPerspectiveTransform(doc_src, doc_dst)
document_scan = cv2.warpPerspective(img, M_document, (w, h))

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.imshow(img_rgb)
plt.title('ì›ë³¸ ì´ë¯¸ì§€')
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(cv2.cvtColor(perspective_transformed, cv2.COLOR_BGR2RGB))
plt.title('ì›ê·¼ ë³€í™˜')
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(cv2.cvtColor(document_scan, cv2.COLOR_BGR2RGB))
plt.title('ë¬¸ì„œ ìŠ¤ìº” íš¨ê³¼')
plt.axis('off')

plt.tight_layout()
plt.show()

print("ì›ê·¼ ë³€í™˜ í–‰ë ¬:")
print(M_perspective)
```

### ì‹¤ë¬´ì—ì„œ í™œìš©í•˜ëŠ” ì´ë¯¸ì§€ ë³€í™˜ íŒŒì´í”„ë¼ì¸

```python
import cv2
import numpy as np
import random

class ImageAugmenter:
    """ì´ë¯¸ì§€ ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.augmentation_functions = [
            self.random_rotation,
            self.random_scale,
            self.random_translation,
            self.random_perspective
        ]
    
    def random_rotation(self, image, max_angle=30):
        """ëœë¤ íšŒì „"""
        h, w = image.shape[:2]
        center = (w//2, h//2)
        angle = random.uniform(-max_angle, max_angle)
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        return cv2.warpAffine(image, M, (w, h))
    
    def random_scale(self, image, scale_range=(0.8, 1.2)):
        """ëœë¤ ìŠ¤ì¼€ì¼ë§"""
        h, w = image.shape[:2]
        center = (w//2, h//2)
        scale = random.uniform(*scale_range)
        M = cv2.getRotationMatrix2D(center, 0, scale)
        return cv2.warpAffine(image, M, (w, h))
    
    def random_translation(self, image, max_shift=50):
        """ëœë¤ í‰í–‰ì´ë™"""
        h, w = image.shape[:2]
        tx = random.uniform(-max_shift, max_shift)
        ty = random.uniform(-max_shift, max_shift)
        M = np.float32([[1, 0, tx], [0, 1, ty]])
        return cv2.warpAffine(image, M, (w, h))
    
    def random_perspective(self, image, max_shift=0.1):
        """ëœë¤ ì›ê·¼ ë³€í™˜"""
        h, w = image.shape[:2]
        
        # ì›ë³¸ ëª¨ì„œë¦¬ ì ë“¤
        src = np.float32([[0, 0], [w-1, 0], [0, h-1], [w-1, h-1]])
        
        # ëœë¤í•˜ê²Œ ì´ë™ëœ ëª©í‘œ ì ë“¤
        dst = np.float32([
            [random.uniform(0, w*max_shift), random.uniform(0, h*max_shift)],
            [w-1-random.uniform(0, w*max_shift), random.uniform(0, h*max_shift)],
            [random.uniform(0, w*max_shift), h-1-random.uniform(0, h*max_shift)],
            [w-1-random.uniform(0, w*max_shift), h-1-random.uniform(0, h*max_shift)]
        ])
        
        M = cv2.getPerspectiveTransform(src, dst)
        return cv2.warpPerspective(image, M, (w, h))
    
    def apply_random_augmentation(self, image):
        """ëœë¤í•œ ì¦ê°• ê¸°ë²• í•˜ë‚˜ ì ìš©"""
        aug_func = random.choice(self.augmentation_functions)
        return aug_func(image)
    
    def apply_multiple_augmentations(self, image, num_augs=2):
        """ì—¬ëŸ¬ ì¦ê°• ê¸°ë²• ì—°ì† ì ìš©"""
        result = image.copy()
        for _ in range(num_augs):
            result = self.apply_random_augmentation(result)
        return result

# ì‚¬ìš© ì˜ˆì‹œ
# augmenter = ImageAugmenter()
# img = cv2.imread('example.jpg')
# augmented = augmenter.apply_multiple_augmentations(img, num_augs=3)
```

> ê¸°í•˜í•™ì  ë³€í™˜ì€ **ë°ì´í„° ì¦ê°•(Data Augmentation)**ì—ì„œ í•µì‹¬ ê¸°ë²•ìœ¼ë¡œ, ì ì€ ë°ì´í„°ë¡œë„ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. {: .prompt-tip}

## ğŸ” ì´ë¯¸ì§€ í•„í„°ë§ê³¼ ë¸”ëŸ¬ ì²˜ë¦¬

ì´ë¯¸ì§€ í•„í„°ë§ì€ **ë…¸ì´ì¦ˆ ì œê±°**, **ì´ë¯¸ì§€ ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ê¸°**, **íŠ¹ì§• ê°•í™”** ë“± ë‹¤ì–‘í•œ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ ê¸°ìˆ ì´ë‹¤. **ì»¨ë³¼ë£¨ì…˜(Convolution)** ì—°ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ë”¥ëŸ¬ë‹ì˜ CNNì—ì„œë„ ë™ì¼í•œ ì›ë¦¬ê°€ ì‚¬ìš©ëœë‹¤.

### ì»¨ë³¼ë£¨ì…˜ì˜ ê¸°ë³¸ ê°œë…

ì»¨ë³¼ë£¨ì…˜ì€ ì´ë¯¸ì§€ì˜ ê° í”½ì…€ê³¼ ê·¸ ì£¼ë³€ í”½ì…€ë“¤ì— **ì»¤ë„(í•„í„°)**ì„ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ ê°’ì„ ê³„ì‚°í•˜ëŠ” ì—°ì‚°ì´ë‹¤.

[ì‹œê°ì  í‘œí˜„ ë„£ê¸° - ì»¨ë³¼ë£¨ì…˜ ì—°ì‚° ê³¼ì • ì• ë‹ˆë©”ì´ì…˜]

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ì»¤ìŠ¤í…€ ì»¤ë„ë¡œ ì»¨ë³¼ë£¨ì…˜ ì ìš©í•˜ê¸°
def apply_custom_kernel(image, kernel):
    """ì‚¬ìš©ì ì •ì˜ ì»¤ë„ ì ìš©"""
    return cv2.filter2D(image, -1, kernel)

# ë‹¤ì–‘í•œ ì»¤ë„ ì •ì˜
kernels = {
    # ë¸”ëŸ¬ ì»¤ë„ (í‰ê·  í•„í„°)
    'blur': np.ones((5, 5), np.float32) / 25,
    
    # ìƒ¤í”„ë‹ ì»¤ë„
    'sharpen': np.array([
        [0, -1, 0],
        [-1, 5, -1], 
        [0, -1, 0]
    ], np.float32),
    
    # ì—£ì§€ ê²€ì¶œ ì»¤ë„
    'edge': np.array([
        [-1, -1, -1],
        [-1, 8, -1],
        [-1, -1, -1]
    ], np.float32),
    
    # ì— ë³´ì‹± íš¨ê³¼
    'emboss': np.array([
        [-2, -1, 0],
        [-1, 1, 1],
        [0, 1, 2]
    ], np.float32)
}

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# ì»¤ë„ ì ìš© ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
plt.imshow(img_rgb)
plt.title('ì›ë³¸ ì´ë¯¸ì§€')
plt.axis('off')

for i, (name, kernel) in enumerate(kernels.items(), 2):
    filtered = apply_custom_kernel(img, kernel)
    filtered_rgb = cv2.cvtColor(filtered, cv2.COLOR_BGR2RGB)
    
    plt.subplot(2, 3, i)
    plt.imshow(filtered_rgb)
    plt.title(f'{name.title()} í•„í„°')
    plt.axis('off')

plt.tight_layout()
plt.show()

# ì»¤ë„ ì‹œê°í™”
plt.figure(figsize=(12, 3))
for i, (name, kernel) in enumerate(kernels.items(), 1):
    plt.subplot(1, 4, i)
    plt.imshow(kernel, cmap='gray')
    plt.title(f'{name.title()} ì»¤ë„')
    plt.colorbar()

plt.tight_layout()
plt.show()
```

### ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ - cv2.GaussianBlur()

**ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬**ëŠ” ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë¸”ëŸ¬ ê¸°ë²•ìœ¼ë¡œ, ìì—°ìŠ¤ëŸ¬ìš´ ë¸”ëŸ¬ íš¨ê³¼ë¥¼ ì œê³µí•œë‹¤.

```python
cv2.GaussianBlur(src, ksize, sigmaX, sigmaY, borderType)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **src**: ì…ë ¥ ì´ë¯¸ì§€
- **ksize**: ê°€ìš°ì‹œì•ˆ ì»¤ë„ í¬ê¸° (í™€ìˆ˜, (width, height))
- **sigmaX**: Xë°©í–¥ í‘œì¤€í¸ì°¨
- **sigmaY**: Yë°©í–¥ í‘œì¤€í¸ì°¨ (0ì´ë©´ sigmaXì™€ ë™ì¼)
- **borderType**: ê²½ê³„ ì²˜ë¦¬ ë°©ë²•

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# ë‹¤ì–‘í•œ ê°•ë„ì˜ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
blur_strengths = [
    (5, 1),    # ì•½í•œ ë¸”ëŸ¬
    (15, 5),   # ì¤‘ê°„ ë¸”ëŸ¬  
    (31, 10),  # ê°•í•œ ë¸”ëŸ¬
    (51, 20)   # ë§¤ìš° ê°•í•œ ë¸”ëŸ¬
]

plt.figure(figsize=(15, 8))

plt.subplot(2, 3, 1)
plt.imshow(img_rgb)
plt.title('ì›ë³¸ ì´ë¯¸ì§€')
plt.axis('off')

for i, (ksize, sigma) in enumerate(blur_strengths, 2):
    blurred = cv2.GaussianBlur(img, (ksize, ksize), sigma)
    blurred_rgb = cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB)
    
    plt.subplot(2, 3, i)
    plt.imshow(blurred_rgb)
    plt.title(f'ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬\n(ksize={ksize}, Ïƒ={sigma})')
    plt.axis('off')

plt.tight_layout()
plt.show()

# ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¸”ëŸ¬ ê°•ë„ ë¹„êµ
def compare_blur_methods(image):
    """ë‹¤ì–‘í•œ ë¸”ëŸ¬ ë°©ë²• ë¹„êµ"""
    
    # í‰ê·  ë¸”ëŸ¬ (ë°•ìŠ¤ í•„í„°)
    avg_blur = cv2.blur(image, (15, 15))
    
    # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
    gaussian_blur = cv2.GaussianBlur(image, (15, 15), 0)
    
    # ë¯¸ë””ì–¸ ë¸”ëŸ¬ (ë…¸ì´ì¦ˆ ì œê±°ì— íš¨ê³¼ì )
    median_blur = cv2.medianBlur(image, 15)
    
    # ì–‘ë°©í–¥ í•„í„° (ì—£ì§€ ë³´ì¡´í•˜ë©´ì„œ ë¸”ëŸ¬)
    bilateral = cv2.bilateralFilter(image, 15, 80, 80)
    
    methods = {
        'ì›ë³¸': image,
        'í‰ê·  ë¸”ëŸ¬': avg_blur,
        'ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬': gaussian_blur, 
        'ë¯¸ë””ì–¸ ë¸”ëŸ¬': median_blur,
        'ì–‘ë°©í–¥ í•„í„°': bilateral
    }
    
    plt.figure(figsize=(15, 6))
    for i, (name, img) in enumerate(methods.items(), 1):
        plt.subplot(1, 5, i)
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        plt.title(name)
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()
    
    return methods

# ë¸”ëŸ¬ ë°©ë²• ë¹„êµ ì‹¤í–‰
blur_results = compare_blur_methods(img)
```

### ì–‘ë°©í–¥ í•„í„° - cv2.bilateralFilter()

**ì–‘ë°©í–¥ í•„í„°(Bilateral Filter)**ëŠ” ì—£ì§€ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ëŠ” ê³ ê¸‰ í•„í„°ë§ ê¸°ë²•ì´ë‹¤.

```python
cv2.bilateralFilter(src, d, sigmaColor, sigmaSpace, borderType)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **src**: ì…ë ¥ ì´ë¯¸ì§€
- **d**: í•„í„°ë§ì— ì‚¬ìš©í•  í”½ì…€ ì´ì›ƒì˜ ì§€ë¦„
- **sigmaColor**: ìƒ‰ìƒ ê³µê°„ì—ì„œì˜ í‘œì¤€í¸ì°¨ (í´ìˆ˜ë¡ ë” ë§ì€ ìƒ‰ìƒì´ ì„ì„)
- **sigmaSpace**: ì¢Œí‘œ ê³µê°„ì—ì„œì˜ í‘œì¤€í¸ì°¨ (í´ìˆ˜ë¡ ë” ë„“ì€ ì˜ì—­ ê³ ë ¤)

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì´ë¯¸ì§€ ìƒì„±
img = cv2.imread('example.jpg')
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
noise = np.random.normal(0, 25, img.shape).astype(np.uint8)
noisy_img = cv2.add(img, noise)
noisy_rgb = cv2.cvtColor(noisy_img, cv2.COLOR_BGR2RGB)

# ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ì œê±° ë°©ë²• ë¹„êµ
methods = {
    'ë…¸ì´ì¦ˆ ì›ë³¸': noisy_img,
    'ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬': cv2.GaussianBlur(noisy_img, (15, 15), 0),
    'ì–‘ë°©í–¥ í•„í„° (ì•½í•¨)': cv2.bilateralFilter(noisy_img, 9, 75, 75),
    'ì–‘ë°©í–¥ í•„í„° (ê°•í•¨)': cv2.bilateralFilter(noisy_img, 15, 100, 100),
    'ë¯¸ë””ì–¸ í•„í„°': cv2.medianBlur(noisy_img, 5)
}

plt.figure(figsize=(15, 6))
for i, (name, processed) in enumerate(methods.items(), 1):
    plt.subplot(1, 5, i)
    plt.imshow(cv2.cvtColor(processed, cv2.COLOR_BGR2RGB))
    plt.title(name)
    plt.axis('off')

plt.tight_layout()
plt.show()

# ì–‘ë°©í–¥ í•„í„° ë§¤ê°œë³€ìˆ˜ íš¨ê³¼ ë¶„ì„
def bilateral_parameter_study(image):
    """ì–‘ë°©í–¥ í•„í„° ë§¤ê°œë³€ìˆ˜ë³„ íš¨ê³¼ ì—°êµ¬"""
    
    # sigmaColor ë³€í™” (sigmaSpace=75 ê³ ì •)
    sigma_colors = [25, 50, 75, 150]
    
    plt.figure(figsize=(16, 8))
    
    # sigmaColor íš¨ê³¼
    plt.suptitle('ì–‘ë°©í–¥ í•„í„° ë§¤ê°œë³€ìˆ˜ íš¨ê³¼', fontsize=16)
    
    for i, sigma_color in enumerate(sigma_colors, 1):
        result = cv2.bilateralFilter(image, 15, sigma_color, 75)
        plt.subplot(2, 4, i)
        plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))
        plt.title(f'sigmaColor={sigma_color}')
        plt.axis('off')
    
    # sigmaSpace íš¨ê³¼ (sigmaColor=75 ê³ ì •)
    sigma_spaces = [25, 50, 75, 150]
    
    for i, sigma_space in enumerate(sigma_spaces, 5):
        result = cv2.bilateralFilter(image, 15, 75, sigma_space)
        plt.subplot(2, 4, i)
        plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))
        plt.title(f'sigmaSpace={sigma_space}')
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()

bilateral_parameter_study(noisy_img)
```

### ì–¸ìƒ¤í”„ ë§ˆìŠ¤í‚¹ - ì´ë¯¸ì§€ ì„ ëª…ë„ í–¥ìƒ

**ì–¸ìƒ¤í”„ ë§ˆìŠ¤í‚¹(Unsharp Masking)**ì€ ì´ë¯¸ì§€ì˜ ì„ ëª…ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì´ë‹¤.

```python
import cv2
import numpy as np

def unsharp_mask(image, kernel_size=(5, 5), sigma=1.0, amount=1.0, threshold=0):
    """ì–¸ìƒ¤í”„ ë§ˆìŠ¤í‚¹ì„ ì´ìš©í•œ ì´ë¯¸ì§€ ì„ ëª…í™”"""
    
    # 1. ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë¸”ëŸ¬ ì²˜ë¦¬
    blurred = cv2.GaussianBlur(image, kernel_size, sigma)
    
    # 2. ì›ë³¸ì—ì„œ ë¸”ëŸ¬ ì´ë¯¸ì§€ë¥¼ ëº€ ì–¸ìƒ¤í”„ ë§ˆìŠ¤í¬ ìƒì„±
    unsharp_mask = cv2.subtract(image, blurred)
    
    # 3. ì–¸ìƒ¤í”„ ë§ˆìŠ¤í¬ë¥¼ ì›ë³¸ì— ì¶”ê°€
    sharpened = cv2.addWeighted(image, 1.0, unsharp_mask, amount, 0)
    
    # 4. ì„ê³„ê°’ ì ìš© (ì„ íƒì‚¬í•­)
    if threshold > 0:
        low_contrast_mask = np.absolute(unsharp_mask) < threshold
        np.copyto(sharpened, image, where=low_contrast_mask)
    
    return sharpened

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')

# ë‹¤ì–‘í•œ ê°•ë„ë¡œ ì„ ëª…í™” ì ìš©
sharpening_levels = [
    (1.0, "ì•½í•œ ì„ ëª…í™”"),
    (2.0, "ì¤‘ê°„ ì„ ëª…í™”"), 
    (3.0, "ê°•í•œ ì„ ëª…í™”"),
    (5.0, "ë§¤ìš° ê°•í•œ ì„ ëª…í™”")
]

plt.figure(figsize=(15, 8))

plt.subplot(2, 3, 1)
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.title('ì›ë³¸ ì´ë¯¸ì§€')
plt.axis('off')

for i, (amount, title) in enumerate(sharpening_levels, 2):
    sharpened = unsharp_mask(img, amount=amount)
    plt.subplot(2, 3, i)
    plt.imshow(cv2.cvtColor(sharpened, cv2.COLOR_BGR2RGB))
    plt.title(title)
    plt.axis('off')

plt.tight_layout()
plt.show()
```

> **ì–‘ë°©í–¥ í•„í„°**ëŠ” **ì–¼êµ´ ë³´ì • ì•±**ì´ë‚˜ **ì˜ë£Œ ì˜ìƒ ì²˜ë¦¬**ì—ì„œ ì„¸ë¶€ ì‚¬í•­ì„ ë³´ì¡´í•˜ë©´ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ëŠ” ë° ìì£¼ í™œìš©ëœë‹¤. {: .prompt-tip}

## âš¡ ì—£ì§€ ê²€ì¶œ

ì—£ì§€ ê²€ì¶œì€ ì´ë¯¸ì§€ì—ì„œ **ë¬¼ì²´ì˜ ê²½ê³„ì„ **ì„ ì°¾ì•„ë‚´ëŠ” í•µì‹¬ ê¸°ìˆ ì´ë‹¤. í”½ì…€ ê°•ë„ì˜ ê¸‰ê²©í•œ ë³€í™”ë¥¼ ê°ì§€í•˜ì—¬ ê°ì²´ì˜ í˜•íƒœì™€ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. **ììœ¨ì£¼í–‰ì°¨ì˜ ì°¨ì„  ì¸ì‹**, **ì˜ë£Œ ì˜ìƒì˜ ì¢…ì–‘ ê²€ì¶œ** ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ëœë‹¤.

### Canny ì—£ì§€ ê²€ì¶œ - cv2.Canny()

**Canny ì—£ì§€ ê²€ì¶œ**ì€ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì—£ì§€ ê²€ì¶œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ ì •í™•ë„ë¥¼ ì œê³µí•œë‹¤.

```python
cv2.Canny(image, threshold1, threshold2, apertureSize, L2gradient)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **image**: ì…ë ¥ ì´ë¯¸ì§€ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼)
- **threshold1**: ì²« ë²ˆì§¸ ì„ê³„ê°’ (ì•½í•œ ì—£ì§€)
- **threshold2**: ë‘ ë²ˆì§¸ ì„ê³„ê°’ (ê°•í•œ ì—£ì§€)
- **apertureSize**: Sobel ì»¤ë„ í¬ê¸° (ê¸°ë³¸ê°’: 3)
- **L2gradient**: ê¸°ìš¸ê¸° í¬ê¸° ê³„ì‚° ë°©ë²• (ê¸°ë³¸ê°’: False)

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# ë‹¤ì–‘í•œ ì„ê³„ê°’ìœ¼ë¡œ Canny ì—£ì§€ ê²€ì¶œ
thresholds = [
    (50, 150),   # ë‚®ì€ ì„ê³„ê°’ (ë” ë§ì€ ì—£ì§€)
    (100, 200),  # ì¤‘ê°„ ì„ê³„ê°’
    (150, 250),  # ë†’ì€ ì„ê³„ê°’ (ì ì€ ì—£ì§€)
]

plt.figure(figsize=(15, 8))

plt.subplot(2, 3, 1)
plt.imshow(gray, cmap='gray')
plt.title('ì›ë³¸ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼)')
plt.axis('off')

for i, (low, high) in enumerate(thresholds, 2):
    edges = cv2.Canny(gray, low, high)
    plt.subplot(2, 3, i)
    plt.imshow(edges, cmap='gray')
    plt.title(f'Canny ì—£ì§€\n(ì„ê³„ê°’: {low}, {high})')
    plt.axis('off')

# ì „ì²˜ë¦¬ê°€ ì—£ì§€ ê²€ì¶œì— ë¯¸ì¹˜ëŠ” ì˜í–¥
plt.subplot(2, 3, 5)
# ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ í›„ ì—£ì§€ ê²€ì¶œ
blurred = cv2.GaussianBlur(gray, (5, 5), 0)
edges_blurred = cv2.Canny(blurred, 100, 200)
plt.imshow(edges_blurred, cmap='gray')
plt.title('ë¸”ëŸ¬ í›„ Canny')
plt.axis('off')

plt.subplot(2, 3, 6)
# ì–‘ë°©í–¥ í•„í„° í›„ ì—£ì§€ ê²€ì¶œ
bilateral = cv2.bilateralFilter(gray, 9, 75, 75)
edges_bilateral = cv2.Canny(bilateral, 100, 200)
plt.imshow(edges_bilateral, cmap='gray')
plt.title('ì–‘ë°©í–¥ í•„í„° í›„ Canny')
plt.axis('off')

plt.tight_layout()
plt.show()

print(f"ê²€ì¶œëœ ì—£ì§€ í”½ì…€ ìˆ˜:")
for i, (low, high) in enumerate(thresholds):
    edges = cv2.Canny(gray, low, high)
    edge_pixels = np.sum(edges > 0)
    print(f"ì„ê³„ê°’ ({low}, {high}): {edge_pixels:,} í”½ì…€")
```

### Sobel ì—£ì§€ ê²€ì¶œ - cv2.Sobel()

**Sobel ì—£ì§€ ê²€ì¶œ**ì€ ë°©í–¥ë³„ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ì—£ì§€ë¥¼ ê²€ì¶œí•œë‹¤.

```python
cv2.Sobel(src, ddepth, dx, dy, ksize, scale, delta, borderType)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **src**: ì…ë ¥ ì´ë¯¸ì§€
- **ddepth**: ì¶œë ¥ ì´ë¯¸ì§€ ê¹Šì´ (ë³´í†µ cv2.CV_64F)
- **dx**: xë°©í–¥ ë¯¸ë¶„ ì°¨ìˆ˜ (0 ë˜ëŠ” 1)
- **dy**: yë°©í–¥ ë¯¸ë¶„ ì°¨ìˆ˜ (0 ë˜ëŠ” 1)
- **ksize**: Sobel ì»¤ë„ í¬ê¸°

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Sobel ì—£ì§€ ê²€ì¶œ (x, y ë°©í–¥)
sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)

# ì ˆëŒ“ê°’ ë³€í™˜ ë° uint8ë¡œ ë³€í™˜
sobel_x = cv2.convertScaleAbs(sobel_x)
sobel_y = cv2.convertScaleAbs(sobel_y)

# ë‘ ë°©í–¥ ê²°í•©
sobel_combined = cv2.addWeighted(sobel_x, 0.5, sobel_y, 0.5, 0)

# í¬ê¸°ì™€ ë°©í–¥ ê³„ì‚°
magnitude = np.sqrt(sobel_x.astype(np.float32)**2 + sobel_y.astype(np.float32)**2)
direction = np.arctan2(sobel_y.astype(np.float32), sobel_x.astype(np.float32))

# ë‹¤ë¥¸ ì—£ì§€ ê²€ì¶œ ë°©ë²•ë“¤ê³¼ ë¹„êµ
laplacian = cv2.Laplacian(gray, cv2.CV_64F, ksize=3)
laplacian = cv2.convertScaleAbs(laplacian)

canny = cv2.Canny(gray, 100, 200)

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 10))

images = [
    (gray, 'ì›ë³¸', 'gray'),
    (sobel_x, 'Sobel X', 'gray'),
    (sobel_y, 'Sobel Y', 'gray'),
    (sobel_combined, 'Sobel ê²°í•©', 'gray'),
    (magnitude, 'ê¸°ìš¸ê¸° í¬ê¸°', 'hot'),
    (direction, 'ê¸°ìš¸ê¸° ë°©í–¥', 'hsv'),
    (laplacian, 'Laplacian', 'gray'),
    (canny, 'Canny', 'gray')
]

for i, (img_data, title, cmap) in enumerate(images, 1):
    plt.subplot(2, 4, i)
    plt.imshow(img_data, cmap=cmap)
    plt.title(title)
    plt.axis('off')

plt.tight_layout()
plt.show()

# ì—£ì§€ ê²€ì¶œ ì„±ëŠ¥ ë¹„êµ í•¨ìˆ˜
def compare_edge_detectors(image, blur_kernel=5):
    """ë‹¤ì–‘í•œ ì—£ì§€ ê²€ì¶œ ë°©ë²• ì„±ëŠ¥ ë¹„êµ"""
    
    # ì „ì²˜ë¦¬: ë…¸ì´ì¦ˆ ê°ì†Œ
    if blur_kernel > 0:
        blurred = cv2.GaussianBlur(image, (blur_kernel, blur_kernel), 0)
    else:
        blurred = image
    
    # ë‹¤ì–‘í•œ ì—£ì§€ ê²€ì¶œ ì ìš©
    methods = {}
    
    # Canny
    methods['Canny'] = cv2.Canny(blurred, 50, 150)
    
    # Sobel
    sobel_x = cv2.Sobel(blurred, cv2.CV_64F, 1, 0, ksize=3)
    sobel_y = cv2.Sobel(blurred, cv2.CV_64F, 0, 1, ksize=3)
    methods['Sobel'] = cv2.convertScaleAbs(cv2.addWeighted(
        cv2.convertScaleAbs(sobel_x), 0.5, 
        cv2.convertScaleAbs(sobel_y), 0.5, 0))
    
    # Laplacian
    laplacian = cv2.Laplacian(blurred, cv2.CV_64F, ksize=3)
    methods['Laplacian'] = cv2.convertScaleAbs(laplacian)
    
    # Scharr (Sobelì˜ ê°œì„ ëœ ë²„ì „)
    scharr_x = cv2.Scharr(blurred, cv2.CV_64F, 1, 0)
    scharr_y = cv2.Scharr(blurred, cv2.CV_64F, 0, 1)
    methods['Scharr'] = cv2.convertScaleAbs(cv2.addWeighted(
        cv2.convertScaleAbs(scharr_x), 0.5,
        cv2.convertScaleAbs(scharr_y), 0.5, 0))
    
    return methods

# ì—£ì§€ ê²€ì¶œ ë¹„êµ ì‹¤í–‰
edge_results = compare_edge_detectors(gray)

plt.figure(figsize=(15, 8))
plt.subplot(2, 3, 1)
plt.imshow(gray, cmap='gray')
plt.title('ì›ë³¸')
plt.axis('off')

for i, (method, result) in enumerate(edge_results.items(), 2):
    plt.subplot(2, 3, i)
    plt.imshow(result, cmap='gray')
    plt.title(f'{method} ì—£ì§€ ê²€ì¶œ')
    plt.axis('off')

plt.tight_layout()
plt.show()
```

### ì ì‘ì  ì—£ì§€ ê²€ì¶œ

ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì¡°ëª… ì¡°ê±´ì´ë‚˜ ì´ë¯¸ì§€ í’ˆì§ˆì´ ë‹¤ì–‘í•˜ë¯€ë¡œ **ì ì‘ì  ì—£ì§€ ê²€ì¶œ**ì´ í•„ìš”í•˜ë‹¤.

```python
import cv2
import numpy as np

def auto_canny(image, sigma=0.33):
    """ìë™ìœ¼ë¡œ Canny ì„ê³„ê°’ì„ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜"""
    
    # ì´ë¯¸ì§€ì˜ ì¤‘ì•™ê°’ ê³„ì‚°
    median = np.median(image)
    
    # ì¤‘ì•™ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì„ê³„ê°’ ìë™ ì„¤ì •
    lower = int(max(0, (1.0 - sigma) * median))
    upper = int(min(255, (1.0 + sigma) * median))
    
    # Canny ì—£ì§€ ê²€ì¶œ ì ìš©
    edges = cv2.Canny(image, lower, upper)
    
    return edges, lower, upper

def adaptive_edge_detection(image):
    """ë‹¤ì–‘í•œ ì ì‘ì  ì—£ì§€ ê²€ì¶œ ë°©ë²•"""
    
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
    
    results = {}
    
    # 1. ìë™ Canny
    auto_edges, low, high = auto_canny(gray)
    results['Auto Canny'] = (auto_edges, f"ì„ê³„ê°’: {low}, {high}")
    
    # 2. ì§€ì—­ì  ì ì‘ ì²˜ë¦¬
    # ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê°ê° ë‹¤ë¥¸ ì„ê³„ê°’ ì ìš©
    h, w = gray.shape
    adaptive_edges = np.zeros_like(gray)
    
    # 4x4 ê·¸ë¦¬ë“œë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for i in range(4):
        for j in range(4):
            y1, y2 = i * h // 4, (i + 1) * h // 4
            x1, x2 = j * w // 4, (j + 1) * w // 4
            
            roi = gray[y1:y2, x1:x2]
            roi_edges, _, _ = auto_canny(roi)
            adaptive_edges[y1:y2, x1:x2] = roi_edges
    
    results['Adaptive Canny'] = (adaptive_edges, "ì§€ì—­ì  ì ì‘")
    
    # 3. íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™” í›„ ì—£ì§€ ê²€ì¶œ
    equalized = cv2.equalizeHist(gray)
    eq_edges, eq_low, eq_high = auto_canny(equalized)
    results['Histogram Equalized'] = (eq_edges, f"íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™” í›„ {eq_low}, {eq_high}")
    
    return results

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ì ì‘ì  ì—£ì§€ ê²€ì¶œ ì‹¤í–‰
img = cv2.imread('example.jpg')
adaptive_results = adaptive_edge_detection(img)

plt.figure(figsize=(15, 8))

# ì›ë³¸ ì´ë¯¸ì§€
plt.subplot(2, 3, 1)
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.title('ì›ë³¸ ì´ë¯¸ì§€')
plt.axis('off')

# ì¼ë°˜ Canny (ë¹„êµìš©)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
normal_canny = cv2.Canny(gray, 100, 200)
plt.subplot(2, 3, 2)
plt.imshow(normal_canny, cmap='gray')
plt.title('ì¼ë°˜ Canny (100, 200)')
plt.axis('off')

# ì ì‘ì  ë°©ë²•ë“¤
for i, (method, (edges, desc)) in enumerate(adaptive_results.items(), 3):
    plt.subplot(2, 3, i)
    plt.imshow(edges, cmap='gray')
    plt.title(f'{method}\n{desc}')
    plt.axis('off')

plt.tight_layout()
plt.show()
```

> **Canny ì—£ì§€ ê²€ì¶œ**ì€ ë…¸ì´ì¦ˆì— ê°•ì¸í•˜ê³  ì—°ê²°ëœ ì—£ì§€ë¥¼ ìƒì„±í•˜ì—¬ **ê°ì²´ ì¸ì‹**ê³¼ **ì´ë¯¸ì§€ ë¶„í• **ì˜ ì „ì²˜ë¦¬ ë‹¨ê³„ë¡œ ë„ë¦¬ í™œìš©ëœë‹¤. {: .prompt-tip}

## ğŸ”² ì»¨íˆ¬ì–´ ê²€ì¶œê³¼ ë¶„ì„

**ì»¨íˆ¬ì–´(Contour)**ëŠ” ë™ì¼í•œ ìƒ‰ìƒì´ë‚˜ ê°•ë„ë¥¼ ê°€ì§„ ì—°ì†ëœ ì ë“¤ì„ ì—°ê²°í•œ ê³¡ì„ ì´ë‹¤. ê°ì²´ì˜ **ê²½ê³„ì„ **ì„ ë‚˜íƒ€ë‚´ë©°, **í˜•íƒœ ë¶„ì„**, **ê°ì²´ ê³„ìˆ˜**, **í¬ê¸° ì¸¡ì •** ë“±ì— í™œìš©ëœë‹¤.

### cv2.findContours() - ì»¨íˆ¬ì–´ ì°¾ê¸°

```python
cv2.findContours(image, mode, method)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **image**: ì…ë ¥ ì´ë¯¸ì§€ (ì´ì§„ ì´ë¯¸ì§€)
- **mode**: ì»¨íˆ¬ì–´ ê²€ìƒ‰ ëª¨ë“œ
    - **cv2.RETR_EXTERNAL**: ì™¸ê³½ ì»¨íˆ¬ì–´ë§Œ
    - **cv2.RETR_LIST**: ëª¨ë“  ì»¨íˆ¬ì–´ë¥¼ ê³„ì¸µ ì—†ì´
    - **cv2.RETR_TREE**: ëª¨ë“  ì»¨íˆ¬ì–´ë¥¼ ê³„ì¸µ êµ¬ì¡°ë¡œ
- **method**: ì»¨íˆ¬ì–´ ê·¼ì‚¬ ë°©ë²•
    - **cv2.CHAIN_APPROX_NONE**: ëª¨ë“  ì  ì €ì¥
    - **cv2.CHAIN_APPROX_SIMPLE**: ì••ì¶•í•˜ì—¬ ì €ì¥

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ ì½ê¸° ë° ì „ì²˜ë¦¬
img = cv2.imread('example.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# ì´ì§„í™” (ì»¨íˆ¬ì–´ ê²€ì¶œì„ ìœ„í•´ í•„ìš”)
_, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

# ì»¨íˆ¬ì–´ ê²€ì¶œ
contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

print(f"ê²€ì¶œëœ ì»¨íˆ¬ì–´ ê°œìˆ˜: {len(contours)}")

# ì»¨íˆ¬ì–´ ê·¸ë¦¬ê¸°
img_contours = img.copy()
cv2.drawContours(img_contours, contours, -1, (0, 255, 0), 2)  # ëª¨ë“  ì»¨íˆ¬ì–´ë¥¼ ì´ˆë¡ìƒ‰ìœ¼ë¡œ

# ê°€ì¥ í° ì»¨íˆ¬ì–´ë“¤ë§Œ ê·¸ë¦¬ê¸°
contour_areas = [cv2.contourArea(contour) for contour in contours]
large_contours = [contours[i] for i in range(len(contours)) if contour_areas[i] > 500]

img_large_contours = img.copy()
cv2.drawContours(img_large_contours, large_contours, -1, (255, 0, 0), 3)

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.title('ì›ë³¸ ì´ë¯¸ì§€')
plt.axis('off')

plt.subplot(2, 3, 2)
plt.imshow(gray, cmap='gray')
plt.title('ê·¸ë ˆì´ìŠ¤ì¼€ì¼')
plt.axis('off')

plt.subplot(2, 3, 3)
plt.imshow(binary, cmap='gray')
plt.title('ì´ì§„í™”')
plt.axis('off')

plt.subplot(2, 3, 4)
plt.imshow(cv2.cvtColor(img_contours, cv2.COLOR_BGR2RGB))
plt.title(f'ëª¨ë“  ì»¨íˆ¬ì–´ ({len(contours)}ê°œ)')
plt.axis('off')

plt.subplot(2, 3, 5)
plt.imshow(cv2.cvtColor(img_large_contours, cv2.COLOR_BGR2RGB))
plt.title(f'í° ì»¨íˆ¬ì–´ë§Œ ({len(large_contours)}ê°œ)')
plt.axis('off')

# ì»¨íˆ¬ì–´ ì •ë³´ ë¶„ì„
plt.subplot(2, 3, 6)
plt.hist(contour_areas, bins=50, alpha=0.7)
plt.title('ì»¨íˆ¬ì–´ ë©´ì  ë¶„í¬')
plt.xlabel('ë©´ì ')
plt.ylabel('ê°œìˆ˜')
plt.yscale('log')

plt.tight_layout()
plt.show()
```

### ì»¨íˆ¬ì–´ ì†ì„± ê³„ì‚°

```python
def analyze_contour(contour):
    """ì»¨íˆ¬ì–´ì˜ ë‹¤ì–‘í•œ ì†ì„± ë¶„ì„"""
    
    # ë©´ì 
    area = cv2.contourArea(contour)
    
    # ë‘˜ë ˆ
    perimeter = cv2.arcLength(contour, True)
    
    # ê²½ê³„ ì‚¬ê°í˜•
    x, y, w, h = cv2.boundingRect(contour)
    
    # ìµœì†Œ ì™¸ì ‘ ì‚¬ê°í˜• (íšŒì „ ê°€ëŠ¥)
    rect = cv2.minAreaRect(contour)
    box = cv2.boxPoints(rect)
    box = np.int0(box)
    
    # ì™¸ì ‘ì›
    (center_x, center_y), radius = cv2.minEnclosingCircle(contour)
    center = (int(center_x), int(center_y))
    radius = int(radius)
    
    # íƒ€ì› í”¼íŒ…
    if len(contour) >= 5:  # íƒ€ì› í”¼íŒ…ì„ ìœ„í•´ ìµœì†Œ 5ê°œ ì  í•„ìš”
        ellipse = cv2.fitEllipse(contour)
    else:
        ellipse = None
    
    # ì»¨íˆ¬ì–´ ê·¼ì‚¬
    epsilon = 0.02 * cv2.arcLength(contour, True)
    approx = cv2.approxPolyDP(contour, epsilon, True)
    
    # ë³¼ë¡ ê»ì§ˆ
    hull = cv2.convexHull(contour)
    
    # ì¢…íš¡ë¹„
    aspect_ratio = float(w) / h
    
    # ì§ì‚¬ê°í˜•ì„± (ë©´ì  ë¹„ìœ¨)
    rect_area = w * h
    extent = float(area) / rect_area
    
    # ì¶©ì‹¤ë„ (ë³¼ë¡ ê»ì§ˆ ëŒ€ë¹„ ë©´ì )
    hull_area = cv2.contourArea(hull)
    solidity = float(area) / hull_area
    
    return {
        'area': area,
        'perimeter': perimeter,
        'bounding_rect': (x, y, w, h),
        'min_area_rect': rect,
        'box': box,
        'enclosing_circle': (center, radius),
        'ellipse': ellipse,
        'approx': approx,
        'hull': hull,
        'aspect_ratio': aspect_ratio,
        'extent': extent,
        'solidity': solidity
    }

# í° ì»¨íˆ¬ì–´ë“¤ ë¶„ì„
img_analysis = img.copy()

for i, contour in enumerate(large_contours[:5]):  # ìƒìœ„ 5ê°œë§Œ ë¶„ì„
    analysis = analyze_contour(contour)
    
    # ê²½ê³„ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
    x, y, w, h = analysis['bounding_rect']
    cv2.rectangle(img_analysis, (x, y), (x + w, y + h), (255, 0, 0), 2)
    
    # ìµœì†Œ ì™¸ì ‘ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
    cv2.drawContours(img_analysis, [analysis['box']], 0, (0, 255, 0), 2)
    
    # ì™¸ì ‘ì› ê·¸ë¦¬ê¸°
    center, radius = analysis['enclosing_circle']
    cv2.circle(img_analysis, center, radius, (0, 0, 255), 2)
    
    # ì •ë³´ í‘œì‹œ
    info_text = f"Area: {analysis['area']:.0f}"
    cv2.putText(img_analysis, info_text, (x, y-10), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    print(f"ì»¨íˆ¬ì–´ {i+1}:")
    print(f"  ë©´ì : {analysis['area']:.2f}")
    print(f"  ë‘˜ë ˆ: {analysis['perimeter']:.2f}")
    print(f"  ì¢…íš¡ë¹„: {analysis['aspect_ratio']:.2f}")
    print(f"  ì§ì‚¬ê°í˜•ì„±: {analysis['extent']:.2f}")
    print(f"  ì¶©ì‹¤ë„: {analysis['solidity']:.2f}")
    print()

plt.figure(figsize=(10, 8))
plt.imshow(cv2.cvtColor(img_analysis, cv2.COLOR_BGR2RGB))
plt.title('ì»¨íˆ¬ì–´ ì†ì„± ë¶„ì„\n(íŒŒë‘: ê²½ê³„ì‚¬ê°í˜•, ì´ˆë¡: ìµœì†Œì™¸ì ‘ì‚¬ê°í˜•, ë¹¨ê°•: ì™¸ì ‘ì›)')
plt.axis('off')
plt.show()
```

### í˜•íƒœ ê¸°ë°˜ ê°ì²´ ë¶„ë¥˜

```python
def classify_shape(contour):
    """ì»¨íˆ¬ì–´ì˜ í˜•íƒœë¥¼ ë¶„ì„í•˜ì—¬ ë„í˜• ë¶„ë¥˜"""
    
    # ì»¨íˆ¬ì–´ ê·¼ì‚¬
    epsilon = 0.02 * cv2.arcLength(contour, True)
    approx = cv2.approxPolyDP(contour, epsilon, True)
    
    # ê¼­ì§“ì  ê°œìˆ˜ë¡œ ê¸°ë³¸ ë¶„ë¥˜
    vertices = len(approx)
    
    # ë©´ì ê³¼ ë‘˜ë ˆ ê³„ì‚°
    area = cv2.contourArea(contour)
    perimeter = cv2.arcLength(contour, True)
    
    # ê²½ê³„ ì‚¬ê°í˜•
    x, y, w, h = cv2.boundingRect(contour)
    aspect_ratio = float(w) / h
    
    # ì›í˜•ë„ ê³„ì‚° (4Ï€*ë©´ì /ë‘˜ë ˆÂ²)
    if perimeter > 0:
        circularity = 4 * np.pi * area / (perimeter * perimeter)
    else:
        circularity = 0
    
    # í˜•íƒœ ë¶„ë¥˜
    if vertices == 3:
        shape = "ì‚¼ê°í˜•"
    elif vertices == 4:
        if 0.95 <= aspect_ratio <= 1.05:
            shape = "ì •ì‚¬ê°í˜•"
        else:
            shape = "ì§ì‚¬ê°í˜•"
    elif vertices == 5:
        shape = "ì˜¤ê°í˜•"
    elif vertices > 5:
        if circularity > 0.85:
            shape = "ì›"
        else:
            shape = f"ë‹¤ê°í˜•({vertices}ê°í˜•)"
    else:
        shape = "ì•Œ ìˆ˜ ì—†ìŒ"
    
    return {
        'shape': shape,
        'vertices': vertices,
        'area': area,
        'perimeter': perimeter,
        'aspect_ratio': aspect_ratio,
        'circularity': circularity
    }

# í˜•íƒœ ë¶„ë¥˜ ì‹¤í–‰
img_shapes = img.copy()

for i, contour in enumerate(large_contours):
    classification = classify_shape(contour)
    
    # ì»¨íˆ¬ì–´ ì¤‘ì‹¬ì  ê³„ì‚°
    M = cv2.moments(contour)
    if M["m00"] != 0:
        cx = int(M["m10"] / M["m00"])
        cy = int(M["m01"] / M["m00"])
        
        # ë¶„ë¥˜ ê²°ê³¼ í‘œì‹œ
        shape_text = classification['shape']
        cv2.putText(img_shapes, shape_text, (cx-30, cy), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(img_shapes, shape_text, (cx-30, cy), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)

# ì»¨íˆ¬ì–´ ê·¸ë¦¬ê¸°
cv2.drawContours(img_shapes, large_contours, -1, (0, 255, 0), 2)

plt.figure(figsize=(12, 8))
plt.imshow(cv2.cvtColor(img_shapes, cv2.COLOR_BGR2RGB))
plt.title('í˜•íƒœ ê¸°ë°˜ ê°ì²´ ë¶„ë¥˜')
plt.axis('off')
plt.show()

# ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½
shape_counts = {}
for contour in large_contours:
    classification = classify_shape(contour)
    shape = classification['shape']
    shape_counts[shape] = shape_counts.get(shape, 0) + 1

print("ê²€ì¶œëœ í˜•íƒœë³„ ê°œìˆ˜:")
for shape, count in shape_counts.items():
    print(f"  {shape}: {count}ê°œ")
```

> ì»¨íˆ¬ì–´ëŠ” **í’ˆì§ˆ ê²€ì‚¬ ì‹œìŠ¤í…œ**ì—ì„œ ì œí’ˆì˜ ê²°í•¨ì„ ì°¾ê±°ë‚˜, **ì˜ë£Œ ì˜ìƒ**ì—ì„œ ì¢…ì–‘ì˜ í¬ê¸°ë¥¼ ì¸¡ì •í•˜ëŠ” ë“± **ì •ë°€í•œ ì¸¡ì •ì´ í•„ìš”í•œ ë¶„ì•¼**ì—ì„œ í•µì‹¬ ê¸°ìˆ ë¡œ í™œìš©ëœë‹¤. {: .prompt-tip}

## ğŸ¯ í…œí”Œë¦¿ ë§¤ì¹­

**í…œí”Œë¦¿ ë§¤ì¹­(Template Matching)**ì€ í° ì´ë¯¸ì§€ì—ì„œ ì‘ì€ í…œí”Œë¦¿ ì´ë¯¸ì§€ì™€ ì¼ì¹˜í•˜ëŠ” ë¶€ë¶„ì„ ì°¾ëŠ” ê¸°ë²•ì´ë‹¤. **ê°ì²´ ê²€ì¶œ**, **íŒ¨í„´ ì¸ì‹**, **ì´ë¯¸ì§€ ê²€ìƒ‰** ë“±ì— í™œìš©ëœë‹¤.

### cv2.matchTemplate() - í…œí”Œë¦¿ ë§¤ì¹­ ìˆ˜í–‰

```python
cv2.matchTemplate(image, templ, method, mask)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **image**: ê²€ìƒ‰í•  ì´ë¯¸ì§€
- **templ**: í…œí”Œë¦¿ ì´ë¯¸ì§€
- **method**: ë§¤ì¹­ ë°©ë²•
    - **cv2.TM_CCOEFF_NORMED**: ì •ê·œí™”ëœ ìƒê´€ê³„ìˆ˜ (ì¶”ì²œ)
    - **cv2.TM_CCORR_NORMED**: ì •ê·œí™”ëœ ìƒê´€
    - **cv2.TM_SQDIFF_NORMED**: ì •ê·œí™”ëœ ì œê³±ì°¨

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ë©”ì¸ ì´ë¯¸ì§€ì™€ í…œí”Œë¦¿ ì½ê¸°
img = cv2.imread('main_image.jpg')
template = cv2.imread('template.jpg')

# ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)

# í…œí”Œë¦¿ í¬ê¸°
h, w = template_gray.shape

# ë‹¤ì–‘í•œ ë§¤ì¹­ ë°©ë²• ë¹„êµ
methods = {
    'TM_CCOEFF_NORMED': cv2.TM_CCOEFF_NORMED,
    'TM_CCORR_NORMED': cv2.TM_CCORR_NORMED,
    'TM_SQDIFF_NORMED': cv2.TM_SQDIFF_NORMED,
    'TM_CCOEFF': cv2.TM_CCOEFF,
    'TM_CCORR': cv2.TM_CCORR,
    'TM_SQDIFF': cv2.TM_SQDIFF
}

plt.figure(figsize=(15, 10))

for i, (method_name, method) in enumerate(methods.items(), 1):
    # í…œí”Œë¦¿ ë§¤ì¹­ ìˆ˜í–‰
    result = cv2.matchTemplate(img_gray, template_gray, method)
    
    # ìµœì  ë§¤ì¹­ ìœ„ì¹˜ ì°¾ê¸°
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
    
    # SQDIFF ê³„ì—´ì€ ìµœì†Ÿê°’ì´ ìµœì  ë§¤ì¹­
    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:
        top_left = min_loc
        match_val = min_val
    else:
        top_left = max_loc
        match_val = max_val
    
    bottom_right = (top_left[0] + w, top_left[1] + h)
    
    # ê²°ê³¼ ì´ë¯¸ì§€ì— ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
    img_result = img.copy()
    cv2.rectangle(img_result, top_left, bottom_right, (0, 255, 0), 3)
    
    plt.subplot(2, 3, i)
    plt.imshow(cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB))
    plt.title(f'{method_name}\në§¤ì¹­ê°’: {match_val:.3f}')
    plt.axis('off')

plt.tight_layout()
plt.show()

# ê°€ì¥ ì¢‹ì€ ë°©ë²•ìœ¼ë¡œ ìƒì„¸ ë¶„ì„
best_method = cv2.TM_CCOEFF_NORMED
result = cv2.matchTemplate(img_gray, template_gray, best_method)

# ì„ê³„ê°’ ì´ìƒì˜ ëª¨ë“  ë§¤ì¹­ ìœ„ì¹˜ ì°¾ê¸°
threshold = 0.8
locations = np.where(result >= threshold)

# ì—¬ëŸ¬ ë§¤ì¹­ ê²°ê³¼ í‘œì‹œ
img_multiple = img.copy()
for pt in zip(*locations[::-1]):
    cv2.rectangle(img_multiple, pt, (pt[0] + w, pt[1] + h), (0, 255, 0), 2)

print(f"ì„ê³„ê°’ {threshold} ì´ìƒì˜ ë§¤ì¹­ ê°œìˆ˜: {len(locations[0])}")
```

### ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í…œí”Œë¦¿ ë§¤ì¹­

ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í…œí”Œë¦¿ê³¼ ë™ì¼í•œ í¬ê¸°ì˜ ê°ì²´ë¥¼ ì°¾ê¸° ì–´ë ¤ìš°ë¯€ë¡œ **ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë§¤ì¹­**ì´ í•„ìš”í•˜ë‹¤.

```python
def multi_scale_template_matching(image, template, scales=None, threshold=0.8):
    """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í…œí”Œë¦¿ ë§¤ì¹­"""
    
    if scales is None:
        scales = np.linspace(0.5, 2.0, 20)  # 50%ë¶€í„° 200%ê¹Œì§€
    
    found = None
    
    # ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY) if len(template.shape) == 3 else template
    
    # í…œí”Œë¦¿ í¬ê¸°
    (tH, tW) = template_gray.shape[:2]
    
    for scale in scales:
        # í˜„ì¬ ìŠ¤ì¼€ì¼ë¡œ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        resized = cv2.resize(image_gray, None, fx=scale, fy=scale)
        r = image_gray.shape[1] / float(resized.shape[1])
        
        # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ê°€ í…œí”Œë¦¿ë³´ë‹¤ ì‘ìœ¼ë©´ ì¤‘ë‹¨
        if resized.shape[0] < tH or resized.shape[1] < tW:
            break
        
        # í…œí”Œë¦¿ ë§¤ì¹­ ìˆ˜í–‰
        result = cv2.matchTemplate(resized, template_gray, cv2.TM_CCOEFF_NORMED)
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
        
        # ìµœê³  ë§¤ì¹­ê°’ ì¶”ì 
        if found is None or max_val > found[0]:
            found = (max_val, max_loc, r, scale)
    
    return found

def visualize_multi_scale_matching(image, template, scales=None):
    """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë§¤ì¹­ ê²°ê³¼ ì‹œê°í™”"""
    
    # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë§¤ì¹­ ìˆ˜í–‰
    found = multi_scale_template_matching(image, template, scales)
    
    if found is None:
        print("ë§¤ì¹­ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê²°ê³¼ ì¶”ì¶œ
    max_val, max_loc, r, best_scale = found
    
    # ì›ë³¸ ì¢Œí‘œë¡œ ë³€í™˜
    (startX, startY) = (int(max_loc[0] * r), int(max_loc[1] * r))
    (endX, endY) = (int((max_loc[0] + template.shape[1]) * r), 
                    int((max_loc[1] + template.shape[0]) * r))
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title('ì›ë³¸ ì´ë¯¸ì§€')
    plt.axis('off')
    
    plt.subplot(1, 3, 2)
    plt.imshow(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))
    plt.title('í…œí”Œë¦¿')
    plt.axis('off')
    
    # ë§¤ì¹­ ê²°ê³¼
    result_img = image.copy()
    cv2.rectangle(result_img, (startX, startY), (endX, endY), (0, 255, 0), 3)
    
    plt.subplot(1, 3, 3)
    plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))
    plt.title(f'ë§¤ì¹­ ê²°ê³¼\nìŠ¤ì¼€ì¼: {best_scale:.2f}, ë§¤ì¹­ê°’: {max_val:.3f}')
    plt.axis('off')
    
    plt.tight_layout()
    plt.show()
    
    return found

# ì‹¤í–‰ ì˜ˆì‹œ
# img = cv2.imread('main_image.jpg')
# template = cv2.imread('template.jpg')
# result = visualize_multi_scale_matching(img, template)
```

### íšŒì „ì— ê°•ì¸í•œ í…œí”Œë¦¿ ë§¤ì¹­

```python
def rotation_resistant_template_matching(image, template, angles=None, scales=None):
    """íšŒì „ê³¼ ìŠ¤ì¼€ì¼ì— ê°•ì¸í•œ í…œí”Œë¦¿ ë§¤ì¹­"""
    
    if angles is None:
        angles = np.arange(0, 360, 15)  # 15ë„ ê°„ê²©ìœ¼ë¡œ íšŒì „
    if scales is None:
        scales = np.linspace(0.7, 1.3, 10)
    
    best_match = None
    
    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY) if len(template.shape) == 3 else template
    
    # í…œí”Œë¦¿ ì¤‘ì‹¬ì 
    (h, w) = template_gray.shape
    center = (w // 2, h // 2)
    
    for angle in angles:
        # í…œí”Œë¦¿ íšŒì „
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated_template = cv2.warpAffine(template_gray, M, (w, h))
        
        for scale in scales:
            # ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼ ì¡°ì •
            resized_image = cv2.resize(image_gray, None, fx=scale, fy=scale)
            
            if resized_image.shape[0] < h or resized_image.shape[1] < w:
                continue
            
            # í…œí”Œë¦¿ ë§¤ì¹­
            result = cv2.matchTemplate(resized_image, rotated_template, cv2.TM_CCOEFF_NORMED)
            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
            
            # ìµœê³  ë§¤ì¹­ê°’ ì—…ë°ì´íŠ¸
            if best_match is None or max_val > best_match[0]:
                best_match = (max_val, max_loc, angle, scale, resized_image.shape)
    
    return best_match

# íŠ¹ì§•ì  ê¸°ë°˜ ë§¤ì¹­ (ë” ê°•ë ¥í•œ ë°©ë²•)
def feature_based_matching(image, template):
    """íŠ¹ì§•ì  ê¸°ë°˜ í…œí”Œë¦¿ ë§¤ì¹­ (SIFT ì‚¬ìš©)"""
    
    # SIFT ê²€ì¶œê¸° ìƒì„±
    sift = cv2.SIFT_create()
    
    # íŠ¹ì§•ì ê³¼ ë””ìŠ¤í¬ë¦½í„° ê²€ì¶œ
    kp1, des1 = sift.detectAndCompute(template, None)
    kp2, des2 = sift.detectAndCompute(image, None)
    
    if des1 is None or des2 is None:
        return None
    
    # FLANN ë§¤ì²˜ ì‚¬ìš©
    FLANN_INDEX_KDTREE = 1
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
    search_params = dict(checks=50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    
    matches = flann.knnMatch(des1, des2, k=2)
    
    # ì¢‹ì€ ë§¤ì¹­ì ë§Œ ì„ ë³„ (Lowe's ratio test)
    good_matches = []
    for match in matches:
        if len(match) == 2:
            m, n = match
            if m.distance < 0.7 * n.distance:
                good_matches.append(m)
    
    # ì¶©ë¶„í•œ ë§¤ì¹­ì ì´ ìˆìœ¼ë©´ í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚°
    if len(good_matches) > 10:
        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        
        # RANSACìœ¼ë¡œ í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚°
        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
        
        if M is not None:
            # í…œí”Œë¦¿ ëª¨ì„œë¦¬ ë³€í™˜
            h, w = template.shape[:2]
            pts = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)
            dst = cv2.perspectiveTransform(pts, M)
            
            return dst, good_matches, kp1, kp2
    
    return None

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
def comprehensive_template_matching(image, template):
    """í¬ê´„ì  í…œí”Œë¦¿ ë§¤ì¹­ (ì—¬ëŸ¬ ë°©ë²• ì¡°í•©)"""
    
    results = {}
    
    # 1. ê¸°ë³¸ í…œí”Œë¦¿ ë§¤ì¹­
    basic_result = cv2.matchTemplate(
        cv2.cvtColor(image, cv2.COLOR_BGR2GRAY),
        cv2.cvtColor(template, cv2.COLOR_BGR2GRAY),
        cv2.TM_CCOEFF_NORMED
    )
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(basic_result)
    results['basic'] = {'confidence': max_val, 'location': max_loc}
    
    # 2. ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë§¤ì¹­
    multi_scale_result = multi_scale_template_matching(image, template)
    if multi_scale_result:
        results['multi_scale'] = {
            'confidence': multi_scale_result[0],
            'scale': multi_scale_result[3]
        }
    
    # 3. íŠ¹ì§•ì  ê¸°ë°˜ ë§¤ì¹­
    feature_result = feature_based_matching(image, template)
    if feature_result:
        results['feature_based'] = {
            'homography': feature_result[0],
            'matches': len(feature_result[1])
        }
    
    return results
```

> í…œí”Œë¦¿ ë§¤ì¹­ì€ **ì œì¡°ì—…ì˜ í’ˆì§ˆ ê²€ì‚¬**ì—ì„œ ë¶€í’ˆì˜ ìœ„ì¹˜ë¥¼ ì°¾ê±°ë‚˜, **ì˜ë£Œ ì˜ìƒ**ì—ì„œ íŠ¹ì • êµ¬ì¡°ë¬¼ì„ ê²€ì¶œí•˜ëŠ” ë“± **ì •í™•í•œ íŒ¨í„´ ì¸ì‹**ì´ í•„ìš”í•œ ë¶„ì•¼ì—ì„œ í™œìš©ëœë‹¤. {: .prompt-tip}

## â­ íŠ¹ì§•ì  ê²€ì¶œê³¼ ë§¤ì¹­

**íŠ¹ì§•ì (Feature Point) ê²€ì¶œ**ì€ ì´ë¯¸ì§€ì—ì„œ **ê³ ìœ í•˜ê³  ë°˜ë³µ ê°€ëŠ¥í•œ ì ë“¤**ì„ ì°¾ëŠ” ê¸°ìˆ ì´ë‹¤. ì´ëŸ¬í•œ íŠ¹ì§•ì ë“¤ì€ **ì´ë¯¸ì§€ ì •í•©**, **ê°ì²´ ì¸ì‹**, **3D ì¬êµ¬ì„±** ë“±ì— í•µì‹¬ì ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.

### SIFT (Scale-Invariant Feature Transform)

**SIFT**ëŠ” ìŠ¤ì¼€ì¼ê³¼ íšŒì „ì— ë¶ˆë³€ì¸ íŠ¹ì§•ì ì„ ê²€ì¶œí•˜ëŠ” ê°•ë ¥í•œ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# SIFT íŠ¹ì§•ì  ê²€ì¶œ ë° ì‹œê°í™”
def detect_and_visualize_sift(image, max_features=500):
    """SIFT íŠ¹ì§•ì  ê²€ì¶œ ë° ì‹œê°í™”"""
    
    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
    
    # SIFT ê²€ì¶œê¸° ìƒì„±
    sift = cv2.SIFT_create(nfeatures=max_features)
    
    # íŠ¹ì§•ì ê³¼ ë””ìŠ¤í¬ë¦½í„° ê²€ì¶œ
    keypoints, descriptors = sift.detectAndCompute(gray, None)
    
    # íŠ¹ì§•ì ì´ ê·¸ë ¤ì§„ ì´ë¯¸ì§€ ìƒì„±
    img_keypoints = cv2.drawKeypoints(
        image, keypoints, None, 
        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
    )
    
    print(f"ê²€ì¶œëœ SIFT íŠ¹ì§•ì  ê°œìˆ˜: {len(keypoints)}")
    print(f"ë””ìŠ¤í¬ë¦½í„° í¬ê¸°: {descriptors.shape if descriptors is not None else 'None'}")
    
    return keypoints, descriptors, img_keypoints

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')

# SIFT íŠ¹ì§•ì  ê²€ì¶œ
keypoints, descriptors, img_sift = detect_and_visualize_sift(img)

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.title('ì›ë³¸ ì´ë¯¸ì§€')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(cv2.cvtColor(img_sift, cv2.COLOR_BGR2RGB))
plt.title(f'SIFT íŠ¹ì§•ì  ({len(keypoints)}ê°œ)')
plt.axis('off')

plt.tight_layout()
plt.show()

# íŠ¹ì§•ì  ì†ì„± ë¶„ì„
scales = [kp.size for kp in keypoints]
angles = [kp.angle for kp in keypoints]
responses = [kp.response for kp in keypoints]

plt.figure(figsize=(15, 4))

plt.subplot(1, 3, 1)
plt.hist(scales, bins=50, alpha=0.7)
plt.title('íŠ¹ì§•ì  ìŠ¤ì¼€ì¼ ë¶„í¬')
plt.xlabel('ìŠ¤ì¼€ì¼')
plt.ylabel('ê°œìˆ˜')

plt.subplot(1, 3, 2)
plt.hist(angles, bins=36, alpha=0.7)  # 36ê°œ bin (10ë„ì”©)
plt.title('íŠ¹ì§•ì  ë°©í–¥ ë¶„í¬')
plt.xlabel('ê°ë„ (ë„)')
plt.ylabel('ê°œìˆ˜')

plt.subplot(1, 3, 3)
plt.hist(responses, bins=50, alpha=0.7)
plt.title('íŠ¹ì§•ì  ì‘ë‹µ ê°•ë„ ë¶„í¬')
plt.xlabel('ì‘ë‹µ ê°•ë„')
plt.ylabel('ê°œìˆ˜')

plt.tight_layout()
plt.show()
```

### ORB (Oriented FAST and Rotated BRIEF)

**ORB**ëŠ” SIFTì˜ ë¹ ë¥¸ ëŒ€ì•ˆìœ¼ë¡œ, ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•˜ë‹¤.

```python
def compare_feature_detectors(image):
    """ë‹¤ì–‘í•œ íŠ¹ì§•ì  ê²€ì¶œê¸° ë¹„êµ"""
    
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
    
    # ë‹¤ì–‘í•œ ê²€ì¶œê¸° ì •ì˜
    detectors = {
        'SIFT': cv2.SIFT_create(nfeatures=1000),
        'ORB': cv2.ORB_create(nfeatures=1000),
        'AKAZE': cv2.AKAZE_create(),
        'BRISK': cv2.BRISK_create()
    }
    
    results = {}
    
    plt.figure(figsize=(20, 10))
    
    for i, (name, detector) in enumerate(detectors.items(), 1):
        try:
            # íŠ¹ì§•ì  ê²€ì¶œ
            keypoints, descriptors = detector.detectAndCompute(gray, None)
            
            # íŠ¹ì§•ì  ê·¸ë¦¬ê¸°
            img_keypoints = cv2.drawKeypoints(
                image, keypoints, None,
                color=(0, 255, 0),
                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
            )
            
            # ê²°ê³¼ ì €ì¥
            results[name] = {
                'keypoints': keypoints,
                'descriptors': descriptors,
                'count': len(keypoints),
                'descriptor_size': descriptors.shape[1] if descriptors is not None else 0
            }
            
            # ì‹œê°í™”
            plt.subplot(2, 4, i)
            plt.imshow(cv2.cvtColor(img_keypoints, cv2.COLOR_BGR2RGB))
            plt.title(f'{name}\n{len(keypoints)}ê°œ íŠ¹ì§•ì ')
            plt.axis('off')
            
            # ì‘ë‹µ ê°•ë„ íˆìŠ¤í† ê·¸ë¨
            plt.subplot(2, 4, i + 4)
            responses = [kp.response for kp in keypoints]
            plt.hist(responses, bins=30, alpha=0.7)
            plt.title(f'{name} ì‘ë‹µ ê°•ë„')
            plt.xlabel('ì‘ë‹µ ê°•ë„')
            plt.ylabel('ê°œìˆ˜')
            
        except Exception as e:
            print(f"{name} ê²€ì¶œê¸° ì˜¤ë¥˜: {e}")
    
    plt.tight_layout()
    plt.show()
    
    # ì„±ëŠ¥ ë¹„êµ ì¶œë ¥
    print("íŠ¹ì§•ì  ê²€ì¶œê¸° ì„±ëŠ¥ ë¹„êµ:")
    print("-" * 60)
    print(f"{'ê²€ì¶œê¸°':<10} {'íŠ¹ì§•ì  ìˆ˜':<10} {'ë””ìŠ¤í¬ë¦½í„° í¬ê¸°':<15} {'í‰ê·  ì‘ë‹µ':<10}")
    print("-" * 60)
    
    for name, result in results.items():
        avg_response = np.mean([kp.response for kp in result['keypoints']]) if result['keypoints'] else 0
        print(f"{name:<10} {result['count']:<10} {result['descriptor_size']:<15} {avg_response:<10.3f}")
    
    return results

# íŠ¹ì§•ì  ê²€ì¶œê¸° ë¹„êµ ì‹¤í–‰
feature_results = compare_feature_detectors(img)
```

### íŠ¹ì§•ì  ë§¤ì¹­

```python
def match_features(img1, img2, detector_type='SIFT', matcher_type='FLANN'):
    """ë‘ ì´ë¯¸ì§€ ê°„ íŠ¹ì§•ì  ë§¤ì¹­"""
    
    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) if len(img1.shape) == 3 else img1
    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) if len(img2.shape) == 3 else img2
    
    # ê²€ì¶œê¸° ì„ íƒ
    if detector_type == 'SIFT':
        detector = cv2.SIFT_create()
    elif detector_type == 'ORB':
        detector = cv2.ORB_create()
    elif detector_type == 'AKAZE':
        detector = cv2.AKAZE_create()
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²€ì¶œê¸°ì…ë‹ˆë‹¤.")
    
    # íŠ¹ì§•ì ê³¼ ë””ìŠ¤í¬ë¦½í„° ê²€ì¶œ
    kp1, des1 = detector.detectAndCompute(gray1, None)
    kp2, des2 = detector.detectAndCompute(gray2, None)
    
    if des1 is None or des2 is None:
        print("ë””ìŠ¤í¬ë¦½í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ë§¤ì²˜ ì„ íƒ ë° ë§¤ì¹­
    if matcher_type == 'FLANN' and detector_type in ['SIFT', 'AKAZE']:
        # FLANN ë§¤ì²˜ (ë¶€ë™ì†Œìˆ˜ì  ë””ìŠ¤í¬ë¦½í„°ìš©)
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=50)
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        matches = flann.knnMatch(des1, des2, k=2)
        
        # Lowe's ratio testë¡œ ì¢‹ì€ ë§¤ì¹­ ì„ ë³„
        good_matches = []
        for match in matches:
            if len(match) == 2:
                m, n = match
                if m.distance < 0.7 * n.distance:
                    good_matches.append(m)
    
    else:
        # BF ë§¤ì²˜ (ì´ì§„ ë””ìŠ¤í¬ë¦½í„°ìš©)
        if detector_type == 'ORB':
            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        else:
            bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
        
        matches = bf.match(des1, des2)
        good_matches = sorted(matches, key=lambda x: x.distance)[:50]  # ìƒìœ„ 50ê°œë§Œ
    
    # ë§¤ì¹­ ê²°ê³¼ ì‹œê°í™”
    img_matches = cv2.drawMatches(
        img1, kp1, img2, kp2, good_matches, None,
        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
    )
    
    print(f"ê²€ì¶œëœ íŠ¹ì§•ì : ì´ë¯¸ì§€1={len(kp1)}, ì´ë¯¸ì§€2={len(kp2)}")
    print(f"ì¢‹ì€ ë§¤ì¹­: {len(good_matches)}ê°œ")
    
    return {
        'keypoints1': kp1,
        'keypoints2': kp2,
        'matches': good_matches,
        'match_image': img_matches
    }

# ë‘ ì´ë¯¸ì§€ ì¤€ë¹„ (ê°™ì€ ì¥ë©´ì˜ ë‹¤ë¥¸ ì‹œì  ë˜ëŠ” ë‹¤ë¥¸ ì´ë¯¸ì§€)
img1 = cv2.imread('image1.jpg')
img2 = cv2.imread('image2.jpg')

# íŠ¹ì§•ì  ë§¤ì¹­ ìˆ˜í–‰
match_result = match_features(img1, img2, 'SIFT', 'FLANN')

if match_result:
    plt.figure(figsize=(15, 8))
    plt.imshow(cv2.cvtColor(match_result['match_image'], cv2.COLOR_BGR2RGB))
    plt.title('íŠ¹ì§•ì  ë§¤ì¹­ ê²°ê³¼')
    plt.axis('off')
    plt.show()
```

### í˜¸ëª¨ê·¸ë˜í”¼ë¥¼ ì´ìš©í•œ ê°ì²´ ê²€ì¶œ

```python
def detect_object_with_homography(template_img, scene_img, min_matches=10):
    """í˜¸ëª¨ê·¸ë˜í”¼ë¥¼ ì´ìš©í•œ ê°ì²´ ê²€ì¶œ"""
    
    # SIFT ê²€ì¶œê¸° ì‚¬ìš©
    sift = cv2.SIFT_create()
    
    # íŠ¹ì§•ì  ê²€ì¶œ
    kp1, des1 = sift.detectAndCompute(template_img, None)
    kp2, des2 = sift.detectAndCompute(scene_img, None)
    
    if des1 is None or des2 is None:
        return None
    
    # FLANN ë§¤ì¹­
    FLANN_INDEX_KDTREE = 1
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
    search_params = dict(checks=50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(des1, des2, k=2)
    
    # ì¢‹ì€ ë§¤ì¹­ì  ì„ ë³„
    good_matches = []
    for match in matches:
        if len(match) == 2:
            m, n = match
            if m.distance < 0.7 * n.distance:
                good_matches.append(m)
    
    if len(good_matches) < min_matches:
        print(f"ì¶©ë¶„í•œ ë§¤ì¹­ì ì´ ì—†ìŠµë‹ˆë‹¤. ({len(good_matches)}/{min_matches})")
        return None
    
    # í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚°
    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    
    homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
    
    if homography is None:
        print("í˜¸ëª¨ê·¸ë˜í”¼ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # í…œí”Œë¦¿ ì´ë¯¸ì§€ì˜ ëª¨ì„œë¦¬ë¥¼ ì¥ë©´ ì´ë¯¸ì§€ë¡œ ë³€í™˜
    h, w = template_img.shape[:2]
    corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)
    transformed_corners = cv2.perspectiveTransform(corners, homography)
    
    # ê²°ê³¼ ì´ë¯¸ì§€ì— ê²€ì¶œëœ ê°ì²´ í‘œì‹œ
    result_img = scene_img.copy()
    cv2.polylines(result_img, [np.int32(transformed_corners)], True, (0, 255, 0), 3)
    
    # ë§¤ì¹­ ì‹œê°í™”
    matches_img = cv2.drawMatches(
        template_img, kp1, scene_img, kp2, 
        good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
    )
    
    return {
        'result_image': result_img,
        'matches_image': matches_img,
        'homography': homography,
        'corners': transformed_corners,
        'good_matches': len(good_matches),
        'inliers': np.sum(mask)
    }

# ê°ì²´ ê²€ì¶œ ì‹¤í–‰
template = cv2.imread('object_template.jpg')
scene = cv2.imread('scene_with_object.jpg')

detection_result = detect_object_with_homography(template, scene)

if detection_result:
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 1, 1)
    plt.imshow(cv2.cvtColor(detection_result['matches_image'], cv2.COLOR_BGR2RGB))
    plt.title(f'íŠ¹ì§•ì  ë§¤ì¹­ ({detection_result["good_matches"]}ê°œ ë§¤ì¹­)')
    plt.axis('off')
    
    plt.subplot(2, 1, 2)
    plt.imshow(cv2.cvtColor(detection_result['result_image'], cv2.COLOR_BGR2RGB))
    plt.title('ê²€ì¶œëœ ê°ì²´')
    plt.axis('off')
    
    plt.tight_layout()
    plt.show()
    
    print(f"ì¢‹ì€ ë§¤ì¹­: {detection_result['good_matches']}ê°œ")
    print(f"í˜¸ëª¨ê·¸ë˜í”¼ ì¸ë¼ì´ì–´: {detection_result['inliers']}ê°œ")
```

> íŠ¹ì§•ì  ê²€ì¶œì€ **íŒŒë…¸ë¼ë§ˆ ì‚¬ì§„ ìƒì„±**, **ì¦ê°• í˜„ì‹¤**, **ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜** ë“±ì—ì„œ í•µì‹¬ ê¸°ìˆ ë¡œ í™œìš©ë˜ë©°, ìµœê·¼ì—ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ íŠ¹ì§•ì  ê²€ì¶œê¸°ë“¤ì´ ë”ìš± ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. {: .prompt-tip}

## ğŸ“Š íˆìŠ¤í† ê·¸ë¨ ë¶„ì„

**íˆìŠ¤í† ê·¸ë¨(Histogram)**ì€ ì´ë¯¸ì§€ì˜ í”½ì…€ ê°•ë„ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ë‹¤. **ë…¸ì¶œ ìƒíƒœ í™•ì¸**, **ìƒ‰ìƒ ë¶„ì„**, **ì´ë¯¸ì§€ ë³´ì •** ë“±ì— í™œìš©ë˜ë©°, ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ íŠ¹ì„±ì„ íŒŒì•…í•˜ëŠ” ë° ì¤‘ìš”í•œ ë„êµ¬ë‹¤.

### cv2.calcHist() - íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°

```python
cv2.calcHist(images, channels, mask, histSize, ranges)
```

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜:**

- **images**: ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
- **channels**: íˆìŠ¤í† ê·¸ë¨ì„ ê³„ì‚°í•  ì±„ë„ (0=B, 1=G, 2=R)
- **mask**: ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ (Noneì´ë©´ ì „ì²´ ì˜ì—­)
- **histSize**: íˆìŠ¤í† ê·¸ë¨ ë¹ˆ(bin)ì˜ ê°œìˆ˜
- **ranges**: í”½ì…€ê°’ ë²”ìœ„

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

def analyze_histogram(image, title="Image"):
    """ì´ë¯¸ì§€ì˜ ìƒ‰ìƒë³„ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„"""
    
    # BGR ìƒ‰ìƒë³„ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
    colors = ['blue', 'green', 'red']
    plt.figure(figsize=(15, 8))
    
    # ì›ë³¸ ì´ë¯¸ì§€ í‘œì‹œ
    plt.subplot(2, 3, 1)
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title(f'{title} - ì›ë³¸')
    plt.axis('off')
    
    # ê° ì±„ë„ë³„ íˆìŠ¤í† ê·¸ë¨
    for i, color in enumerate(colors):
        hist = cv2.calcHist([image], [i], None, [256], [0, 256])
        
        plt.subplot(2, 3, i + 2)
        plt.plot(hist, color=color)
        plt.title(f'{color.title()} ì±„ë„ íˆìŠ¤í† ê·¸ë¨')
        plt.xlabel('í”½ì…€ ê°•ë„')
        plt.ylabel('í”½ì…€ ìˆ˜')
        plt.xlim([0, 256])
    
    # ì „ì²´ ì±„ë„ í†µí•© íˆìŠ¤í† ê·¸ë¨
    plt.subplot(2, 3, 5)
    for i, color in enumerate(colors):
        hist = cv2.calcHist([image], [i], None, [256], [0, 256])
        plt.plot(hist, color=color, label=f'{color.title()} ì±„ë„')
    plt.title('ì „ì²´ íˆìŠ¤í† 
```

```python
    plt.title('ì „ì²´ ì±„ë„ íˆìŠ¤í† ê·¸ë¨')
    plt.xlabel('í”½ì…€ ê°•ë„')
    plt.ylabel('í”½ì…€ ìˆ˜')
    plt.xlim([0, 256])
    plt.legend()
    
    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ íˆìŠ¤í† ê·¸ë¨
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    hist_gray = cv2.calcHist([gray], [0], None, [256], [0, 256])
    
    plt.subplot(2, 3, 6)
    plt.plot(hist_gray, color='black')
    plt.title('ê·¸ë ˆì´ìŠ¤ì¼€ì¼ íˆìŠ¤í† ê·¸ë¨')
    plt.xlabel('í”½ì…€ ê°•ë„')
    plt.ylabel('í”½ì…€ ìˆ˜')
    plt.xlim([0, 256])
    
    plt.tight_layout()
    plt.show()
    
    # íˆìŠ¤í† ê·¸ë¨ í†µê³„ ë¶„ì„
    print(f"\n=== {title} íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ ===")
    for i, color in enumerate(colors):
        hist = cv2.calcHist([image], [i], None, [256], [0, 256])
        mean_val = np.sum(hist * np.arange(256)) / np.sum(hist)
        std_val = np.sqrt(np.sum(hist * (np.arange(256) - mean_val)**2) / np.sum(hist))
        
        print(f"{color.title()} ì±„ë„:")
        print(f"  í‰ê· : {mean_val:.2f}")
        print(f"  í‘œì¤€í¸ì°¨: {std_val:.2f}")
        print(f"  ìµœëŒ“ê°’ ìœ„ì¹˜: {np.argmax(hist)}")

# ì´ë¯¸ì§€ ì½ê¸°
img = cv2.imread('example.jpg')

# íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ ì‹¤í–‰
analyze_histogram(img, "ì˜ˆì‹œ ì´ë¯¸ì§€")
```

### íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”

**íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”(Histogram Equalization)**ëŠ” ì´ë¯¸ì§€ì˜ ëŒ€ë¹„ë¥¼ ê°œì„ í•˜ëŠ” ê¸°ë²•ì´ë‹¤.

```python
def histogram_equalization_demo(image):
    """íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™” ì „í›„ ë¹„êµ"""
    
    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™” ì ìš©
    equalized = cv2.equalizeHist(gray)
    
    # CLAHE (Contrast Limited Adaptive Histogram Equalization) ì ìš©
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    clahe_result = clahe.apply(gray)
    
    # ì»¬ëŸ¬ ì´ë¯¸ì§€ì— CLAHE ì ìš©
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    lab[:, :, 0] = clahe.apply(lab[:, :, 0])  # L ì±„ë„ì—ë§Œ ì ìš©
    clahe_color = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(20, 12))
    
    images = [
        (gray, 'ì›ë³¸ ê·¸ë ˆì´ìŠ¤ì¼€ì¼', 'gray'),
        (equalized, 'íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”', 'gray'),
        (clahe_result, 'CLAHE', 'gray'),
        (cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 'ì›ë³¸ ì»¬ëŸ¬', None),
        (cv2.cvtColor(clahe_color, cv2.COLOR_BGR2RGB), 'CLAHE ì»¬ëŸ¬', None)
    ]
    
    # ì´ë¯¸ì§€ í‘œì‹œ
    for i, (img_data, title, cmap) in enumerate(images):
        plt.subplot(3, 5, i + 1)
        if cmap:
            plt.imshow(img_data, cmap=cmap)
        else:
            plt.imshow(img_data)
        plt.title(title)
        plt.axis('off')
    
    # íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
    hists = [
        cv2.calcHist([gray], [0], None, [256], [0, 256]),
        cv2.calcHist([equalized], [0], None, [256], [0, 256]),
        cv2.calcHist([clahe_result], [0], None, [256], [0, 256])
    ]
    
    hist_titles = ['ì›ë³¸ íˆìŠ¤í† ê·¸ë¨', 'ê· ë“±í™” íˆìŠ¤í† ê·¸ë¨', 'CLAHE íˆìŠ¤í† ê·¸ë¨']
    colors = ['blue', 'red', 'green']
    
    for i, (hist, title, color) in enumerate(zip(hists, hist_titles, colors)):
        plt.subplot(3, 5, i + 6)
        plt.plot(hist, color=color)
        plt.title(title)
        plt.xlabel('í”½ì…€ ê°•ë„')
        plt.ylabel('í”½ì…€ ìˆ˜')
        plt.xlim([0, 256])
    
    # í†µí•© íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
    plt.subplot(3, 5, 10)
    for hist, title, color in zip(hists, hist_titles, colors):
        plt.plot(hist, color=color, label=title.split()[0])
    plt.title('íˆìŠ¤í† ê·¸ë¨ ë¹„êµ')
    plt.xlabel('í”½ì…€ ê°•ë„')
    plt.ylabel('í”½ì…€ ìˆ˜')
    plt.xlim([0, 256])
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    # ì´ë¯¸ì§€ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
    def calculate_contrast_metrics(img):
        """ëŒ€ë¹„ ê´€ë ¨ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        mean_val = np.mean(img)
        std_val = np.std(img)
        rms_contrast = std_val / mean_val if mean_val > 0 else 0
        
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì •ë³´ëŸ‰ ì¸¡ì •)
        hist = cv2.calcHist([img], [0], None, [256], [0, 256])
        hist_norm = hist / np.sum(hist)
        hist_norm = hist_norm[hist_norm > 0]  # 0 ì œê±°
        entropy = -np.sum(hist_norm * np.log2(hist_norm))
        
        return {
            'mean': mean_val,
            'std': std_val,
            'rms_contrast': rms_contrast,
            'entropy': entropy
        }
    
    print("\n=== ì´ë¯¸ì§€ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¹„êµ ===")
    metrics = [
        calculate_contrast_metrics(gray),
        calculate_contrast_metrics(equalized),
        calculate_contrast_metrics(clahe_result)
    ]
    
    method_names = ['ì›ë³¸', 'íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”', 'CLAHE']
    
    for name, metric in zip(method_names, metrics):
        print(f"\n{name}:")
        print(f"  í‰ê· : {metric['mean']:.2f}")
        print(f"  í‘œì¤€í¸ì°¨: {metric['std']:.2f}")
        print(f"  RMS ëŒ€ë¹„: {metric['rms_contrast']:.3f}")
        print(f"  ì—”íŠ¸ë¡œí”¼: {metric['entropy']:.3f}")

# íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™” ë°ëª¨ ì‹¤í–‰
histogram_equalization_demo(img)
```

### íˆìŠ¤í† ê·¸ë¨ ì—­íˆ¬ì˜

**íˆìŠ¤í† ê·¸ë¨ ì—­íˆ¬ì˜(Histogram Backprojection)**ì€ íŠ¹ì • ìƒ‰ìƒ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì²´ë¥¼ ê²€ì¶œí•˜ëŠ” ê¸°ë²•ì´ë‹¤.

```python
def histogram_backprojection_demo(image, roi_coords=None):
    """íˆìŠ¤í† ê·¸ë¨ ì—­íˆ¬ì˜ì„ ì´ìš©í•œ ê°ì²´ ì¶”ì """
    
    # HSV ìƒ‰ìƒ ê³µê°„ìœ¼ë¡œ ë³€í™˜
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    
    # ROI ì„¤ì • (ê´€ì‹¬ ê°ì²´ ì˜ì—­)
    if roi_coords is None:
        # ì´ë¯¸ì§€ ì¤‘ì•™ ë¶€ë¶„ì„ ROIë¡œ ì„¤ì •
        h, w = image.shape[:2]
        roi_coords = (w//4, h//4, w//2, h//2)
    
    x, y, w_roi, h_roi = roi_coords
    roi = hsv[y:y+h_roi, x:x+w_roi]
    
    # ROIì˜ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° (Hueì™€ Saturation ì±„ë„)
    roi_hist = cv2.calcHist([roi], [0, 1], None, [180, 256], [0, 180, 0, 256])
    
    # íˆìŠ¤í† ê·¸ë¨ ì •ê·œí™”
    cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)
    
    # ì—­íˆ¬ì˜ ìˆ˜í–‰
    backproject = cv2.calcBackProject([hsv], [0, 1], roi_hist, [0, 180, 0, 256], 1)
    
    # ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•œ í•„í„°ë§
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    backproject_filtered = cv2.filter2D(backproject, -1, kernel)
    
    # ì´ì§„í™”
    _, binary = cv2.threshold(backproject_filtered, 50, 255, cv2.THRESH_BINARY)
    
    # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ í›„ì²˜ë¦¬
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
    binary_cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
    binary_cleaned = cv2.morphologyEx(binary_cleaned, cv2.MORPH_OPEN, kernel)
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(15, 10))
    
    # ì›ë³¸ê³¼ ROI
    img_roi = image.copy()
    cv2.rectangle(img_roi, (x, y), (x+w_roi, y+h_roi), (0, 255, 0), 3)
    
    plt.subplot(2, 4, 1)
    plt.imshow(cv2.cvtColor(img_roi, cv2.COLOR_BGR2RGB))
    plt.title('ì›ë³¸ + ROI')
    plt.axis('off')
    
    plt.subplot(2, 4, 2)
    plt.imshow(cv2.cvtColor(roi, cv2.COLOR_HSV2RGB))
    plt.title('ROI (HSV)')
    plt.axis('off')
    
    # íˆìŠ¤í† ê·¸ë¨
    plt.subplot(2, 4, 3)
    plt.imshow(roi_hist, interpolation='nearest')
    plt.title('ROI íˆìŠ¤í† ê·¸ë¨\n(H-S)')
    plt.xlabel('Saturation')
    plt.ylabel('Hue')
    
    # ì—­íˆ¬ì˜ ê²°ê³¼ë“¤
    plt.subplot(2, 4, 4)
    plt.imshow(backproject, cmap='gray')
    plt.title('ì—­íˆ¬ì˜')
    plt.axis('off')
    
    plt.subplot(2, 4, 5)
    plt.imshow(backproject_filtered, cmap='gray')
    plt.title('í•„í„°ë§ í›„')
    plt.axis('off')
    
    plt.subplot(2, 4, 6)
    plt.imshow(binary, cmap='gray')
    plt.title('ì´ì§„í™”')
    plt.axis('off')
    
    plt.subplot(2, 4, 7)
    plt.imshow(binary_cleaned, cmap='gray')
    plt.title('ëª¨í´ë¡œì§€ í›„ì²˜ë¦¬')
    plt.axis('off')
    
    # ìµœì¢… ê²°ê³¼ ì˜¤ë²„ë ˆì´
    result = image.copy()
    result[binary_cleaned > 0] = [0, 255, 0]  # ê²€ì¶œëœ ì˜ì—­ì„ ë…¹ìƒ‰ìœ¼ë¡œ
    overlay = cv2.addWeighted(image, 0.7, result, 0.3, 0)
    
    plt.subplot(2, 4, 8)
    plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))
    plt.title('ìµœì¢… ê²€ì¶œ ê²°ê³¼')
    plt.axis('off')
    
    plt.tight_layout()
    plt.show()
    
    return roi_hist, backproject, binary_cleaned

# ì—­íˆ¬ì˜ ë°ëª¨ ì‹¤í–‰
hist, backproj, binary = histogram_backprojection_demo(img)
```

> íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ì€ **ì˜ë£Œ ì˜ìƒì˜ ëŒ€ë¹„ ê°œì„ **, **ë³´ì•ˆ ì‹œìŠ¤í…œì˜ ì–¼êµ´ ì¸ì‹ ì „ì²˜ë¦¬**, **ììœ¨ì£¼í–‰ì°¨ì˜ ì•¼ê°„ ì‹œì•¼ ê°œì„ ** ë“±ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤. {: .prompt-tip}

## ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬

OpenCVëŠ” ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. **ì›¹ìº  ì…ë ¥**, **ë™ì˜ìƒ íŒŒì¼ ì²˜ë¦¬**, **ì‹¤ì‹œê°„ ì˜ìƒ ë¶„ì„** ë“±ì´ ê°€ëŠ¥í•˜ë‹¤.

### cv2.VideoCapture() - ë¹„ë””ì˜¤ ì…ë ¥

```python
import cv2
import time
import numpy as np

def webcam_basic_demo():
    """ì›¹ìº  ê¸°ë³¸ ì‚¬ìš©ë²• ë°ëª¨"""
    
    # ì›¹ìº  ì—°ê²° (0ì€ ê¸°ë³¸ ì¹´ë©”ë¼)
    cap = cv2.VideoCapture(0)
    
    # ì¹´ë©”ë¼ ì„¤ì •
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    if not cap.isOpened():
        print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ì›¹ìº  ì‹œì‘ (q í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ)")
    
    while True:
        # í”„ë ˆì„ ì½ê¸°
        ret, frame = cap.read()
        
        if not ret:
            print("í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        # í”„ë ˆì„ ì •ë³´ í‘œì‹œ
        height, width = frame.shape[:2]
        info_text = f"Resolution: {width}x{height}"
        cv2.putText(frame, info_text, (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # í˜„ì¬ ì‹œê°„ í‘œì‹œ
        current_time = time.strftime("%Y-%m-%d %H:%M:%S")
        cv2.putText(frame, current_time, (10, height-20), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # í”„ë ˆì„ í‘œì‹œ
        cv2.imshow('Webcam Feed', frame)
        
        # í‚¤ ì…ë ¥ í™•ì¸
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s'):
            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            filename = f"screenshot_{int(time.time())}.jpg"
            cv2.imwrite(filename, frame)
            print(f"ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filename}")
    
    # ë¦¬ì†ŒìŠ¤ í•´ì œ
    cap.release()
    cv2.destroyAllWindows()

# ì›¹ìº  ë°ëª¨ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
# webcam_basic_demo()
```

### ì‹¤ì‹œê°„ í•„í„° ì ìš©

```python
class RealTimeFilters:
    """ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ í•„í„° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.current_filter = 0
        self.filters = [
            self.original,
            self.grayscale,
            self.blur,
            self.edge_detection,
            self.color_pop,
            self.cartoon_effect,
            self.heatmap,
            self.sketch_effect
        ]
        self.filter_names = [
            "Original", "Grayscale", "Blur", "Edge Detection",
            "Color Pop", "Cartoon", "Heatmap", "Sketch"
        ]
    
    def original(self, frame):
        """ì›ë³¸"""
        return frame
    
    def grayscale(self, frame):
        """ê·¸ë ˆì´ìŠ¤ì¼€ì¼"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
    
    def blur(self, frame):
        """ë¸”ëŸ¬ íš¨ê³¼"""
        return cv2.GaussianBlur(frame, (21, 21), 0)
    
    def edge_detection(self, frame):
        """ì—£ì§€ ê²€ì¶œ"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        return cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    
    def color_pop(self, frame):
        """ìƒ‰ìƒ ê°•ì¡° (ë¹¨ê°„ìƒ‰ë§Œ ìœ ì§€)"""
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # ë¹¨ê°„ìƒ‰ ë²”ìœ„ ì •ì˜
        lower_red1 = np.array([0, 50, 50])
        upper_red1 = np.array([10, 255, 255])
        lower_red2 = np.array([170, 50, 50])
        upper_red2 = np.array([180, 255, 255])
        
        # ë¹¨ê°„ìƒ‰ ë§ˆìŠ¤í¬ ìƒì„±
        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
        mask = cv2.bitwise_or(mask1, mask2)
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë°°ê²½
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray_bgr = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
        
        # ë¹¨ê°„ìƒ‰ ë¶€ë¶„ë§Œ ì»¬ëŸ¬ë¡œ ìœ ì§€
        result = gray_bgr.copy()
        result[mask > 0] = frame[mask > 0]
        
        return result
    
    def cartoon_effect(self, frame):
        """ì¹´íˆ° íš¨ê³¼"""
        # ì–‘ë°©í–¥ í•„í„°ë¡œ ë¶€ë“œëŸ½ê²Œ
        bilateral = cv2.bilateralFilter(frame, 15, 80, 80)
        
        # ì—£ì§€ ê²€ì¶œ
        gray = cv2.cvtColor(bilateral, cv2.COLOR_BGR2GRAY)
        gray_blur = cv2.medianBlur(gray, 5)
        edges = cv2.adaptiveThreshold(gray_blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, 
                                      cv2.THRESH_BINARY, 9, 9)
        
        # ìƒ‰ìƒ ì–‘ìí™”
        data = np.float32(bilateral).reshape((-1, 3))
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
        _, labels, centers = cv2.kmeans(data, 8, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        quantized = centers[labels.flatten()].reshape(bilateral.shape)
        
        # ì—£ì§€ì™€ ê²°í•©
        edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        cartoon = cv2.bitwise_and(quantized, edges)
        
        return cartoon
    
    def heatmap(self, frame):
        """ì—´í™”ìƒ íš¨ê³¼"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        heatmap = cv2.applyColorMap(gray, cv2.COLORMAP_JET)
        return heatmap
    
    def sketch_effect(self, frame):
        """ìŠ¤ì¼€ì¹˜ íš¨ê³¼"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ë…¸ì´ì¦ˆ ì œê±°
        gray_blur = cv2.GaussianBlur(gray, (21, 21), 0)
        
        # ìŠ¤ì¼€ì¹˜ íš¨ê³¼ ìƒì„±
        sketch = cv2.divide(gray, gray_blur, scale=256.0)
        
        return cv2.cvtColor(sketch, cv2.COLOR_GRAY2BGR)
    
    def run_filter_demo(self):
        """ì‹¤ì‹œê°„ í•„í„° ë°ëª¨ ì‹¤í–‰"""
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ì‹¤ì‹œê°„ í•„í„° ë°ëª¨")
        print("í‚¤ ì¡°ì‘:")
        print("  ìŠ¤í˜ì´ìŠ¤ë°”: ë‹¤ìŒ í•„í„°")
        print("  r: ì´ì „ í•„í„°")
        print("  q: ì¢…ë£Œ")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í˜„ì¬ í•„í„° ì ìš©
            try:
                filtered_frame = self.filters[self.current_filter](frame)
            except Exception as e:
                print(f"í•„í„° ì ìš© ì˜¤ë¥˜: {e}")
                filtered_frame = frame
            
            # í•„í„° ì´ë¦„ í‘œì‹œ
            filter_name = self.filter_names[self.current_filter]
            cv2.putText(filtered_frame, f"Filter: {filter_name}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(filtered_frame, f"Filter: {filter_name}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1)
            
            # ì¡°ì‘ ì•ˆë‚´
            cv2.putText(filtered_frame, "SPACE: Next Filter, R: Prev Filter, Q: Quit", 
                        (10, filtered_frame.shape[0] - 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            cv2.imshow('Real-time Filters', filtered_frame)
            
            # í‚¤ ì…ë ¥ ì²˜ë¦¬
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord(' '):  # ìŠ¤í˜ì´ìŠ¤ë°”
                self.current_filter = (self.current_filter + 1) % len(self.filters)
                print(f"í•„í„° ë³€ê²½: {self.filter_names[self.current_filter]}")
            elif key == ord('r'):
                self.current_filter = (self.current_filter - 1) % len(self.filters)
                print(f"í•„í„° ë³€ê²½: {self.filter_names[self.current_filter]}")
        
        cap.release()
        cv2.destroyAllWindows()

# ì‹¤ì‹œê°„ í•„í„° ë°ëª¨ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
# filter_demo = RealTimeFilters()
# filter_demo.run_filter_demo()
```

### ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬

```python
def process_video_file(input_path, output_path, processing_function=None):
    """ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ë° ì €ì¥"""
    
    # ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°
    cap = cv2.VideoCapture(input_path)
    
    if not cap.isOpened():
        print(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        return False
    
    # ë¹„ë””ì˜¤ ì†ì„± ê°€ì ¸ì˜¤ê¸°
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"ì…ë ¥ ë¹„ë””ì˜¤ ì •ë³´:")
    print(f"  í•´ìƒë„: {width}x{height}")
    print(f"  FPS: {fps}")
    print(f"  ì´ í”„ë ˆì„: {total_frames}")
    print(f"  ì´ ê¸¸ì´: {total_frames/fps:.2f}ì´ˆ")
    
    # ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # ë˜ëŠ” 'XVID'
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    if not out.isOpened():
        print(f"ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {output_path}")
        cap.release()
        return False
    
    frame_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # ì²˜ë¦¬ í•¨ìˆ˜ ì ìš© (ì œê³µëœ ê²½ìš°)
        if processing_function:
            processed_frame = processing_function(frame)
        else:
            processed_frame = frame
        
        # í”„ë ˆì„ ì €ì¥
        out.write(processed_frame)
        
        frame_count += 1
        
        # ì§„í–‰ë¥  í‘œì‹œ
        if frame_count % (total_frames // 10) == 0:
            progress = (frame_count / total_frames) * 100
            print(f"ì§„í–‰ë¥ : {progress:.1f}% ({frame_count}/{total_frames})")
    
    # ë¦¬ì†ŒìŠ¤ í•´ì œ
    cap.release()
    out.release()
    
    print(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ: {output_path}")
    return True

# ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜ˆì‹œ í•¨ìˆ˜ë“¤
def video_edge_detection(frame):
    """ë¹„ë””ì˜¤ìš© ì—£ì§€ ê²€ì¶œ í•¨ìˆ˜"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 50, 150)
    return cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)

def video_stabilization_demo(frame_list):
    """ê°„ë‹¨í•œ ë¹„ë””ì˜¤ ì•ˆì •í™” ì˜ˆì‹œ"""
    if len(frame_list) < 2:
        return frame_list
    
    # íŠ¹ì§•ì  ê²€ì¶œ ë° ë§¤ì¹­ìœ¼ë¡œ ì¹´ë©”ë¼ ì›€ì§ì„ ì¶”ì •
    # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
    stabilized = []
    prev_gray = cv2.cvtColor(frame_list[0], cv2.COLOR_BGR2GRAY)
    stabilized.append(frame_list[0])
    
    for i in range(1, len(frame_list)):
        curr_gray = cv2.cvtColor(frame_list[i], cv2.COLOR_BGR2GRAY)
        
        # íŠ¹ì§•ì  ê²€ì¶œ
        detector = cv2.ORB_create()
        kp1, des1 = detector.detectAndCompute(prev_gray, None)
        kp2, des2 = detector.detectAndCompute(curr_gray, None)
        
        if des1 is not None and des2 is not None and len(des1) > 10 and len(des2) > 10:
            # ë§¤ì¹­
            matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
            matches = matcher.match(des1, des2)
            matches = sorted(matches, key=lambda x: x.distance)
            
            if len(matches) > 10:
                # í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚°
                src_pts = np.float32([kp1[m.queryIdx].pt for m in matches[:10]]).reshape(-1, 1, 2)
                dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches[:10]]).reshape(-1, 1, 2)
                
                H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)
                
                if H is not None:
                    # ì•ˆì •í™”ëœ í”„ë ˆì„ ìƒì„±
                    h, w = frame_list[i].shape[:2]
                    stabilized_frame = cv2.warpPerspective(frame_list[i], np.linalg.inv(H), (w, h))
                    stabilized.append(stabilized_frame)
                else:
                    stabilized.append(frame_list[i])
            else:
                stabilized.append(frame_list[i])
        else:
            stabilized.append(frame_list[i])
        
        prev_gray = curr_gray
    
    return stabilized

# ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤í–‰ ì˜ˆì‹œ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
# process_video_file('input_video.mp4', 'output_edges.mp4', video_edge_detection)
```

> ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì²˜ë¦¬ëŠ” **ë³´ì•ˆ ê°ì‹œ ì‹œìŠ¤í…œ**, **ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° í•„í„°**, **í™”ìƒ íšŒì˜ ë°°ê²½ ì œê±°** ë“±ì—ì„œ í•µì‹¬ ê¸°ìˆ ë¡œ í™œìš©ëœë‹¤. {: .prompt-tip}

## ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ

OpenCVëŠ” **Haar Cascade**ì™€ **DNN ê¸°ë°˜ ì–¼êµ´ ê²€ì¶œ**ì„ ì œê³µí•œë‹¤. ì‹¤ì‹œê°„ ì–¼êµ´ ì¸ì‹, ê°ì • ë¶„ì„, ë³´ì•ˆ ì‹œìŠ¤í…œ ë“±ì— í™œìš©ëœë‹¤.

### Haar Cascadeë¥¼ ì´ìš©í•œ ì–¼êµ´ ê²€ì¶œ

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

class FaceDetector:
    """ì–¼êµ´ ê²€ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # Haar Cascade ë¶„ë¥˜ê¸° ë¡œë“œ
        try:
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
            self.smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
            print("Haar Cascade ë¶„ë¥˜ê¸° ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"ë¶„ë¥˜ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def detect_faces_image(self, image, draw_results=True):
        """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ì–¼êµ´ ê²€ì¶œ
        faces = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,      # ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼ ê°ì†Œ ë¹„ìœ¨
            minNeighbors=5,       # í›„ë³´ ì‚¬ê°í˜•ì´ ìœ ì§€ë˜ê¸° ìœ„í•œ ìµœì†Œ ì´ì›ƒ ìˆ˜
            minSize=(30, 30),     # ê°€ëŠ¥í•œ ìµœì†Œ ì–¼êµ´ í¬ê¸°
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        result_image = image.copy() if draw_results else None
        face_info = []
        
        for i, (x, y, w, h) in enumerate(faces):
            if draw_results:
                # ì–¼êµ´ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
                cv2.rectangle(result_image, (x, y), (x+w, y+h), (255, 0, 0), 2)
                cv2.putText(result_image, f'Face {i+1}', (x, y-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
            
            # ì–¼êµ´ ROIì—ì„œ ëˆˆ ê²€ì¶œ
            roi_gray = gray[y:y+h, x:x+w]
            roi_color = image[y:y+h, x:x+w]
            
            eyes = self.eye_cascade.detectMultiScale(roi_gray, 1.1, 10, minSize=(10, 10))
            
            eye_info = []
            for (ex, ey, ew, eh) in eyes:
                if draw_results:
                    cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)
                eye_info.append((ex, ey, ew, eh))
            
            # ì›ƒìŒ ê²€ì¶œ (ì–¼êµ´ í•˜ë°˜ë¶€ì—ì„œë§Œ)
            roi_gray_smile = roi_gray[h//2:, :]
            roi_color_smile = roi_color[h//2:, :]
            
            smiles = self.smile_cascade.detectMultiScale(
                roi_gray_smile, 
                scaleFactor=1.7, 
                minNeighbors=22, 
                minSize=(25, 25)
            )
            
            smile_detected = len(smiles) > 0
            if draw_results and smile_detected:
                for (sx, sy, sw, sh) in smiles:
                    cv2.rectangle(roi_color_smile, (sx, sy), (sx+sw, sy+sh), (0, 0, 255), 2)
                cv2.putText(result_image, 'Smile!', (x, y+h+20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            
            # ì–¼êµ´ ì •ë³´ ì €ì¥
            face_info.append({
                'bbox': (x, y, w, h),
                'eyes': eye_info,
                'smile': smile_detected,
                'confidence': 1.0  # Haar CascadeëŠ” ì‹ ë¢°ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
            })
        
        return face_info, result_image
    
    def detect_faces_realtime(self):
        """ì‹¤ì‹œê°„ ì–¼êµ´ ê²€ì¶œ"""
        
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ì‹¤ì‹œê°„ ì–¼êµ´ ê²€ì¶œ ì‹œì‘ (q í‚¤ë¡œ ì¢…ë£Œ)")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ í”„ë ˆì„ í¬ê¸° ì¶•ì†Œ
            small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
            
            # ì–¼êµ´ ê²€ì¶œ
            face_info, result_small = self.detect_faces_image(small_frame, True)
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
            result_frame = cv2.resize(result_small, (frame.shape[1], frame.shape[0]))
            
            # í†µê³„ ì •ë³´ í‘œì‹œ
            info_text = f"Faces detected: {len(face_info)}"
            cv2.putText(result_frame, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            cv2.imshow('Face Detection', result_frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

# ì–¼êµ´ ê²€ì¶œê¸° ì‚¬ìš© ì˜ˆì‹œ
detector = FaceDetector()

# ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê²€ì¶œ (ì˜ˆì‹œ)
# img = cv2.imread('people.jpg')
# faces, result = detector.detect_faces_image(img)
# 
# plt.figure(figsize=(12, 8))
# plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))
# plt.title(f'ì–¼êµ´ ê²€ì¶œ ê²°ê³¼ ({len(faces)}ê°œ ê²€ì¶œ)')
# plt.axis('off')
# plt.show()

# ì‹¤ì‹œê°„ ì–¼êµ´ ê²€ì¶œ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
# detector.detect_faces_realtime()
```

### DNN ê¸°ë°˜ ì–¼êµ´ ê²€ì¶œ

```python
class DNNFaceDetector:
    """DNN ê¸°ë°˜ ì–¼êµ´ ê²€ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path=None, config_path=None):
        """DNN ëª¨ë¸ ì´ˆê¸°í™”"""
        
        try:
            if model_path and config_path:
                # ì‚¬ìš©ì ì œê³µ ëª¨ë¸ ì‚¬ìš©
                self.net = cv2.dnn.readNetFromTensorflow(model_path, config_path)
            else:
                # OpenCVì— í¬í•¨ëœ DNN ëª¨ë¸ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ëª¨ë¸ íŒŒì¼ì´ í•„ìš”)
                print("DNN ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ ëª¨ë¸ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                print("opencv_face_detector_uint8.pbì™€ opencv_face_detector.pbtxt íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                self.net = None
        except Exception as e:
            print(f"DNN ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.net = None
    
    def detect_faces_dnn(self, image, confidence_threshold=0.5):
        """DNNì„ ì´ìš©í•œ ì–¼êµ´ ê²€ì¶œ"""
        
        if self.net is None:
            print("DNN ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return [], image
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        h, w = image.shape[:2]
        blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123])
        
        # DNN ì¶”ë¡ 
        self.net.setInput(blob)
        detections = self.net.forward()
        
        faces = []
        result_image = image.copy()
        
        # ê²€ì¶œ ê²°ê³¼ ì²˜ë¦¬
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            
            if confidence > confidence_threshold:
                # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê³„ì‚°
                x1 = int(detections[0, 0, i, 3] * w)
                y1 = int(detections[0, 0, i, 4] * h)
                x2 = int(detections[0, 0, i, 5] * w)
                y2 = int(detections[0, 0, i, 6] * h)
                
                # ê²°ê³¼ ê·¸ë¦¬ê¸°
                cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ì‹ ë¢°ë„ í‘œì‹œ
                label = f'Face: {confidence:.2f}'
                cv2.putText(result_image, label, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                
                faces.append({
                    'bbox': (x1, y1, x2-x1, y2-y1),
                    'confidence': confidence
                })
        
        return faces, result_image

# DNN ì–¼êµ´ ê²€ì¶œê¸° (ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì‚¬ìš©)
# dnn_detector = DNNFaceDetector('opencv_face_detector_uint8.pb', 
#                                'opencv_face_detector.pbtxt')
```

### ì–¼êµ´ ì¸ì‹ íŒŒì´í”„ë¼ì¸

```python
def create_face_recognition_pipeline():
    """ì–¼êµ´ ì¸ì‹ì„ ìœ„í•œ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸"""
    
    class FaceRecognitionPipeline:
        def __init__(self):
            # ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”
            self.face_cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            
            # ì–¼êµ´ ì¸ì‹ê¸° ì´ˆê¸°í™” (LBPH ì‚¬ìš©)
            self.face_recognizer = cv2.face.LBPHFaceRecognizer_create()
            
            # ì•Œë ¤ì§„ ì–¼êµ´ ë°ì´í„°
            self.known_faces = []
            self.known_names = []
            
        def extract_faces_from_image(self, image, label=None):
            """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ì¶”ì¶œ ë° ë¼ë²¨ë§"""
            
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(100, 100))
            
            face_data = []
            
            for (x, y, w, h) in faces:
                # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
                face_roi = gray[y:y+h, x:x+w]
                
                # í¬ê¸° ì •ê·œí™”
                face_normalized = cv2.resize(face_roi, (200, 200))
                
                # íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”ë¡œ ì¡°ëª… ì •ê·œí™”
                face_normalized = cv2.equalizeHist(face_normalized)
                
                face_data.append({
                    'image': face_normalized,
                    'bbox': (x, y, w, h),
                    'label': label
                })
                
                if label:
                    self.known_faces.append(face_normalized)
                    self.known_names.append(label)
            
            return face_data
        
        def train_recognizer(self):
            """ì–¼êµ´ ì¸ì‹ê¸° í›ˆë ¨"""
            
            if len(self.known_faces) == 0:
                print("í›ˆë ¨í•  ì–¼êµ´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
            unique_names = list(set(self.known_names))
            name_to_id = {name: idx for idx, name in enumerate(unique_names)}
            
            labels = [name_to_id[name] for name in self.known_names]
            
            # í›ˆë ¨
            self.face_recognizer.train(self.known_faces, np.array(labels))
            self.name_to_id = name_to_id
            self.id_to_name = {idx: name for name, idx in name_to_id.items()}
            
            print(f"í›ˆë ¨ ì™„ë£Œ: {len(unique_names)}ëª…, {len(self.known_faces)}ê°œ ì–¼êµ´")
            return True
        
        def recognize_faces(self, image, confidence_threshold=80):
            """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ì¸ì‹"""
            
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(100, 100))
            
            result_image = image.copy()
            recognition_results = []
            
            for (x, y, w, h) in faces:
                # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
                face_roi = gray[y:y+h, x:x+w]
                face_normalized = cv2.resize(face_roi, (200, 200))
                face_normalized = cv2.equalizeHist(face_normalized)
                
                # ì–¼êµ´ ì¸ì‹
                label, confidence = self.face_recognizer.predict(face_normalized)
                
                # ê²°ê³¼ ì²˜ë¦¬
                if confidence < confidence_threshold:
                    name = self.id_to_name.get(label, "Unknown")
                    color = (0, 255, 0)  # ë…¹ìƒ‰ (ì¸ì‹ë¨)
                else:
                    name = "Unknown"
                    color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ (ì¸ì‹ ì•ˆë¨)
                
                # ê²°ê³¼ ê·¸ë¦¬ê¸°
                cv2.rectangle(result_image, (x, y), (x+w, y+h), color, 2)
                
                # ì´ë¦„ê³¼ ì‹ ë¢°ë„ í‘œì‹œ
                text = f"{name} ({confidence:.1f})"
                cv2.putText(result_image, text, (x, y-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                
                recognition_results.append({
                    'bbox': (x, y, w, h),
                    'name': name,
                    'confidence': confidence,
                    'recognized': confidence < confidence_threshold
                })
            
            return recognition_results, result_image
        
        def save_model(self, filepath):
            """í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥"""
            try:
                self.face_recognizer.save(filepath)
                
                # ì´ë¦„ ë§¤í•‘ë„ í•¨ê»˜ ì €ì¥
                import pickle
                mapping_file = filepath.replace('.yml', '_mapping.pkl')
                with open(mapping_file, 'wb') as f:
                    pickle.dump({
                        'name_to_id': self.name_to_id,
                        'id_to_name': self.id_to_name
                    }, f)
                
                print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
                return True
            except Exception as e:
                print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
        
        def load_model(self, filepath):
            """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
            try:
                self.face_recognizer.read(filepath)
                
                # ì´ë¦„ ë§¤í•‘ ë¡œë“œ
                import pickle
                mapping_file = filepath.replace('.yml', '_mapping.pkl')
                with open(mapping_file, 'rb') as f:
                    mapping = pickle.load(f)
                    self.name_to_id = mapping['name_to_id']
                    self.id_to_name = mapping['id_to_name']
                
                print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
                return True
            except Exception as e:
                print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
    
    return FaceRecognitionPipeline()

# ì–¼êµ´ ì¸ì‹ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì˜ˆì‹œ
# pipeline = create_face_recognition_pipeline()
# 
# # í›ˆë ¨ ë°ì´í„° ì¶”ê°€
# person1_img = cv2.imread('person1.jpg')
# pipeline.extract_faces_from_image(person1_img, 'Alice')
# 
# person2_img = cv2.imread('person2.jpg')  
# pipeline.extract_faces_from_image(person2_img, 'Bob')
# 
# # ëª¨ë¸ í›ˆë ¨
# pipeline.train_recognizer()
# 
# # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ì¸ì‹
# test_img = cv2.imread('test_group.jpg')
# results, output = pipeline.recognize_faces(test_img)
# 
# plt.figure(figsize=(12, 8))
# plt.imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))
# plt.title('ì–¼êµ´ ì¸ì‹ ê²°ê³¼')
# plt.axis('off')
# plt.show()
```

> ì–¼êµ´ ê²€ì¶œ ê¸°ìˆ ì€ **ì¶œì… í†µì œ ì‹œìŠ¤í…œ**, **ì†Œì…œ ë¯¸ë””ì–´ ìë™ íƒœê¹…**, **ë””ì§€í„¸ ì¹´ë©”ë¼ì˜ ìë™ ì´ˆì ** ë“±ì—ì„œ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ í™œìš©ë˜ë©°, ìµœê·¼ì—ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë°©ë²•ë“¤ì´ ë”ìš± ì •í™•í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. {: .prompt-tip}

## ğŸš€ ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

OpenCVì˜ ì‹¤ë¬´ í™œìš© ì‚¬ë¡€ë¥¼ í†µí•´ ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ ì‚´í´ë³´ì.

### ë¬¸ì„œ ìŠ¤ìºë„ˆ ì• í”Œë¦¬ì¼€ì´ì…˜

```python
import cv2
import numpy as np
from scipy.spatial import distance as dist
import matplotlib.pyplot as plt

class DocumentScanner:
    """ë¬¸ì„œ ìŠ¤ìºë„ˆ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.original_image = None
        self.processed_image = None
    
    def order_points(self, pts):
        """ë„¤ ì ì„ ì¢Œìƒë‹¨, ìš°ìƒë‹¨, ìš°í•˜ë‹¨, ì¢Œí•˜ë‹¨ ìˆœìœ¼ë¡œ ì •ë ¬"""
        
        # ì¢Œí‘œì˜ í•©ê³¼ ì°¨ì´ë¥¼ ì´ìš©í•´ ì •ë ¬
        s = pts.sum(axis=1)
        diff = np.diff(pts, axis=1)
        
        # ì¢Œìƒë‹¨ì€ í•©ì´ ê°€ì¥ ì‘ê³ , ìš°í•˜ë‹¨ì€ í•©ì´ ê°€ì¥ í¼
        rect = np.zeros((4, 2), dtype="float32")
        rect[0] = pts[np.argmin(s)]      # ì¢Œìƒë‹¨
        rect[2] = pts[np.argmax(s)]      # ìš°í•˜ë‹¨
        
        # ìš°ìƒë‹¨ì€ ì°¨ì´ê°€ ê°€ì¥ ì‘ê³ , ì¢Œí•˜ë‹¨ì€ ì°¨ì´ê°€ ê°€ì¥ í¼
        rect[1] = pts[np.argmin(diff)]   # ìš°ìƒë‹¨
        rect[3] = pts[np.argmax(diff)]   # ì¢Œí•˜ë‹¨
        
        return rect
    
    def four_point_transform(self, image, pts):
        """ë„¤ ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì›ê·¼ ë³€í™˜ ìˆ˜í–‰"""
        
        # ì ë“¤ì„ ì •ë ¬
        rect = self.order_points(pts)
        (tl, tr, br, bl) = rect
        
        # ìƒˆë¡œìš´ ì´ë¯¸ì§€ì˜ ë„ˆë¹„ ê³„ì‚°
        widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
        widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
        maxWidth = max(int(widthA), int(widthB))
        
        # ìƒˆë¡œìš´ ì´ë¯¸ì§€ì˜ ë†’ì´ ê³„ì‚°
        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
        heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
        maxHeight = max(int(heightA), int(heightB))
        
        # ëª©í‘œ ì¢Œí‘œ ì„¤ì •
        dst = np.array([
            [0, 0],
            [maxWidth - 1, 0],
            [maxWidth - 1, maxHeight - 1],
            [0, maxHeight - 1]
        ], dtype="float32")
        
        # ì›ê·¼ ë³€í™˜ í–‰ë ¬ ê³„ì‚° ë° ì ìš©
        M = cv2.getPerspectiveTransform(rect, dst)
        warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))
        
        return warped
    
    def find_document_contour(self, image):
        """ë¬¸ì„œì˜ ìœ¤ê³½ì„  ì°¾ê¸°"""
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ì²˜ë¦¬ ì†ë„ í–¥ìƒ)
        ratio = image.shape[0] / 500.0
        original = image.copy()
        image = cv2.resize(image, (int(image.shape[1] / ratio), 500))
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ë…¸ì´ì¦ˆ ì œê±°
        gray = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # ì—£ì§€ ê²€ì¶œ
        edged = cv2.Canny(gray, 75, 200)
        
        # ì»¨íˆ¬ì–´ ì°¾ê¸°
        contours, _ = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
        contours = sorted(contours, key=cv2.contourArea, reverse=True)[:5]
        
        screen_cnt = None
        
        # 4ê°œì˜ ê¼­ì§“ì ì„ ê°€ì§„ ì»¨íˆ¬ì–´ ì°¾ê¸°
        for c in contours:
            # ì»¨íˆ¬ì–´ ê·¼ì‚¬
            peri = cv2.arcLength(c, True)
            approx = cv2.approxPolyDP(c, 0.02 * peri, True)
            
            # 4ê°œì˜ ì ì„ ê°€ì§„ ì»¨íˆ¬ì–´ê°€ ë¬¸ì„œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            if len(approx) == 4:
                screen_cnt = approx
                break
        
        if screen_cnt is not None:
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì¢Œí‘œ ì¡°ì •
            screen_cnt = screen_cnt * ratio
            screen_cnt = screen_cnt.astype("int")
            return screen_cnt.reshape(4, 2)
        else:
            return None
    
    def enhance_document(self, image):
        """ë¬¸ì„œ ì´ë¯¸ì§€ í–¥ìƒ ì²˜ë¦¬"""
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ì ì‘ì  ì„ê³„ê°’ ì ìš©
        thresh = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 11, 10
        )
        
        # ë…¸ì´ì¦ˆ ì œê±°
        kernel = np.ones((2, 2), np.uint8)
        cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
        cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel)
        
        return cleaned
    
    def scan_document(self, image_path):
        """ë¬¸ì„œ ìŠ¤ìº” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            print(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return None
        
        self.original_image = image.copy()
        
        # ë¬¸ì„œ ìœ¤ê³½ì„  ì°¾ê¸°
        document_contour = self.find_document_contour(image)
        
        if document_contour is None:
            print("ë¬¸ì„œ ìœ¤ê³½ì„ ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ì›ê·¼ ë³€í™˜ìœ¼ë¡œ ë¬¸ì„œ ì •ë©´í™”
        warped = self.four_point_transform(image, document_contour)
        
        # ë¬¸ì„œ ì´ë¯¸ì§€ í–¥ìƒ
        enhanced = self.enhance_document(warped)
        
        self.processed_image = enhanced
        
        # ê²°ê³¼ ì‹œê°í™”
        self.visualize_results(image, document_contour, warped, enhanced)
        
        return enhanced
    
    def visualize_results(self, original, contour, warped, enhanced):
        """ìŠ¤ìº” ê³¼ì • ì‹œê°í™”"""
        
        plt.figure(figsize=(20, 10))
        
        # ì›ë³¸ ì´ë¯¸ì§€ + ê²€ì¶œëœ ìœ¤ê³½ì„ 
        original_with_contour = original.copy()
        cv2.drawContours(original_with_contour, [contour], -1, (0, 255, 0), 3)
        
        plt.subplot(2, 3, 1)
        plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))
        plt.title('1. ì›ë³¸ ì´ë¯¸ì§€')
        plt.axis('off')
        
        plt.subplot(2, 3, 2)
        plt.imshow(cv2.cvtColor(original_with_contour, cv2.COLOR_BGR2RGB))
        plt.title('2. ë¬¸ì„œ ìœ¤ê³½ì„  ê²€ì¶œ')
        plt.axis('off')
        
        plt.subplot(2, 3, 3)
        plt.imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))
        plt.title('3. ì›ê·¼ ë³€í™˜')
        plt.axis('off')
        
        plt.subplot(2, 3, 4)
        plt.imshow(enhanced, cmap='gray')
        plt.title('4. ì´ë¯¸ì§€ í–¥ìƒ')
        plt.axis('off')
        
        # ì „í›„ ë¹„êµ
        plt.subplot(2, 3, 5)
        plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))
        plt.title('ë³€í™˜ ì „')
        plt.axis('off')
        
        plt.subplot(2, 3, 6)
        plt.imshow(enhanced, cmap='gray')
        plt.title('ë³€í™˜ í›„')
        plt.axis('off')
        
        plt.tight_layout()
        plt.show()

# ë¬¸ì„œ ìŠ¤ìºë„ˆ ì‚¬ìš© ì˜ˆì‹œ
# scanner = DocumentScanner()
# result = scanner.scan_document('document_photo.jpg')
# 
# if result is not None:
#     cv2.imwrite('scanned_document.jpg', result)
#     print("ìŠ¤ìº”ëœ ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

### ì‹¤ì‹œê°„ ê°ì²´ ì¶”ì  ì‹œìŠ¤í…œ

```python
class MultiObjectTracker:
    """ë‹¤ì¤‘ ê°ì²´ ì¶”ì  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ë‹¤ì–‘í•œ ì¶”ì  ì•Œê³ ë¦¬ì¦˜ ì´ˆê¸°í™”
        self.tracker_types = {
            'KCF': cv2.TrackerKCF_create,
            'CSRT': cv2.TrackerCSRT_create,
            'MIL': cv2.TrackerMIL_create,
        }
        
        self.trackers = []
        self.colors = []
        self.object_ids = []
        self.next_id = 1
        
        # ë°°ê²½ ì°¨ë¶„ê¸° ì´ˆê¸°í™” (ì›€ì§ì´ëŠ” ê°ì²´ ê²€ì¶œìš©)
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
            detectShadows=True
        )
    
    def generate_random_color(self):
        """ëœë¤ ìƒ‰ìƒ ìƒì„±"""
        return tuple(np.random.randint(0, 255, 3).tolist())
    
    def detect_moving_objects(self, frame):
        """ì›€ì§ì´ëŠ” ê°ì²´ ê²€ì¶œ"""
        
        # ë°°ê²½ ì°¨ë¶„
        fg_mask = self.bg_subtractor.apply(frame)
        
        # ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)
        
        # ì»¨íˆ¬ì–´ ì°¾ê¸°
        contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # ìœ íš¨í•œ í¬ê¸°ì˜ ê°ì²´ë§Œ ì„ ë³„
        min_area = 500
        objects = []
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > min_area:
                x, y, w, h = cv2.boundingRect(contour)
                
                # ì¢…íš¡ë¹„ í•„í„°ë§ (ì‚¬ëŒ í˜•íƒœì˜ ê°ì²´ ì„ ë³„)
                aspect_ratio = h / w
                if 1.2 < aspect_ratio < 4.0:
                    objects.append((x, y, w, h))
        
        return objects, fg_mask
    
    def add_tracker(self, frame, bbox, tracker_type='CSRT'):
        """ìƒˆë¡œìš´ ì¶”ì ê¸° ì¶”ê°€"""
        
        if tracker_type in self.tracker_types:
            tracker = self.tracker_types[tracker_type]()
            
            if tracker.init(frame, tuple(bbox)):
                self.trackers.append(tracker)
                self.colors.append(self.generate_random_color())
                self.object_ids.append(self.next_id)
                self.next_id += 1
                return True
        
        return False
    
    def update_trackers(self, frame):
        """ëª¨ë“  ì¶”ì ê¸° ì—…ë°ì´íŠ¸"""
        
        results = []
        valid_trackers = []
        valid_colors = []
        valid_ids = []
        
        for i, tracker in enumerate(self.trackers):
            success, bbox = tracker.update(frame)
            
            if success:
                # ìœ íš¨í•œ ì¶”ì  ê²°ê³¼
                results.append({
                    'bbox': bbox,
                    'color': self.colors[i],
                    'id': self.object_ids[i],
                    'success': True
                })
                valid_trackers.append(tracker)
                valid_colors.append(self.colors[i])
                valid_ids.append(self.object_ids[i])
            else:
                # ì¶”ì  ì‹¤íŒ¨ - í•´ë‹¹ ì¶”ì ê¸° ì œê±°
                results.append({
                    'id': self.object_ids[i],
                    'success': False
                })
        
        # ìœ íš¨í•œ ì¶”ì ê¸°ë§Œ ìœ ì§€
        self.trackers = valid_trackers
        self.colors = valid_colors
        self.object_ids = valid_ids
        
        return results
    
    def run_tracking_demo(self, video_source=0):
        """ì‹¤ì‹œê°„ ì¶”ì  ë°ëª¨ ì‹¤í–‰"""
        
        cap = cv2.VideoCapture(video_source)
        
        if not cap.isOpened():
            print("ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ë‹¤ì¤‘ ê°ì²´ ì¶”ì  ì‹œì‘")
        print("í‚¤ ì¡°ì‘:")
        print("  ìŠ¤í˜ì´ìŠ¤ë°”: ìë™ ê°ì²´ ê²€ì¶œ ë° ì¶”ì  ì‹œì‘")
        print("  ë§ˆìš°ìŠ¤: ìˆ˜ë™ìœ¼ë¡œ ì¶”ì í•  ê°ì²´ ì„ íƒ")
        print("  r: ëª¨ë“  ì¶”ì ê¸° ë¦¬ì…‹")
        print("  q: ì¢…ë£Œ")
        
        selecting_object = False
        selected_bbox = None
        
        def mouse_callback(event, x, y, flags, param):
            nonlocal selecting_object, selected_bbox
            
            if event == cv2.EVENT_LBUTTONDOWN:
                selecting_object = True
                selected_bbox = [x, y, 0, 0]
            
            elif event == cv2.EVENT_MOUSEMOVE and selecting_object:
                selected_bbox[2] = x - selected_bbox[0]
                selected_bbox[3] = y - selected_bbox[1]
            
            elif event == cv2.EVENT_LBUTTONUP:
                selecting_object = False
                if selected_bbox[2] > 10 and selected_bbox[3] > 10:
                    # ìœ íš¨í•œ í¬ê¸°ì˜ ë°•ìŠ¤ê°€ ì„ íƒë¨
                    return selected_bbox
        
        cv2.namedWindow('Multi-Object Tracking')
        cv2.setMouseCallback('Multi-Object Tracking', mouse_callback)
        
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            display_frame = frame.copy()
            
            # ì¶”ì ê¸° ì—…ë°ì´íŠ¸
            if len(self.trackers) > 0:
                tracking_results = self.update_trackers(frame)
                
                # ì¶”ì  ê²°ê³¼ ì‹œê°í™”
                for result in tracking_results:
                    if result['success']:
                        bbox = result['bbox']
                        color = result['color']
                        obj_id = result['id']
                        
                        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                        x, y, w, h = [int(v) for v in bbox]
                        cv2.rectangle(display_frame, (x, y), (x+w, y+h), color, 2)
                        
                        # ê°ì²´ ID í‘œì‹œ
                        cv2.putText(display_frame, f'ID: {obj_id}', (x, y-10),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            
            # ìë™ ê°ì²´ ê²€ì¶œ (5í”„ë ˆì„ë§ˆë‹¤)
            if frame_count % 5 == 0:
                detected_objects, fg_mask = self.detect_moving_objects(frame)
                
                # ìƒˆë¡œìš´ ê°ì²´ ìë™ ì¶”ê°€ (ê¸°ì¡´ ì¶”ì ê¸°ì™€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²½ìš°)
                for obj_bbox in detected_objects:
                    # ê¸°ì¡´ ì¶”ì ê¸°ì™€ì˜ ê²¹ì¹¨ í™•ì¸
                    overlaps = False
                    for tracker in self.trackers:
                        success, existing_bbox = tracker.update(frame)
                        if success:
                            # IoU ê³„ì‚°í•˜ì—¬ ê²¹ì¹¨ í™•ì¸
                            if self.calculate_iou(obj_bbox, existing_bbox) > 0.3:
                                overlaps = True
                                break
                    
                    if not overlaps and len(self.trackers) < 10:  # ìµœëŒ€ 10ê°œ ê°ì²´
                        self.add_tracker(frame, obj_bbox)
            
            # í˜„ì¬ ì„ íƒ ì¤‘ì¸ ë°•ìŠ¤ í‘œì‹œ
            if selecting_object and selected_bbox:
                x, y, w, h = selected_bbox
                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (255, 255, 255), 2)
            
            # ì •ë³´ í‘œì‹œ
            info_text = f"Tracking {len(self.trackers)} objects"
            cv2.putText(display_frame, info_text, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            cv2.imshow('Multi-Object Tracking', display_frame)
            
            # í‚¤ ì…ë ¥ ì²˜ë¦¬
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                break
            elif key == ord('r'):
                # ëª¨ë“  ì¶”ì ê¸° ë¦¬ì…‹
                self.trackers = []
                self.colors = []
                self.object_ids = []
                print("ëª¨ë“  ì¶”ì ê¸°ê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")
            elif key == ord(' '):
                # ìë™ ê°ì²´ ê²€ì¶œ
                detected_objects, _ = self.detect_moving_objects(frame)
                for obj_bbox in detected_objects:
                    if len(self.trackers) < 10:
                        self.add_tracker(frame, obj_bbox)
                print(f"{len(detected_objects)}ê°œ ê°ì²´ê°€ ê²€ì¶œë˜ì–´ ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        cap.release()
        cv2.destroyAllWindows()
    
    def calculate_iou(self, bbox1, bbox2):
        """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ì˜ IoU ê³„ì‚°"""
        
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        # êµì§‘í•© ì˜ì—­ ê³„ì‚°
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        intersection_area = (x_right - x_left) * (y_bottom - y_top)
        
        # í•©ì§‘í•© ì˜ì—­ ê³„ì‚°
        area1 = w1 * h1
        area2 = w2 * h2
        union_area = area1 + area2 - intersection_area
        
        return intersection_area / union_area if union_area > 0 else 0.0

# ë‹¤ì¤‘ ê°ì²´ ì¶”ì  ì‹œìŠ¤í…œ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
# tracker = MultiObjectTracker()
# tracker.run_tracking_demo()
```

### í’ˆì§ˆ ê²€ì‚¬ ì‹œìŠ¤í…œ

```python
class QualityInspectionSystem:
    """ì œí’ˆ í’ˆì§ˆ ê²€ì‚¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.reference_template = None
        self.defect_threshold = 0.8
        
    def load_reference_template(self, template_path):
        """ê¸°ì¤€ í…œí”Œë¦¿ ë¡œë“œ"""
        self.reference_template = cv2.imread(template_path)
        if self.reference_template is None:
            print(f"ê¸°ì¤€ í…œí”Œë¦¿ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {template_path}")
            return False
        print("ê¸°ì¤€ í…œí”Œë¦¿ ë¡œë“œ ì™„ë£Œ")
        return True
    
    def align_images(self, image, template):
        """ì´ë¯¸ì§€ ì •ë ¬ (íšŒì „, í¬ê¸°, ìœ„ì¹˜ ë³´ì •)"""
        
        # SIFT íŠ¹ì§•ì  ê²€ì¶œ
        sift = cv2.SIFT_create()
        
        kp1, des1 = sift.detectAndCompute(template, None)
        kp2, des2 = sift.detectAndCompute(image, None)
        
        if des1 is None or des2 is None:
            return image  # íŠ¹ì§•ì ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        
        # íŠ¹ì§•ì  ë§¤ì¹­
        matcher = cv2.FlannBasedMatcher(
            dict(algorithm=1, trees=5),
            dict(checks=50)
        )
        
        matches = matcher.knnMatch(des1, des2, k=2)
        
        # ì¢‹ì€ ë§¤ì¹­ì  ì„ ë³„
        good_matches = []
        for match in matches:
            if len(match) == 2:
                m, n = match
                if m.distance < 0.7 * n.distance:
                    good_matches.append(m)
        
        if len(good_matches) < 10:
            return image  # ë§¤ì¹­ì ì´ ë¶€ì¡±í•˜ë©´ ì›ë³¸ ë°˜í™˜
        
        # í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚°
        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        
        homography, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)
        
        if homography is not None:
            # ì´ë¯¸ì§€ ì •ë ¬
            h, w = template.shape[:2]
            aligned = cv2.warpPerspective(image, homography, (w, h))
            return aligned
        
        return image
    
    def detect_defects(self, image, template):
        """ê²°í•¨ ê²€ì¶œ"""
        
        # ì´ë¯¸ì§€ ì •ë ¬
        aligned = self.align_images(image, template)
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
        aligned_gray = cv2.cvtColor(aligned, cv2.COLOR_BGR2GRAY)
        
        # ì°¨ì´ ì´ë¯¸ì§€ ê³„ì‚°
        diff = cv2.absdiff(template_gray, aligned_gray)
        
        # ë…¸ì´ì¦ˆ ì œê±°
        diff = cv2.GaussianBlur(diff, (5, 5), 0)
        
        # ì„ê³„ê°’ ì ìš©
        _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)
        
        # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
        
        # ê²°í•¨ ì˜ì—­ ê²€ì¶œ
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        defects = []
        min_defect_area = 50
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > min_defect_area:
                x, y, w, h = cv2.boundingRect(contour)
                defects.append({
                    'bbox': (x, y, w, h),
                    'area': area,
                    'contour': contour
                })
        
        return defects, aligned, diff, thresh
    
    def classify_defect_severity(self, defect):
        """ê²°í•¨ ì‹¬ê°ë„ ë¶„ë¥˜"""
        
        area = defect['area']
        
        if area < 100:
            return "Minor", (255, 255, 0)  # ë…¸ë€ìƒ‰
        elif area < 500:
            return "Major", (255, 165, 0)  # ì£¼í™©ìƒ‰
        else:
            return "Critical", (255, 0, 0)  # ë¹¨ê°„ìƒ‰
    
    def inspect_product(self, image_path):
        """ì œí’ˆ ê²€ì‚¬ ìˆ˜í–‰"""
        
        if self.reference_template is None:
            print("ê¸°ì¤€ í…œí”Œë¦¿ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # ê²€ì‚¬í•  ì´ë¯¸ì§€ ë¡œë“œ
        test_image = cv2.imread(image_path)
        if test_image is None:
            print(f"ê²€ì‚¬ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return None
        
        # ê²°í•¨ ê²€ì¶œ
        defects, aligned, diff, thresh = self.detect_defects(test_image, self.reference_template)
        
        # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
        result_image = aligned.copy()
        
        # ê²°í•¨ ì˜ì—­ í‘œì‹œ
        defect_summary = {"Minor": 0, "Major": 0, "Critical": 0}
        
        for defect in defects:
            severity, color = self.classify_defect_severity(defect)
            defect_summary[severity] += 1
            
            # ê²°í•¨ ì˜ì—­ í‘œì‹œ
            x, y, w, h = defect['bbox']
            cv2.rectangle(result_image, (x, y), (x+w, y+h), color, 2)
            cv2.putText(result_image, severity, (x, y-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        # ì „ì²´ í’ˆì§ˆ íŒì •
        if defect_summary["Critical"] > 0:
            quality_status = "REJECT"
            status_color = (0, 0, 255)
        elif defect_summary["Major"] > 2:
            quality_status = "REJECT"
            status_color = (0, 0, 255)
        elif defect_summary["Minor"] > 5:
            quality_status = "WARNING"
            status_color = (0, 165, 255)
        else:
            quality_status = "PASS"
            status_color = (0, 255, 0)
        
        # ê²€ì‚¬ ê²°ê³¼ ì‹œê°í™”
        self.visualize_inspection_results(
            self.reference_template, test_image, aligned, 
            diff, thresh, result_image, defects, 
            defect_summary, quality_status, status_color
        )
        
        # ê²€ì‚¬ ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'image_path': image_path,
            'quality_status': quality_status,
            'total_defects': len(defects),
            'defect_summary': defect_summary,
            'defects': defects
        }
        
        return report
    
    def visualize_inspection_results(self, template, original, aligned, 
                                   diff, thresh, result, defects, 
                                   summary, status, status_color):
        """ê²€ì‚¬ ê²°ê³¼ ì‹œê°í™”"""
        
        plt.figure(figsize=(20, 12))
        
        # ê¸°ì¤€ í…œí”Œë¦¿
        plt.subplot(2, 4, 1)
        plt.imshow(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))
        plt.title('ê¸°ì¤€ í…œí”Œë¦¿')
        plt.axis('off')
        
        # ì›ë³¸ ì´ë¯¸ì§€
        plt.subplot(2, 4, 2)
        plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))
        plt.title('ê²€ì‚¬ ëŒ€ìƒ')
        plt.axis('off')
        
        # ì •ë ¬ëœ ì´ë¯¸ì§€
        plt.subplot(2, 4, 3)
        plt.imshow(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))
        plt.title('ì •ë ¬ëœ ì´ë¯¸ì§€')
        plt.axis('off')
        
        # ì°¨ì´ ì´ë¯¸ì§€
        plt.subplot(2, 4, 4)
        plt.imshow(diff, cmap='hot')
        plt.title('ì°¨ì´ ì´ë¯¸ì§€')
        plt.axis('off')
        
        # ì„ê³„ê°’ ì ìš©
        plt.subplot(2, 4, 5)
        plt.imshow(thresh, cmap='gray')
        plt.title('ê²°í•¨ ì˜ì—­')
        plt.axis('off')
        
        # ìµœì¢… ê²°ê³¼
        plt.subplot(2, 4, 6)
        plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))
        plt.title(f'ê²€ì‚¬ ê²°ê³¼: {status}')
        plt.axis('off')
        
        # ê²°í•¨ í†µê³„
        plt.subplot(2, 4, 7)
        categories = list(summary.keys())
        counts = list(summary.values())
        colors = ['yellow', 'orange', 'red']
        
        bars = plt.bar(categories, counts, color=colors)
        plt.title('ê²°í•¨ ë¶„í¬')
        plt.ylabel('ê°œìˆ˜')
        
        # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ
        for bar, count in zip(bars, counts):
            if count > 0:
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        str(count), ha='center', va='bottom')
        
        # ê²€ì‚¬ ìš”ì•½
        plt.subplot(2, 4, 8)
        plt.text(0.1, 0.8, f'í’ˆì§ˆ ìƒíƒœ: {status}', fontsize=14, weight='bold',
                color='red' if status == 'REJECT' else 'green' if status == 'PASS' else 'orange')
        plt.text(0.1, 0.6, f'ì´ ê²°í•¨ ìˆ˜: {len(defects)}', fontsize=12)
        plt.text(0.1, 0.4, f'ì‹¬ê°: {summary["Critical"]}', fontsize=12, color='red')
        plt.text(0.1, 0.3, f'ì¤‘ìš”: {summary["Major"]}', fontsize=12, color='orange')
        plt.text(0.1, 0.2, f'ê²½ë¯¸: {summary["Minor"]}', fontsize=12, color='yellow')
        plt.xlim(0, 1)
        plt.ylim(0, 1)
        plt.axis('off')
        plt.title('ê²€ì‚¬ ìš”ì•½')
        
        plt.tight_layout()
        plt.show()

# í’ˆì§ˆ ê²€ì‚¬ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ
# inspector = QualityInspectionSystem()
# inspector.load_reference_template('reference_product.jpg')
# 
# # ì œí’ˆ ê²€ì‚¬ ìˆ˜í–‰
# report = inspector.inspect_product('test_product.jpg')
# 
# if report:
#     print(f"ê²€ì‚¬ ê²°ê³¼: {report['quality_status']}")
#     print(f"ì´ ê²°í•¨ ìˆ˜: {report['total_defects']}")
#     print(f"ê²°í•¨ ë¶„í¬: {report['defect_summary']}")
```

> ì‹¤ë¬´ì—ì„œ OpenCVëŠ” **ìë™í™”ëœ í’ˆì§ˆ ê´€ë¦¬**, **ì˜ë£Œ ì˜ìƒ ë¶„ì„**, **ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ**, **ë³´ì•ˆ ê°ì‹œ** ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•µì‹¬ ê¸°ìˆ ë¡œ í™œìš©ë˜ë©°, ì •í™•ë„ì™€ ì²˜ë¦¬ ì†ë„ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³µì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤. {: .prompt-tip}

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

OpenCV ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ì‹¤ì „ ê¸°ë²•ë“¤ì„ ì‚´í´ë³´ì.

### ë©”ëª¨ë¦¬ì™€ ì—°ì‚° ìµœì í™”

```python
import cv2
import numpy as np
import time
from functools import wraps

def measure_performance(func):
    """ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")
        return result
    return wrapper

class OpenCVOptimization:
    """OpenCV ì„±ëŠ¥ ìµœì í™” ê¸°ë²•"""
    
    def __init__(self):
        self.cache = {}
    
    @measure_performance
    def naive_image_processing(self, image):
        """ë¹„íš¨ìœ¨ì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ (ì˜ˆì‹œ)"""
        
        # ë§¤ë²ˆ ìƒˆë¡œìš´ ë°°ì—´ ìƒì„± (ë¹„íš¨ìœ¨ì )
        result = np.zeros_like(image)
        
        # í”½ì…€ë³„ ë°˜ë³µë¬¸ (ë§¤ìš° ë¹„íš¨ìœ¨ì )
        for i in range(image.shape[0]):
            for j in range(image.shape[1]):
                # ë³µì¡í•œ ê³„ì‚°
                result[i, j] = np.clip(image[i, j] * 1.2 + 10, 0, 255)
        
        return result
    
    @measure_performance  
    def optimized_image_processing(self, image):
        """ìµœì í™”ëœ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        
        # ë²¡í„°í™”ëœ ì—°ì‚° ì‚¬ìš©
        result = np.clip(image.astype(np.float32) * 1.2 + 10, 0, 255).astype(np.uint8)
        return result
    
    def memory_efficient_resize(self, image, target_size, interpolation=cv2.INTER_LINEAR):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¦¬ì‚¬ì´ì¦ˆ"""
        
        # í° ì´ë¯¸ì§€ì˜ ê²½ìš° ë‹¨ê³„ì  ë¦¬ì‚¬ì´ì¦ˆ
        current_size = max(image.shape[:2])
        target_max = max(target_size)
        
        if current_size > target_max * 4:
            # ë‹¨ê³„ì ìœ¼ë¡œ ì¶•ì†Œ
            intermediate_size = current_size // 2
            while intermediate_size > target_max * 2:
                scale = intermediate_size / current_size
                image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=interpolation)
                current_size = intermediate_size
                intermediate_size //= 2
        
        # ìµœì¢… ë¦¬ì‚¬ì´ì¦ˆ
        scale_x = target_size[0] / image.shape[1]
        scale_y = target_size[1] / image.shape[0]
        
        return cv2.resize(image, target_size, fx=scale_x, fy=scale_y, interpolation=interpolation)
    
    def cached_template_matching(self, image, template, method=cv2.TM_CCOEFF_NORMED):
        """ìºì‹œëœ í…œí”Œë¦¿ ë§¤ì¹­"""
        
        # í…œí”Œë¦¿ì˜ í•´ì‹œë¥¼ í‚¤ë¡œ ì‚¬ìš©
        template_hash = hash(template.tobytes())
        
        if template_hash in self.cache:
            print("ìºì‹œì—ì„œ í…œí”Œë¦¿ ë¡œë“œ")
            processed_template = self.cache[template_hash]
        else:
            # í…œí”Œë¦¿ ì „ì²˜ë¦¬
            processed_template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
            processed_template = cv2.GaussianBlur(processed_template, (3, 3), 0)
            self.cache[template_hash] = processed_template
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        gray_image = cv2.GaussianBlur(gray_image, (3, 3), 0)
        
        # í…œí”Œë¦¿ ë§¤ì¹­
        result = cv2.matchTemplate(gray_image, processed_template, method)
        return result
    
    def roi_based_processing(self, image, roi_coords, processing_func):
        """ROI ê¸°ë°˜ ì²˜ë¦¬ë¡œ ì—°ì‚°ëŸ‰ ê°ì†Œ"""
        
        x, y, w, h = roi_coords
        
        # ROIë§Œ ì¶”ì¶œí•˜ì—¬ ì²˜ë¦¬
        roi = image[y:y+h, x:x+w]
        processed_roi = processing_func(roi)
        
        # ê²°ê³¼ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ì— ë³µì‚¬
        result = image.copy()
        result[y:y+h, x:x+w] = processed_roi
        
        return result
    
    def multi_scale_processing(self, image, scales=[0.5, 1.0]):
        """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì²˜ë¦¬"""
        
        results = []
        
        for scale in scales:
            if scale != 1.0:
                # ìŠ¤ì¼€ì¼ ì¡°ì •
                scaled_image = cv2.resize(image, None, fx=scale, fy=scale)
            else:
                scaled_image = image
            
            # ì²˜ë¦¬ ìˆ˜í–‰ (ì˜ˆ: ì–¼êµ´ ê²€ì¶œ)
            gray = cv2.cvtColor(scaled_image, cv2.COLOR_BGR2GRAY)
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
            
            # ì¢Œí‘œë¥¼ ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            if scale != 1.0:
                faces = (faces / scale).astype(int)
            
            results.extend(faces)
        
        return results

# ì„±ëŠ¥ ë¹„êµ ë°ëª¨
def performance_comparison_demo():
    """ì„±ëŠ¥ ìµœì í™” íš¨ê³¼ ë¹„êµ"""
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    test_image = np.random.randint(0, 255, (1000, 1000, 3), dtype=np.uint8)
    
    optimizer = OpenCVOptimization()
    
    print("=== ì„±ëŠ¥ ë¹„êµ ë°ëª¨ ===")
    
    # 1. í”½ì…€ ë‹¨ìœ„ vs ë²¡í„°í™” ì—°ì‚°
    print("\n1. í”½ì…€ ë‹¨ìœ„ ì—°ì‚° vs ë²¡í„°í™” ì—°ì‚°")
    
    # ì‘ì€ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸ (í”½ì…€ ë‹¨ìœ„ëŠ” ë„ˆë¬´ ëŠë¦¼)
    small_image = test_image[:100, :100]
    
    result1 = optimizer.naive_image_processing(small_image)
    result2 = optimizer.optimized_image_processing(small_image)
    
    # 2. ì¼ë°˜ ë¦¬ì‚¬ì´ì¦ˆ vs ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¦¬ì‚¬ì´ì¦ˆ
    print("\n2. ë¦¬ì‚¬ì´ì¦ˆ ì„±ëŠ¥ ë¹„êµ")
    
    large_image = np.random.randint(0, 255, (4000, 4000, 3), dtype=np.uint8)
    target_size = (800, 600)
    
    @measure_performance
    def standard_resize(img, size):
        return cv2.resize(img, size)
    
    @measure_performance  
    def efficient_resize(img, size):
        return optimizer.memory_efficient_resize(img, size)
    
    result3 = standard_resize(large_image, target_size)
    result4 = efficient_resize(large_image, target_size)
    
    # 3. ROI ê¸°ë°˜ ì²˜ë¦¬
    print("\n3. ì „ì²´ ì´ë¯¸ì§€ vs ROI ê¸°ë°˜ ì²˜ë¦¬")
    
    def expensive_processing(img):
        # ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        return cv2.GaussianBlur(img, (21, 21), 0)
    
    @measure_performance
    def process_full_image(img):
        return expensive_processing(img)
    
    @measure_performance
    def process_roi_only(img):
        roi_coords = (200, 200, 400, 400)  # ì¤‘ì•™ 400x400 ì˜ì—­
        return optimizer.roi_based_processing(img, roi_coords, expensive_processing)
    
    result5 = process_full_image(test_image)
    result6 = process_roi_only(test_image)

# ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
# performance_comparison_demo()
```

### ë©€í‹°ìŠ¤ë ˆë”©ê³¼ ë³‘ë ¬ ì²˜ë¦¬

```python
import cv2
import numpy as np
import threading
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import queue
import time

class ParallelProcessing:
    """ë³‘ë ¬ ì²˜ë¦¬ ê¸°ë²•ë“¤"""
    
    def __init__(self):
        self.cpu_count = mp.cpu_count()
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´: {self.cpu_count}")
    
    def process_image_chunk(self, args):
        """ì´ë¯¸ì§€ ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜"""
        chunk, operation = args
        
        if operation == 'blur':
            return cv2.GaussianBlur(chunk, (15, 15), 0)
        elif operation == 'edge':
            gray = cv2.cvtColor(chunk, cv2.COLOR_BGR2GRAY) if len(chunk.shape) == 3 else chunk
            return cv2.Canny(gray, 50, 150)
        elif operation == 'sharpen':
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            return cv2.filter2D(chunk, -1, kernel)
        
        return chunk
    
    def parallel_image_processing(self, image, operation='blur', num_processes=None):
        """ì´ë¯¸ì§€ë¥¼ ë¶„í• í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬"""
        
        if num_processes is None:
            num_processes = self.cpu_count
        
        h, w = image.shape[:2]
        
        # ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§ìœ¼ë¡œ ë¶„í• 
        chunk_height = h // num_processes
        chunks = []
        
        for i in range(num_processes):
            start_y = i * chunk_height
            end_y = (i + 1) * chunk_height if i < num_processes - 1 else h
            
            chunk = image[start_y:end_y, :].copy()
            chunks.append((chunk, operation))
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        with ProcessPoolExecutor(max_workers=num_processes) as executor:
            processed_chunks = list(executor.map(self.process_image_chunk, chunks))
        
        # ì²˜ë¦¬ëœ ì²­í¬ë“¤ì„ í•©ì¹˜ê¸°
        if operation == 'edge':
            result = np.vstack(processed_chunks)
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì„ 3ì±„ë„ë¡œ ë³€í™˜
            if len(result.shape) == 2:
                result = cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)
        else:
            result = np.vstack(processed_chunks)
        
        return result
    
    def threaded_video_processing(self, video_source=0):
        """ë©€í‹°ìŠ¤ë ˆë“œ ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        
        # ë¹„ë””ì˜¤ ìº¡ì²˜
        cap = cv2.VideoCapture(video_source)
        
        # í”„ë ˆì„ í
        frame_queue = queue.Queue(maxsize=10)
        result_queue = queue.Queue(maxsize=10)
        
        # í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ
        def frame_reader():
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if not frame_queue.full():
                    frame_queue.put(frame)
        
        # í”„ë ˆì„ ì²˜ë¦¬ ìŠ¤ë ˆë“œ
        def frame_processor():
            while True:
                try:
                    frame = frame_queue.get(timeout=1)
                    
                    # ì´ë¯¸ì§€ ì²˜ë¦¬ (ì˜ˆ: ë¸”ëŸ¬ íš¨ê³¼)
                    processed = cv2.GaussianBlur(frame, (15, 15), 0)
                    
                    if not result_queue.full():
                        result_queue.put(processed)
                    
                    frame_queue.task_done()
                except queue.Empty:
                    continue
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        reader_thread = threading.Thread(target=frame_reader)
        processor_thread = threading.Thread(target=frame_processor)
        
        reader_thread.daemon = True
        processor_thread.daemon = True
        
        reader_thread.start()
        processor_thread.start()
        
        print("ë©€í‹°ìŠ¤ë ˆë“œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘ (që¡œ ì¢…ë£Œ)")
        
        while True:
            try:
                processed_frame = result_queue.get(timeout=1)
                cv2.imshow('Threaded Processing', processed_frame)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                    
                result_queue.task_done()
            except queue.Empty:
                continue
        
        cap.release()
        cv2.destroyAllWindows()
    
    def batch_image_processing(self, image_paths, output_dir, operation='resize'):
        """ë°°ì¹˜ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        
        def process_single_image(args):
            input_path, output_path, op = args
            
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ
                image = cv2.imread(input_path)
                if image is None:
                    return f"ì‹¤íŒ¨: {input_path} (ë¡œë“œ ë¶ˆê°€)"
                
                # ì²˜ë¦¬ ìˆ˜í–‰
                if op == 'resize':
                    processed = cv2.resize(image, (800, 600))
                elif op == 'thumbnail':
                    processed = cv2.resize(image, (200, 150))
                elif op == 'grayscale':
                    processed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                    processed = cv2.cvtColor(processed, cv2.COLOR_GRAY2BGR)
                else:
                    processed = image
                
                # ì €ì¥
                cv2.imwrite(output_path, processed)
                return f"ì„±ê³µ: {input_path}"
                
            except Exception as e:
                return f"ì˜¤ë¥˜: {input_path} - {str(e)}"
        
        # ì‘ì—… ëª©ë¡ ìƒì„±
        tasks = []
        for i, input_path in enumerate(image_paths):
            output_filename = f"processed_{i:04d}.jpg"
            output_path = f"{output_dir}/{output_filename}"
            tasks.append((input_path, output_path, operation))
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        start_time = time.time()
        
        with ProcessPoolExecutor(max_workers=self.cpu_count) as executor:
            results = list(executor.map(process_single_image, tasks))
        
        end_time = time.time()
        
        # ê²°ê³¼ ì¶œë ¥
        success_count = sum(1 for r in results if r.startswith("ì„±ê³µ"))
        print(f"\në°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  ì´ íŒŒì¼: {len(image_paths)}")
        print(f"  ì„±ê³µ: {success_count}")
        print(f"  ì‹¤íŒ¨: {len(image_paths) - success_count}")
        print(f"  ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        
        return results

# ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ
def parallel_processing_demo():
    """ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ ë°ëª¨"""
    
    # í° í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    test_image = np.random.randint(0, 255, (2000, 2000, 3), dtype=np.uint8)
    
    processor = ParallelProcessing()
    
    print("=== ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ ===")
    
    # ìˆœì°¨ ì²˜ë¦¬ vs ë³‘ë ¬ ì²˜ë¦¬
    @measure_performance
    def sequential_processing(img):
        return cv2.GaussianBlur(img, (15, 15), 0)
    
    @measure_performance
    def parallel_processing_func(img):
        return processor.parallel_image_processing(img, 'blur', processor.cpu_count)
    
    print("1. ìˆœì°¨ ì²˜ë¦¬:")
    result1 = sequential_processing(test_image)
    
    print("2. ë³‘ë ¬ ì²˜ë¦¬:")
    result2 = parallel_processing_func(test_image)
    
    print(f"ì´ë¯¸ì§€ í¬ê¸°: {test_image.shape}")
    print(f"ì‚¬ìš©ëœ í”„ë¡œì„¸ìŠ¤ ìˆ˜: {processor.cpu_count}")

# ë³‘ë ¬ ì²˜ë¦¬ ë°ëª¨ ì‹¤í–‰
# parallel_processing_demo()
```

### GPU ê°€ì† (OpenCV CUDA)

```python
# ì£¼ì˜: CUDA ì§€ì› OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤
try:
    import cv2
    
    # CUDA ì§€ì› í™•ì¸
    cuda_available = cv2.cuda.getCudaEnabledDeviceCount() > 0
    print(f"CUDA ì§€ì› ì¥ì¹˜ ìˆ˜: {cv2.cuda.getCudaEnabledDeviceCount()}")
    
except AttributeError:
    cuda_available = False
    print("CUDA ì§€ì›ì´ ì—†ëŠ” OpenCVì…ë‹ˆë‹¤.")

if cuda_available:
    class GPUAcceleration:
        """GPU ê°€ì† ì²˜ë¦¬"""
        
        def __init__(self):
            self.device_count = cv2.cuda.getCudaEnabledDeviceCount()
            print(f"CUDA ì¥ì¹˜ {self.device_count}ê°œ ê°ì§€")
        
        def gpu_image_processing(self, image):
            """GPUë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ì²˜ë¦¬"""
            
            # ì´ë¯¸ì§€ë¥¼ GPU ë©”ëª¨ë¦¬ë¡œ ì—…ë¡œë“œ
            gpu_image = cv2.cuda_GpuMat()
            gpu_image.upload(image)
            
            # GPUì—ì„œ ì²˜ë¦¬ ìˆ˜í–‰
            gpu_blurred = cv2.cuda.bilateralFilter(gpu_image, -1, 50, 50)
            gpu_gray = cv2.cuda.cvtColor(gpu_blurred, cv2.COLOR_BGR2GRAY)
            
            # CPUë¡œ ë‹¤ìš´ë¡œë“œ
            result = gpu_gray.download()
            
            return result
        
        def compare_cpu_gpu_performance(self, image, iterations=10):
            """CPU vs GPU ì„±ëŠ¥ ë¹„êµ"""
            
            print(f"=== CPU vs GPU ì„±ëŠ¥ ë¹„êµ ({iterations}íšŒ ë°˜ë³µ) ===")
            
            # CPU ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            for _ in range(iterations):
                blurred = cv2.bilateralFilter(image, -1, 50, 50)
                gray = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)
            cpu_time = time.time() - start_time
            
            # GPU ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            gpu_image = cv2.cuda_GpuMat()
            
            for _ in range(iterations):
                gpu_image.upload(image)
                gpu_blurred = cv2.cuda.bilateralFilter(gpu_image, -1, 50, 50)
                gpu_gray = cv2.cuda.cvtColor(gpu_blurred, cv2.COLOR_BGR2GRAY)
                result = gpu_gray.download()
            
            gpu_time = time.time() - start_time
            
            print(f"CPU ì²˜ë¦¬ ì‹œê°„: {cpu_time:.4f}ì´ˆ")
            print(f"GPU ì²˜ë¦¬ ì‹œê°„: {gpu_time:.4f}ì´ˆ")
            print(f"GPU ê°€ì† ë¹„ìœ¨: {cpu_time/gpu_time:.2f}x")
            
            return cpu_time, gpu_time

# ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
class MemoryOptimization:
    """ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ë²•"""
    
    def __init__(self):
        self.temp_images = []
    
    def memory_efficient_cascade(self, image, operations):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì—°ì† ì²˜ë¦¬"""
        
        result = image
        
        for operation in operations:
            if operation == 'blur':
                # ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ë®ì–´ì“°ê¸°
                result = cv2.GaussianBlur(result, (5, 5), 0, dst=result)
            elif operation == 'resize':
                result = cv2.resize(result, (result.shape[1]//2, result.shape[0]//2))
            elif operation == 'gray':
                result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
        
        return result
    
    def pre_allocate_arrays(self, shape, dtype, count=5):
        """ë°°ì—´ ë¯¸ë¦¬ í• ë‹¹"""
        
        self.temp_images = [np.empty(shape, dtype=dtype) for _ in range(count)]
        print(f"{count}ê°œì˜ ì„ì‹œ ë°°ì—´ í• ë‹¹ ì™„ë£Œ: {shape}")
    
    def use_pre_allocated_array(self, image, operation_func):
        """ë¯¸ë¦¬ í• ë‹¹ëœ ë°°ì—´ ì‚¬ìš©"""
        
        if len(self.temp_images) > 0:
            temp = self.temp_images.pop()
            result = operation_func(image, temp)
            self.temp_images.append(temp)  # ì¬ì‚¬ìš©ì„ ìœ„í•´ ë°˜í™˜
            return result
        else:
            return operation_func(image)

print("\n=== OpenCV ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ ===")
print("ì£¼ìš” ìµœì í™” ê¸°ë²•:")
print("1. ë²¡í„°í™” ì—°ì‚° í™œìš©")
print("2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬")
print("3. ìºì‹œ í™œìš©")  
print("4. ROI ê¸°ë°˜ ì²˜ë¦¬")
print("5. ë³‘ë ¬ ì²˜ë¦¬")
if cuda_available:
    print("6. GPU ê°€ì†")
print("7. ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”")
```

> ì„±ëŠ¥ ìµœì í™”ëŠ” **ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ì‹œìŠ¤í…œ**ì—ì„œ í•„ìˆ˜ì ì´ë©°, **ë²¡í„°í™” ì—°ì‚°**, **ë³‘ë ¬ ì²˜ë¦¬**, **ë©”ëª¨ë¦¬ ê´€ë¦¬** ë“±ì„ í†µí•´ ìˆ˜ì‹­ ë°°ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤. {: .prompt-tip}

## ğŸ¯ ë§ˆë¬´ë¦¬

ì´ ê°€ì´ë“œë¥¼ í†µí•´ OpenCVì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ê³¼ ì‹¤ë¬´ í™œìš©ë²•ì„ ì‚´í´ë´¤ë‹¤. OpenCVëŠ” ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë„˜ì–´ì„œ **ì»´í“¨í„° ë¹„ì „ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì˜ í•„ìˆ˜ ë„êµ¬**ë‹¤.

### í•µì‹¬ ìš”ì  ì •ë¦¬

**ê¸°ë³¸ ì´ë¯¸ì§€ ì²˜ë¦¬ë¶€í„° ê³ ê¸‰ ê¸°ëŠ¥ê¹Œì§€** OpenCVëŠ” í¬ê´„ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•œë‹¤. ì´ë¯¸ì§€ ì½ê¸°/ì €ì¥ë¶€í„° íŠ¹ì§•ì  ê²€ì¶œ, ì–¼êµ´ ì¸ì‹ê¹Œì§€ ëª¨ë“  ë‹¨ê³„ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.

**ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™”**ê°€ OpenCVì˜ ê°€ì¥ í° ê°•ì ì´ë‹¤. ì›¹ìº  ì…ë ¥, ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬, ì‹¤ì‹œê°„ í•„í„° ì ìš© ë“±ì´ ëª¨ë‘ ê°€ëŠ¥í•˜ë©°, ì ì ˆí•œ ìµœì í™”ë¥¼ í†µí•´ ìƒì—…ì  ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.

**ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œì˜ í™œìš©**ì´ OpenCVì˜ ë§¤ë ¥ì´ë‹¤. ì˜ë£Œ ì˜ìƒ ë¶„ì„, ììœ¨ì£¼í–‰, ë³´ì•ˆ ì‹œìŠ¤í…œ, ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬ ë“±ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ê³  ìˆìœ¼ë©°, ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ ì°½ì¶œí•˜ê³  ìˆë‹¤.

### ë‹¤ìŒ ë‹¨ê³„ í•™ìŠµ ë°©í–¥

**ë”¥ëŸ¬ë‹ê³¼ì˜ ì—°ê³„**ë¥¼ í†µí•´ ë”ìš± ê°•ë ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤. OpenCVì˜ DNN ëª¨ë“ˆì„ í™œìš©í•˜ì—¬ YOLO, SSD ë“±ì˜ ìµœì‹  ê°ì²´ ê²€ì¶œ ëª¨ë¸ì„ í†µí•©í•˜ê±°ë‚˜, TensorFlow/PyTorchì™€ ì—°ê³„í•˜ì—¬ ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼ë³„ íŠ¹í™”**ë„ ì¤‘ìš”í•œ ë°©í–¥ì´ë‹¤. ì˜ë£Œ ì˜ìƒì—ì„œëŠ” DICOM ì²˜ë¦¬, ë¡œë³´í‹±ìŠ¤ì—ì„œëŠ” 3D ë¹„ì „, ììœ¨ì£¼í–‰ì—ì„œëŠ” ì°¨ì„  ê²€ì¶œê³¼ ê°ì²´ ì¶”ì  ë“± ê° ë¶„ì•¼ì˜ íŠ¹ìˆ˜í•œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” ê¸°ìˆ ì„ ê°œë°œí•  ìˆ˜ ìˆë‹¤.

**ì„±ëŠ¥ ìµœì í™”ì™€ ë°°í¬**ëŠ” ì‹¤ë¬´ì—ì„œ í•„ìˆ˜ì ì´ë‹¤. CUDA ê°€ì†, ë©€í‹°ìŠ¤ë ˆë”©, ëª¨ë°”ì¼ ìµœì í™” ë“±ì„ í†µí•´ ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ í™•ë³´í•´ì•¼ í•œë‹¤.

### ì‹¤ë¬´ í”„ë¡œì íŠ¸ ì œì•ˆ

í•™ìŠµí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•´ë³´ì:

1. **ì‹¤ì‹œê°„ ì–¼êµ´ ì¸ì‹ ì¶œì… ì‹œìŠ¤í…œ** - ì–¼êµ´ ê²€ì¶œ, ì¸ì‹, ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
2. **ë¬¸ì„œ ìë™ ìŠ¤ìº” ë° OCR ì‹œìŠ¤í…œ** - ë¬¸ì„œ ê²€ì¶œ, ì›ê·¼ ë³€í™˜, í…ìŠ¤íŠ¸ ì¸ì‹
3. **ì œí’ˆ ë¶ˆëŸ‰ ê²€ì‚¬ ìë™í™”** - í…œí”Œë¦¿ ë§¤ì¹­, ê²°í•¨ ê²€ì¶œ, í’ˆì§ˆ íŒì •
4. **ì‹¤ì‹œê°„ ê°ì²´ ì¶”ì  CCTV** - ì›€ì§ì„ ê²€ì¶œ, ë‹¤ì¤‘ ê°ì²´ ì¶”ì , ì´ë²¤íŠ¸ ì•Œë¦¼

> OpenCV ë§ˆìŠ¤í„°ê°€ ë˜ëŠ” ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ **ì‹¤ì œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í”„ë¡œì íŠ¸**ë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¡ ê³¼ ì‹¤ìŠµì„ ë°˜ë³µí•˜ë©° ì ì§„ì ìœ¼ë¡œ ë³µì¡í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ì. {: .prompt-tip}

ì»´í“¨í„° ë¹„ì „ì€ AIì˜ ëˆˆì´ ë˜ëŠ” ê¸°ìˆ ì´ë‹¤. OpenCVë¥¼ í†µí•´ ì„¸ìƒì„ ë‹¤ë¥´ê²Œ ë³´ëŠ” ëŠ¥ë ¥ì„ ê¸°ë¥´ê³ , í˜ì‹ ì ì¸ ì†”ë£¨ì…˜ì„ ë§Œë“¤ì–´ë‚˜ê°€ê¸¸ ë°”ë€ë‹¤.