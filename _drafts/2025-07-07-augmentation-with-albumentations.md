---
title: "Albumentations ì™„ë²½ ê°€ì´ë“œ: ì´ë¯¸ì§€ ì¦ê°•ì˜ ëª¨ë“  ê²ƒ"
date: 2025-07-07 18:57:00 +0900
categories: 
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

- albumentations==1.4.15
- opencv-python==4.9.0.80
- pillow==10.4.0
- numpy==1.26.4
- torch==2.5.1
- torchvision==0.20.1
- matplotlib==3.10.1

## ğŸš€ TL;DR

- **Albumentations**ëŠ” ì´ë¯¸ì§€ ì¦ê°•ì„ ìœ„í•œ ê³ ì„±ëŠ¥ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, **OpenCV ê¸°ë°˜**ìœ¼ë¡œ êµ¬ì¶•ë˜ì–´ ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ë¥¼ ìë‘í•œë‹¤
- **Transform-Compose-Pipeline** êµ¬ì¡°ë¡œ ë‹¤ì–‘í•œ ë³€í™˜ì„ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì¦ê°• íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤
- **Offline ì¦ê°•**ì€ ë¯¸ë¦¬ ì´ë¯¸ì§€ë¥¼ ë³€í™˜í•´ ì €ì¥í•˜ê³ , **Online ì¦ê°•**ì€ í•™ìŠµ ì‹œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ì‹ì´ë‹¤
- **PyTorch Dataset**ê³¼ **DataLoader**ì— ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©ë˜ì–´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì—ì„œ seamlessí•˜ê²Œ í™œìš© ê°€ëŠ¥í•˜ë‹¤
- **Bounding Box, Keypoint, Mask** ë“± ë‹¤ì–‘í•œ ì–´ë…¸í…Œì´ì…˜ í˜•íƒœë¥¼ ì§€ì›í•˜ì—¬ ê°ì²´ íƒì§€, ë¶„í• , í¬ì¦ˆ ì¶”ì • ë“±ì— í™œìš©ëœë‹¤
- **A.ReplayCompose**ë¥¼ í†µí•´ **ì¬í˜„ ê°€ëŠ¥í•œ ì¦ê°•**ì„ êµ¬í˜„í•  ìˆ˜ ìˆì–´ ë””ë²„ê¹…ê³¼ ê²°ê³¼ ë¶„ì„ì— ìœ ìš©í•˜ë‹¤
- **ì»´í“¨í„° ë¹„ì „ ëŒ€íšŒ**ì™€ **ì‹¤ë¬´ í”„ë¡œì íŠ¸**ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ í•„ìˆ˜ ë„êµ¬ë¡œ ë„ë¦¬ ì‚¬ìš©ëœë‹¤

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [Albumentations ì™„ë²½ ê°€ì´ë“œ ì‹¤ìŠµ](https://github.com/yuiyeong/notebooks/blob/main/computer_vision/albumentations_guide.ipynb)

## ğŸ¨ Albumentationsë€?

**Albumentations**ëŠ” ì´ë¯¸ì§€ ì¦ê°•(Image Augmentation)ì„ ìœ„í•œ ë¹ ë¥´ê³  ìœ ì—°í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤. ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í›ˆë ¨ ë°ì´í„°ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ë° ì‚¬ìš©ëœë‹¤.

ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” **OpenCV**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì–´ ë‹¤ë¥¸ ì´ë¯¸ì§€ ì¦ê°• ë¼ì´ë¸ŒëŸ¬ë¦¬ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ë¥¼ ì œê³µí•œë‹¤. ë˜í•œ **ê°ì²´ íƒì§€**, **ì´ë¯¸ì§€ ë¶„í• **, **í‚¤í¬ì¸íŠ¸ ê²€ì¶œ** ë“± ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì–´ë…¸í…Œì´ì…˜ í˜•íƒœë¥¼ ëª¨ë‘ ì§€ì›í•œë‹¤.

> AlbumentationsëŠ” **Kaggle ì»´í“¨í„° ë¹„ì „ ëŒ€íšŒ**ì—ì„œ ìƒìœ„ íŒ€ë“¤ì´ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì´ë¯¸ì§€ ì¦ê°• ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì‹¤ë¬´ì—ì„œë„ í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ê³  ìˆë‹¤. {: .prompt-tip}

### ì£¼ìš” íŠ¹ì§•

- **ë¹ ë¥¸ ì„±ëŠ¥**: OpenCV ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ë¹„ 2-3ë°° ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„
- **ë‹¤ì–‘í•œ ë³€í™˜**: 80ê°€ì§€ ì´ìƒì˜ ì´ë¯¸ì§€ ë³€í™˜ ê¸°ë²• ì œê³µ
- **ì–´ë…¸í…Œì´ì…˜ ì§€ì›**: Bounding Box, Keypoint, Mask ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ë¼ë²¨ ë™ì‹œ ë³€í™˜
- **ìœ ì—°í•œ íŒŒì´í”„ë¼ì¸**: ë³€í™˜ë“¤ì„ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì¦ê°• ì‹œë‚˜ë¦¬ì˜¤ êµ¬ì„± ê°€ëŠ¥
- **í”„ë ˆì„ì›Œí¬ ë…ë¦½ì **: PyTorch, TensorFlow, Keras ë“± ëª¨ë“  ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì™€ í˜¸í™˜

```python
import albumentations as A
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ê°„ë‹¨í•œ ì˜ˆì‹œ
image = cv2.imread('sample_image.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# ê¸°ë³¸ ë³€í™˜ ì •ì˜
transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.Rotate(limit=30, p=0.5),
])

# ë³€í™˜ ì ìš©
augmented = transform(image=image)
augmented_image = augmented['image']

print(f"ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
print(f"ë³€í™˜ëœ ì´ë¯¸ì§€ í¬ê¸°: {augmented_image.shape}")
# ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: (480, 640, 3)
# ë³€í™˜ëœ ì´ë¯¸ì§€ í¬ê¸°: (480, 640, 3)
```

## ğŸ§© Albumentationsì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

AlbumentationsëŠ” **ëª¨ë“ˆëŸ¬ ì„¤ê³„**ë¥¼ ì±„íƒí•˜ì—¬ ê° ë³€í™˜ì„ ë…ë¦½ì ì¸ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±í•˜ê³ , ì´ë“¤ì„ ì¡°í•©í•´ ë³µì¡í•œ ì¦ê°• íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

```mermaid
graph TD
    A[Transform] --> B[BasicTransform]
    A --> C[ImageOnlyTransform] 
    A --> D[DualTransform]
    B --> E[Compose]
    C --> E
    D --> E
    E --> F[Pipeline]
    F --> G[ReplayCompose]
    
    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style F fill:#e8f5e8
```

### Transform í´ë˜ìŠ¤ ê³„ì¸µêµ¬ì¡°

**Transform**ì€ ëª¨ë“  ë³€í™˜ì˜ ê¸°ë³¸ í´ë˜ìŠ¤ë¡œ, ì„¸ ê°€ì§€ ì£¼ìš” ìœ í˜•ìœ¼ë¡œ ë‚˜ë‰œë‹¤.

- **ImageOnlyTransform**: ì´ë¯¸ì§€ì—ë§Œ ì ìš©ë˜ëŠ” ë³€í™˜ (ë…¸ì´ì¦ˆ ì¶”ê°€, ìƒ‰ìƒ ì¡°ì • ë“±)
- **DualTransform**: ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ì— ë™ì‹œ ì ìš©ë˜ëŠ” ë³€í™˜ (íšŒì „, í¬ê¸° ì¡°ì • ë“±)
- **BasicTransform**: ê°€ì¥ ê¸°ë³¸ì ì¸ ë³€í™˜ ì¸í„°í˜ì´ìŠ¤

```python
import albumentations as A

# ImageOnlyTransform ì˜ˆì‹œ
image_only_transforms = [
    A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),
    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),
    A.ToGray(p=1.0),
]

# DualTransform ì˜ˆì‹œ  
dual_transforms = [
    A.HorizontalFlip(p=1.0),
    A.Rotate(limit=45, p=1.0),
    A.RandomCrop(height=224, width=224, p=1.0),
]

# ê° ë³€í™˜ì˜ íƒ€ì… í™•ì¸
for transform in image_only_transforms:
    print(f"{transform.__class__.__name__}: {type(transform).__bases__}")
    
for transform in dual_transforms:
    print(f"{transform.__class__.__name__}: {type(transform).__bases__}")

# GaussNoise: (<class 'albumentations.core.transforms_interface.ImageOnlyTransform'>,)
# ColorJitter: (<class 'albumentations.core.transforms_interface.ImageOnlyTransform'>,)
# ToGray: (<class 'albumentations.core.transforms_interface.ImageOnlyTransform'>,)
# HorizontalFlip: (<class 'albumentations.core.transforms_interface.DualTransform'>,)
# Rotate: (<class 'albumentations.core.transforms_interface.DualTransform'>,)
# RandomCrop: (<class 'albumentations.core.transforms_interface.DualTransform'>,)
```

### Compose: ë³€í™˜ ì¡°í•©ì˜ í•µì‹¬

**A.Compose**ëŠ” ì—¬ëŸ¬ ë³€í™˜ì„ í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¡°í•©í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë‹¤. ìˆœì°¨ì ìœ¼ë¡œ ë³€í™˜ì„ ì ìš©í•˜ë©°, ê° ë³€í™˜ì˜ í™•ë¥ ì„ ê°œë³„ì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆë‹¤.

```python
import albumentations as A
import numpy as np

# ë³µì¡í•œ ë³€í™˜ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
complex_transform = A.Compose([
    # ê¸°í•˜í•™ì  ë³€í™˜
    A.OneOf([
        A.HorizontalFlip(p=1.0),
        A.VerticalFlip(p=1.0),
        A.Rotate(limit=90, p=1.0),
    ], p=0.8),
    
    # ìƒ‰ìƒ ë³€í™˜
    A.OneOf([
        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=1.0),
        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0),
        A.RandomGamma(gamma_limit=(80, 120), p=1.0),
    ], p=0.6),
    
    # ë…¸ì´ì¦ˆ ë° ë¸”ëŸ¬
    A.OneOf([
        A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),
        A.Blur(blur_limit=3, p=1.0),
        A.MotionBlur(blur_limit=3, p=1.0),
    ], p=0.4),
    
    # í¬ê¸° ì¡°ì • (í•­ìƒ ì ìš©)
    A.Resize(height=256, width=256, p=1.0),
])

# ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
dummy_image = np.random.randint(0, 255, (300, 400, 3), dtype=np.uint8)

# ë³€í™˜ ì ìš©
result = complex_transform(image=dummy_image)
print(f"ë³€í™˜ í›„ ì´ë¯¸ì§€ í¬ê¸°: {result['image'].shape}")
# ë³€í™˜ í›„ ì´ë¯¸ì§€ í¬ê¸°: (256, 256, 3)
```

### í™•ë¥  ì œì–´ ì‹œìŠ¤í…œ

Albumentationsì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” **ì„¸ë°€í•œ í™•ë¥  ì œì–´**ë‹¤. ê° ë³€í™˜ë§ˆë‹¤ ì ìš© í™•ë¥ ì„ ì„¤ì •í•˜ê³ , **A.OneOf**ì™€ **A.SomeOf**ë¥¼ ì‚¬ìš©í•´ ì¡°ê±´ë¶€ ë³€í™˜ì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.

```python
import albumentations as A

# í™•ë¥  ê¸°ë°˜ ë³€í™˜ ì˜ˆì‹œ
probabilistic_transform = A.Compose([
    # 50% í™•ë¥ ë¡œ ìˆ˜í‰ ë’¤ì§‘ê¸°
    A.HorizontalFlip(p=0.5),
    
    # OneOf: í•˜ë‚˜ë§Œ ì„ íƒ (ì „ì²´ ê·¸ë£¹ì˜ 80% í™•ë¥ ë¡œ ì ìš©)
    A.OneOf([
        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),
        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0),
        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),
    ], p=0.8),
    
    # SomeOf: ì—¬ëŸ¬ ê°œ ì„ íƒ (0~2ê°œë¥¼ 70% í™•ë¥ ë¡œ ì ìš©)
    A.SomeOf([
        A.GaussNoise(var_limit=(10.0, 30.0), p=1.0),
        A.Blur(blur_limit=3, p=1.0),
        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1.0),
    ], n=2, p=0.7),  # ìµœëŒ€ 2ê°œê¹Œì§€ ì ìš©
])

# ì—¬ëŸ¬ ë²ˆ ì ìš©í•´ì„œ í™•ë¥ ì  ë™ì‘ í™•ì¸
dummy_image = np.random.randint(0, 255, (200, 200, 3), dtype=np.uint8)

for i in range(5):
    result = probabilistic_transform(image=dummy_image)
    print(f"ì‹¤í–‰ {i+1}: ë³€í™˜ ì™„ë£Œ")
    
# ì‹¤í–‰ 1: ë³€í™˜ ì™„ë£Œ
# ì‹¤í–‰ 2: ë³€í™˜ ì™„ë£Œ
# ì‹¤í–‰ 3: ë³€í™˜ ì™„ë£Œ
# ì‹¤í–‰ 4: ë³€í™˜ ì™„ë£Œ
# ì‹¤í–‰ 5: ë³€í™˜ ì™„ë£Œ
```

## ğŸ”„ ê¸°ë³¸ ì´ë¯¸ì§€ ë³€í™˜ ë°©ë²•ë“¤

AlbumentationsëŠ” **80ê°€ì§€ ì´ìƒ**ì˜ ë‹¤ì–‘í•œ ë³€í™˜ ê¸°ë²•ì„ ì œê³µí•œë‹¤. ì´ë“¤ì€ í¬ê²Œ **ê¸°í•˜í•™ì  ë³€í™˜**, **ìƒ‰ìƒ ë³€í™˜**, **ë…¸ì´ì¦ˆ/ë¸”ëŸ¬ ë³€í™˜**, **í¬ë¡­/ë¦¬ì‚¬ì´ì¦ˆ ë³€í™˜**ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.

### ê¸°í•˜í•™ì  ë³€í™˜ (Geometric Transforms)

ê¸°í•˜í•™ì  ë³€í™˜ì€ ì´ë¯¸ì§€ì˜ ê³µê°„ì  êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ëŠ” ë³€í™˜ìœ¼ë¡œ, ê°ì²´ì˜ ìœ„ì¹˜ë‚˜ ëª¨ì–‘ì´ í•¨ê»˜ ë³€ê²½ëœë‹¤.

```python
import albumentations as A
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (ì²´ìŠ¤íŒ íŒ¨í„´)
def create_checkerboard(height, width, square_size=20):
    image = np.zeros((height, width, 3), dtype=np.uint8)
    for i in range(0, height, square_size):
        for j in range(0, width, square_size):
            if (i // square_size + j // square_size) % 2 == 0:
                image[i:i+square_size, j:j+square_size] = [255, 255, 255]
    return image

sample_image = create_checkerboard(200, 200)

# ê¸°í•˜í•™ì  ë³€í™˜ë“¤
geometric_transforms = {
    'HorizontalFlip': A.HorizontalFlip(p=1.0),
    'VerticalFlip': A.VerticalFlip(p=1.0),
    'Rotate': A.Rotate(limit=45, p=1.0),
    'RandomScale': A.RandomScale(scale_limit=0.3, p=1.0),
    'Affine': A.Affine(scale=(0.8, 1.2), translate_percent=0.1, rotate=(-30, 30), shear=(-10, 10), p=1.0),
    'Perspective': A.Perspective(scale=(0.05, 0.1), p=1.0),
}

# ê° ë³€í™˜ ì ìš© ë° ê²°ê³¼ ì¶œë ¥
for name, transform in geometric_transforms.items():
    result = transform(image=sample_image)
    print(f"{name}: {result['image'].shape}")
    
# HorizontalFlip: (200, 200, 3)
# VerticalFlip: (200, 200, 3)
# Rotate: (200, 200, 3)
# RandomScale: (200, 200, 3)
# Affine: (200, 200, 3)
# Perspective: (200, 200, 3)
```

### ìƒ‰ìƒ ë³€í™˜ (Color Transforms)

ìƒ‰ìƒ ë³€í™˜ì€ ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ì„ ë³€ê²½í•˜ë˜ ê³µê°„ì  êµ¬ì¡°ëŠ” ìœ ì§€í•˜ëŠ” ë³€í™˜ì´ë‹¤.

```python
# ìƒ‰ìƒ ë³€í™˜ë“¤
color_transforms = {
    'RandomBrightnessContrast': A.RandomBrightnessContrast(
        brightness_limit=0.3, contrast_limit=0.3, p=1.0
    ),
    'HueSaturationValue': A.HueSaturationValue(
        hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0
    ),
    'ColorJitter': A.ColorJitter(
        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0
    ),
    'ChannelShuffle': A.ChannelShuffle(p=1.0),
    'ToGray': A.ToGray(p=1.0),
    'RandomGamma': A.RandomGamma(gamma_limit=(80, 120), p=1.0),
}

# ì»¬ëŸ¬ ì´ë¯¸ì§€ ìƒì„± (ê·¸ë¼ë””ì–¸íŠ¸)
def create_gradient_image(height, width):
    image = np.zeros((height, width, 3), dtype=np.uint8)
    for i in range(height):
        for j in range(width):
            image[i, j] = [
                int(255 * i / height),  # Red channel
                int(255 * j / width),   # Green channel
                int(255 * (i + j) / (height + width))  # Blue channel
            ]
    return image

color_sample = create_gradient_image(150, 150)

# ìƒ‰ìƒ ë³€í™˜ ì ìš©
for name, transform in color_transforms.items():
    result = transform(image=color_sample)
    print(f"{name}: ë³€í™˜ ì™„ë£Œ, ë°ì´í„° íƒ€ì…: {result['image'].dtype}")
    
# RandomBrightnessContrast: ë³€í™˜ ì™„ë£Œ, ë°ì´í„° íƒ€ì…: uint8
# HueSaturationValue: ë³€í™˜ ì™„ë£Œ, ë°ì´í„° íƒ€ì…: uint8
# ColorJitter: ë³€í™˜ ì™„ë£Œ, ë°ì´í„° íƒ€ì…: uint8
# ChannelShuffle: ë³€í™˜ ì™„ë£Œ, ë°ì´í„° íƒ€ì…: uint8
# ToGray: ë³€í™˜ ì™„ë£Œ, ë°ì´í„° íƒ€ì…: uint8
# RandomGamma: ë³€í™˜ ì™„ë£Œ, ë°ì´í„° íƒ€ì…: uint8
```

### ë…¸ì´ì¦ˆ ë° ë¸”ëŸ¬ ë³€í™˜

ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ ì˜ë„ì ìœ¼ë¡œ ë³€í™”ì‹œì¼œ ëª¨ë¸ì˜ robustnessë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë³€í™˜ë“¤ì´ë‹¤.

```python
# ë…¸ì´ì¦ˆ ë° ë¸”ëŸ¬ ë³€í™˜ë“¤
noise_blur_transforms = {
    'GaussNoise': A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),
    'ISONoise': A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1.0),
    'MultiplicativeNoise': A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),
    'Blur': A.Blur(blur_limit=7, p=1.0),
    'MotionBlur': A.MotionBlur(blur_limit=7, p=1.0),
    'GaussianBlur': A.GaussianBlur(blur_limit=7, p=1.0),
    'Sharpen': A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1.0),
}

# ê¹”ë”í•œ ì´ë¯¸ì§€ ìƒì„±
clean_image = np.full((100, 100, 3), [128, 128, 128], dtype=np.uint8)
# ì¤‘ì•™ì— í°ìƒ‰ ì‚¬ê°í˜• ì¶”ê°€
clean_image[30:70, 30:70] = [255, 255, 255]

# ë…¸ì´ì¦ˆ/ë¸”ëŸ¬ ë³€í™˜ ì ìš©
for name, transform in noise_blur_transforms.items():
    result = transform(image=clean_image)
    # í”½ì…€ ê°’ ë²”ìœ„ í™•ì¸
    min_val, max_val = result['image'].min(), result['image'].max()
    print(f"{name}: í”½ì…€ ë²”ìœ„ [{min_val}, {max_val}]")
    
# GaussNoise: í”½ì…€ ë²”ìœ„ [58, 255]
# ISONoise: í”½ì…€ ë²”ìœ„ [84, 255]
# MultiplicativeNoise: í”½ì…€ ë²”ìœ„ [115, 255]
# Blur: í”½ì…€ ë²”ìœ„ [128, 255]
# MotionBlur: í”½ì…€ ë²”ìœ„ [128, 255]
# GaussianBlur: í”½ì…€ ë²”ìœ„ [128, 255]
# Sharpen: í”½ì…€ ë²”ìœ„ [64, 255]
```

## ğŸ’¾ Offline ì´ë¯¸ì§€ ì¦ê°•

**Offline ì¦ê°•**ì€ ëª¨ë¸ í›ˆë ¨ ì´ì „ì— ë¯¸ë¦¬ ì´ë¯¸ì§€ë¥¼ ë³€í™˜í•˜ì—¬ ë””ìŠ¤í¬ì— ì €ì¥í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ ë°©ë²•ì€ **ë™ì¼í•œ ë°ì´í„°ì…‹ì„ ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©**í•˜ê±°ë‚˜ **ì¦ê°•ëœ ë°ì´í„°ë¥¼ ì˜êµ¬ ë³´ê´€**í•´ì•¼ í•  ë•Œ ìœ ìš©í•˜ë‹¤.

### Offline ì¦ê°•ì˜ ì¥ë‹¨ì 

**ì¥ì **

- í›ˆë ¨ ì¤‘ ë³€í™˜ ì‹œê°„ì´ ì—†ì–´ ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥
- ì¦ê°•ëœ ë°ì´í„°ë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
- ë³€í™˜ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ ê²€ì¦ ê°€ëŠ¥

**ë‹¨ì **

- ë§ì€ ì €ì¥ ê³µê°„ í•„ìš”
- ë°ì´í„° ë‹¤ì–‘ì„±ì´ ì œí•œì  (ê³ ì •ëœ ë³€í™˜)
- ë³€í™˜ íŒŒë¼ë¯¸í„° ë³€ê²½ ì‹œ ì „ì²´ ì¬ìƒì„± í•„ìš”

```python
import albumentations as A
import cv2
import os
import numpy as np
from pathlib import Path

def offline_augmentation(input_dir, output_dir, transforms, multiplier=5):
    """
    ì˜¤í”„ë¼ì¸ ì´ë¯¸ì§€ ì¦ê°• í•¨ìˆ˜
    
    Args:
        input_dir: ì›ë³¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
        output_dir: ì¦ê°•ëœ ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬  
        transforms: Albumentations ë³€í™˜ íŒŒì´í”„ë¼ì¸
        multiplier: ê° ì´ë¯¸ì§€ë‹¹ ìƒì„±í•  ì¦ê°• ì´ë¯¸ì§€ ê°œìˆ˜
    """
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
    
    # ì›ë³¸ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    image_files = [f for f in input_path.iterdir() 
                  if f.suffix.lower() in image_extensions]
    
    print(f"ë°œê²¬ëœ ì´ë¯¸ì§€ íŒŒì¼: {len(image_files)}ê°œ")
    
    total_generated = 0
    
    for image_file in image_files:
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(str(image_file))
        if image is None:
            print(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_file}")
            continue
            
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
        original_output = output_path / f"original_{image_file.name}"
        cv2.imwrite(str(original_output), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
        
        # ì¦ê°• ì´ë¯¸ì§€ ìƒì„±
        for i in range(multiplier):
            try:
                # ë³€í™˜ ì ìš©
                augmented = transforms(image=image)
                augmented_image = augmented['image']
                
                # ì¦ê°•ëœ ì´ë¯¸ì§€ ì €ì¥
                stem = image_file.stem  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
                ext = image_file.suffix
                augmented_filename = f"{stem}_aug_{i+1}{ext}"
                augmented_output = output_path / augmented_filename
                
                cv2.imwrite(str(augmented_output), 
                           cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))
                total_generated += 1
                
            except Exception as e:
                print(f"ì¦ê°• ì‹¤íŒ¨ {image_file.name}_{i+1}: {e}")
    
    print(f"ì´ {total_generated}ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return total_generated

# ì¦ê°• íŒŒì´í”„ë¼ì¸ ì •ì˜
offline_transforms = A.Compose([
    A.OneOf([
        A.HorizontalFlip(p=1.0),
        A.VerticalFlip(p=1.0),
        A.Rotate(limit=30, p=1.0),
    ], p=0.8),
    
    A.OneOf([
        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),
        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0),
    ], p=0.6),
    
    A.OneOf([
        A.GaussNoise(var_limit=(10.0, 30.0), p=1.0),
        A.Blur(blur_limit=3, p=1.0),
    ], p=0.4),
])

# ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
def create_sample_images(sample_dir, num_images=3):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±"""
    sample_path = Path(sample_dir)
    sample_path.mkdir(parents=True, exist_ok=True)
    
    for i in range(num_images):
        # ëœë¤ ìƒ‰ìƒ ì´ë¯¸ì§€ ìƒì„±
        image = np.random.randint(0, 255, (200, 200, 3), dtype=np.uint8)
        # ì¤‘ì•™ì— ë„í˜• ì¶”ê°€
        cv2.rectangle(image, (50, 50), (150, 150), (255, 255, 255), -1)
        cv2.circle(image, (100, 100), 30, (0, 0, 0), -1)
        
        # ì´ë¯¸ì§€ ì €ì¥
        filename = sample_path / f"sample_{i+1}.jpg"
        cv2.imwrite(str(filename), image)
    
    print(f"{num_images}ê°œì˜ ìƒ˜í”Œ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
# create_sample_images("sample_images")
# offline_augmentation("sample_images", "augmented_images", offline_transforms, multiplier=3)
```

### ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬

ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì˜ ê²½ìš° **ë©€í‹°í”„ë¡œì„¸ì‹±**ì„ í™œìš©í•˜ì—¬ ì˜¤í”„ë¼ì¸ ì¦ê°• ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.

```python
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import albumentations as A

def process_single_image(args):
    """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    image_path, output_dir, transforms, multiplier = args
    
    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            return 0
        
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image_name = Path(image_path).stem
        
        generated = 0
        for i in range(multiplier):
            # ë³€í™˜ ì ìš©
            augmented = transforms(image=image)
            augmented_image = augmented['image']
            
            # ì €ì¥
            output_path = Path(output_dir) / f"{image_name}_aug_{i+1}.jpg"
            cv2.imwrite(str(output_path), cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))
            generated += 1
            
        return generated
        
    except Exception as e:
        print(f"ì²˜ë¦¬ ì‹¤íŒ¨ {image_path}: {e}")
        return 0

def parallel_offline_augmentation(input_dir, output_dir, transforms, multiplier=5, num_workers=None):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì´ìš©í•œ ì˜¤í”„ë¼ì¸ ì¦ê°•"""
    
    if num_workers is None:
        num_workers = mp.cpu_count()
    
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ìƒì„±
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}
    image_files = [str(f) for f in input_path.iterdir() 
                  if f.suffix.lower() in image_extensions]
    
    # ì‘ì—… ì¸ì ì¤€ë¹„
    args_list = [(img_path, str(output_path), transforms, multiplier) 
                 for img_path in image_files]
    
    print(f"{len(image_files)}ê°œ íŒŒì¼ì„ {num_workers}ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        results = list(executor.map(process_single_image, args_list))
    
    total_generated = sum(results)
    print(f"ì´ {total_generated}ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return total_generated

# ì‚¬ìš© ì˜ˆì‹œ
# parallel_offline_augmentation("large_dataset", "augmented_large", offline_transforms, multiplier=3, num_workers=4)
```

> Offline ì¦ê°•ì€ **ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê°€ ì œí•œì **ì´ê±°ë‚˜ **ë™ì¼í•œ ì¦ê°• ë°ì´í„°ë¥¼ ë°˜ë³µ ì‚¬ìš©**í•´ì•¼ í•˜ëŠ” í™˜ê²½ì—ì„œ íŠ¹íˆ ìœ ìš©í•˜ë‹¤. ë‹¤ë§Œ **ì €ì¥ ê³µê°„**ê³¼ **ë°ì´í„° ë‹¤ì–‘ì„±** ì‚¬ì´ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ê³ ë ¤í•´ì•¼ í•œë‹¤. {: .prompt-tip}

## âš¡ Online ì´ë¯¸ì§€ ì¦ê°•

**Online ì¦ê°•**ì€ ëª¨ë¸ í›ˆë ¨ ì¤‘ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë³€í™˜í•˜ëŠ” ë°©ì‹ì´ë‹¤. ë§¤ ì—í­ë§ˆë‹¤ ë‹¤ë¥¸ ë³€í™˜ì´ ì ìš©ë˜ì–´ **ë¬´í•œí•œ ë°ì´í„° ë‹¤ì–‘ì„±**ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤.

### Online ì¦ê°•ì˜ ì¥ë‹¨ì 

**ì¥ì **

- ì €ì¥ ê³µê°„ ì ˆì•½ (ì›ë³¸ ì´ë¯¸ì§€ë§Œ ë³´ê´€)
- ë¬´í•œí•œ ë°ì´í„° ë‹¤ì–‘ì„± (ë§¤ë²ˆ ë‹¤ë¥¸ ë³€í™˜)
- ë³€í™˜ íŒŒë¼ë¯¸í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥
- ì˜¤ë²„í”¼íŒ… ë°©ì§€ì— íš¨ê³¼ì 

**ë‹¨ì **

- í›ˆë ¨ ì¤‘ ì¶”ê°€ ì—°ì‚° ì‹œê°„ í•„ìš”
- ì¬í˜„ ê°€ëŠ¥ì„±ì´ ì œí•œì  (ë™ì¼í•œ ì¦ê°• ê²°ê³¼ ì–»ê¸° ì–´ë ¤ì›€)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ê°€ëŠ¥

```python
import albumentations as A
import torch
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
from pathlib import Path

class OnlineAugmentationDataset(Dataset):
    """Online ì¦ê°•ì„ ì§€ì›í•˜ëŠ” ì»¤ìŠ¤í…€ Dataset"""
    
    def __init__(self, image_paths, labels=None, transforms=None, image_size=(224, 224)):
        """
        Args:
            image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            transforms: Albumentations ë³€í™˜ íŒŒì´í”„ë¼ì¸
            image_size: ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸°
        """
        self.image_paths = image_paths
        self.labels = labels if labels is not None else [0] * len(image_paths)
        self.transforms = transforms
        self.image_size = image_size
        
        # ê¸°ë³¸ ë¦¬ì‚¬ì´ì¦ˆ ë³€í™˜ (ë³€í™˜ì´ ì—†ëŠ” ê²½ìš°)
        self.base_transform = A.Resize(height=image_size[0], width=image_size[1])
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        image_path = self.image_paths[idx]
        image = cv2.imread(str(image_path))
        
        if image is None:
            # ì—ëŸ¬ ì‹œ ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
            image = np.zeros((self.image_size[0], self.image_size[1], 3), dtype=np.uint8)
        else:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ë³€í™˜ ì ìš©
        if self.transforms is not None:
            augmented = self.transforms(image=image)
            image = augmented['image']
        else:
            # ê¸°ë³¸ ë¦¬ì‚¬ì´ì¦ˆë§Œ ì ìš©
            resized = self.base_transform(image=image)
            image = resized['image']
        
        # í…ì„œë¡œ ë³€í™˜ (HWC -> CHW, 0-1 ì •ê·œí™”)
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        
        return image, label

# ë‹¤ì–‘í•œ ê°•ë„ì˜ ì˜¨ë¼ì¸ ì¦ê°• íŒŒì´í”„ë¼ì¸
def get_train_transforms(image_size=(224, 224), augmentation_level='medium'):
    """
    í›ˆë ¨ìš© ì˜¨ë¼ì¸ ì¦ê°• ë³€í™˜ ìƒì„±
    
    Args:
        image_size: ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸°
        augmentation_level: ì¦ê°• ê°•ë„ ('light', 'medium', 'heavy')
    """
    
    base_transforms = [A.Resize(height=image_size[0], width=image_size[1])]
    
    if augmentation_level == 'light':
        aug_transforms = [
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
        ]
    elif augmentation_level == 'medium':
        aug_transforms = [
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),
            A.Rotate(limit=15, p=0.3),
            A.OneOf([
                A.GaussNoise(var_limit=(10.0, 30.0), p=1.0),
                A.Blur(blur_limit=3, p=1.0),
            ], p=0.2),
        ]
    elif augmentation_level == 'heavy':
        aug_transforms = [
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.2),
            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),
            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),
            A.OneOf([
                A.Rotate(limit=30, p=1.0),
                A.Affine(scale=(0.8, 1.2), translate_percent=0.1, rotate=(-15, 15), p=1.0),
            ], p=0.5),
            A.OneOf([
                A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),
                A.Blur(blur_limit=5, p=1.0),
                A.MotionBlur(blur_limit=5, p=1.0),
            ], p=0.4),
            A.RandomCrop(height=int(image_size[0]*0.9), width=int(image_size[1]*0.9), p=0.3),
        ]
    else:
        aug_transforms = []
    
    return A.Compose(base_transforms + aug_transforms)

def get_val_transforms(image_size=(224, 224)):
    """ê²€ì¦ìš© ë³€í™˜ (ì¦ê°• ì—†ìŒ)"""
    return A.Compose([
        A.Resize(height=image_size[0], width=image_size[1]),
    ])

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
def create_online_dataloaders(train_images, val_images, train_labels=None, val_labels=None,
                             batch_size=32, num_workers=4, augmentation_level='medium'):
    """ì˜¨ë¼ì¸ ì¦ê°•ì„ ì‚¬ìš©í•˜ëŠ” DataLoader ìƒì„±"""
    
    # ë³€í™˜ ì •ì˜
    train_transforms = get_train_transforms(augmentation_level=augmentation_level)
    val_transforms = get_val_transforms()
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = OnlineAugmentationDataset(
        image_paths=train_images,
        labels=train_labels,
        transforms=train_transforms
    )
    
    val_dataset = OnlineAugmentationDataset(
        image_paths=val_images,
        labels=val_labels,
        transforms=val_transforms
    )
    
    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader

# ì‚¬ìš© ì˜ˆì‹œ
sample_images = ["image1.jpg", "image2.jpg", "image3.jpg"] * 100  # ë”ë¯¸ ë°ì´í„°
sample_labels = [0, 1, 2] * 100

train_loader, val_loader = create_online_dataloaders(
    train_images=sample_images[:240],
    val_images=sample_images[240:],
    train_labels=sample_labels[:240],
    val_labels=sample_labels[240:],
    batch_size=16,
    augmentation_level='medium'
)

print(f"í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
print(f"ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")

# ì²« ë²ˆì§¸ ë°°ì¹˜ í™•ì¸
for batch_idx, (images, labels) in enumerate(train_loader):
    print(f"ë°°ì¹˜ {batch_idx}: ì´ë¯¸ì§€ shape {images.shape}, ë¼ë²¨ shape {labels.shape}")
    if batch_idx == 0:  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ í™•ì¸
        break

# í›ˆë ¨ ë°°ì¹˜ ìˆ˜: 15
# ê²€ì¦ ë°°ì¹˜ ìˆ˜: 4
# ë°°ì¹˜ 0: ì´ë¯¸ì§€ shape torch.Size([16, 3, 224, 224]), ë¼ë²¨ shape torch.Size([16])
```

### ë™ì  ì¦ê°• ê°•ë„ ì¡°ì ˆ

í›ˆë ¨ ì§„í–‰ì— ë”°ë¼ ì¦ê°• ê°•ë„ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” ê³ ê¸‰ ê¸°ë²•ë„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

```python
class AdaptiveAugmentationDataset(Dataset):
    """ì ì‘ì  ì¦ê°• ê°•ë„ ì¡°ì ˆì´ ê°€ëŠ¥í•œ Dataset"""
    
    def __init__(self, image_paths, labels=None, base_transforms=None, 
                 image_size=(224, 224)):
        self.image_paths = image_paths
        self.labels = labels if labels is not None else [0] * len(image_paths)
        self.base_transforms = base_transforms
        self.image_size = image_size
        self.augmentation_strength = 0.5  # ì´ˆê¸° ì¦ê°• ê°•ë„
        
    def set_augmentation_strength(self, strength):
        """ì¦ê°• ê°•ë„ ì„¤ì • (0.0 ~ 1.0)"""
        self.augmentation_strength = max(0.0, min(1.0, strength))
        
    def get_current_transforms(self):
        """í˜„ì¬ ì¦ê°• ê°•ë„ì— ë”°ë¥¸ ë³€í™˜ íŒŒì´í”„ë¼ì¸ ìƒì„±"""
        strength = self.augmentation_strength
        
        transforms = [A.Resize(height=self.image_size[0], width=self.image_size[1])]
        
        if strength > 0.1:
            transforms.append(A.HorizontalFlip(p=0.5 * strength))
            
        if strength > 0.3:
            transforms.append(A.RandomBrightnessContrast(
                brightness_limit=0.3 * strength, 
                contrast_limit=0.3 * strength, 
                p=0.7 * strength
            ))
            
        if strength > 0.5:
            transforms.append(A.OneOf([
                A.Rotate(limit=int(30 * strength), p=1.0),
                A.Affine(scale=(1-0.2*strength, 1+0.2*strength), p=1.0),
            ], p=0.5 * strength))
            
        if strength > 0.7:
            transforms.append(A.OneOf([
                A.GaussNoise(var_limit=(10.0, 50.0 * strength), p=1.0),
                A.Blur(blur_limit=int(5 * strength), p=1.0),
            ], p=0.4 * strength))
        
        return A.Compose(transforms)
    
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        image_path = self.image_paths[idx]
        image = cv2.imread(str(image_path))
        
        if image is None:
            image = np.zeros((self.image_size[0], self.image_size[1], 3), dtype=np.uint8)
        else:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # í˜„ì¬ ê°•ë„ì— ë§ëŠ” ë³€í™˜ ì ìš©
        current_transforms = self.get_current_transforms()
        augmented = current_transforms(image=image)
        image = augmented['image']
        
        # í…ì„œ ë³€í™˜
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        
        return image, label
    
    def __len__(self):
        return len(self.image_paths)

# ì‚¬ìš© ì˜ˆì‹œ: ì—í­ì— ë”°ë¥¸ ì¦ê°• ê°•ë„ ì¡°ì ˆ
def train_with_adaptive_augmentation(model, dataset, num_epochs=100):
    """ì ì‘ì  ì¦ê°•ì„ ì‚¬ìš©í•œ í›ˆë ¨ ì˜ˆì‹œ"""
    
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    for epoch in range(num_epochs):
        # ì¦ê°• ê°•ë„ ì¡°ì ˆ (ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ë§)
        strength = 0.5 + 0.4 * np.cos(np.pi * epoch / num_epochs)
        dataset.set_augmentation_strength(strength)
        
        print(f"Epoch {epoch+1}: ì¦ê°• ê°•ë„ {strength:.3f}")
        
        # ì‹¤ì œ í›ˆë ¨ ë£¨í”„ëŠ” ì—¬ê¸°ì„œ êµ¬í˜„
        # for batch_idx, (images, labels) in enumerate(dataloader):
        #     # í›ˆë ¨ ì½”ë“œ
        #     pass

# ì¦ê°• ê°•ë„ ë³€í™” ì‹œë®¬ë ˆì´ì…˜
epochs = np.arange(100)
strengths = [0.5 + 0.4 * np.cos(np.pi * epoch / 100) for epoch in epochs]

print("ì—í­ë³„ ì¦ê°• ê°•ë„ (ì²˜ìŒ 10ê°œ):")
for i in range(10):
    print(f"Epoch {i+1}: {strengths[i]:.3f}")
    
# ì—í­ë³„ ì¦ê°• ê°•ë„ (ì²˜ìŒ 10ê°œ):
# Epoch 1: 0.900
# Epoch 2: 0.892
# Epoch 3: 0.869
# Epoch 4: 0.832
# Epoch 5: 0.780
# Epoch 6: 0.714
# Epoch 7: 0.636
# Epoch 8: 0.547
# Epoch 9: 0.449
# Epoch 10: 0.345
```

> Online ì¦ê°•ì€ **ë¬´í•œí•œ ë°ì´í„° ë‹¤ì–‘ì„±**ì„ ì œê³µí•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. íŠ¹íˆ **ë°ì´í„°ê°€ ì œí•œì ì¸ í™˜ê²½**ì—ì„œ ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ëŠ” ë° ë§¤ìš° íš¨ê³¼ì ì´ë‹¤. {: .prompt-tip}

## ğŸ”¥ PyTorchì™€ì˜ ì—°ë™

AlbumentationsëŠ” PyTorchì™€ ë§¤ìš° ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©ëœë‹¤. **torch.utils.data.Dataset**, **DataLoader**ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.

### ê¸°ë³¸ PyTorch Dataset í†µí•©

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import numpy as np
from pathlib import Path

class AlbumentationsDataset(Dataset):
    """Albumentationsì™€ PyTorch í†µí•© Dataset"""
    
    def __init__(self, image_paths, labels, transforms=None, preprocessing=None):
        """
        Args:
            image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            transforms: Albumentations ë³€í™˜ íŒŒì´í”„ë¼ì¸
            preprocessing: ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì˜µì…˜)
        """
        self.image_paths = image_paths
        self.labels = labels
        self.transforms = transforms
        self.preprocessing = preprocessing
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(self.image_paths[idx])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        label = self.labels[idx]
        
        # ì „ì²˜ë¦¬ ì ìš© (ì˜µì…˜)
        if self.preprocessing:
            image = self.preprocessing(image)
        
        # Albumentations ë³€í™˜ ì ìš©
        if self.transforms:
            sample = self.transforms(image=image)
            image = sample['image']
        
        return image, label

# í‘œì¤€í™”ëœ ë³€í™˜ íŒŒì´í”„ë¼ì¸ (ToTensorV2 í¬í•¨)
def get_training_augmentation(image_size=224):
    """í›ˆë ¨ìš© ì¦ê°• íŒŒì´í”„ë¼ì¸ (PyTorch í…ì„œ ë³€í™˜ í¬í•¨)"""
    return A.Compose([
        A.Resize(image_size, image_size),
        A.HorizontalFlip(p=0.5),
        A.OneOf([
            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),
            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0),
            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),
        ], p=0.8),
        A.OneOf([
            A.Rotate(limit=30, p=1.0),
            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=1.0),
        ], p=0.5),
        A.OneOf([
            A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),
            A.Blur(blur_limit=3, p=1.0),
            A.MotionBlur(blur_limit=3, p=1.0),
        ], p=0.3),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet ì •ê·œí™”
        ToTensorV2(),  # ì¤‘ìš”: ë§ˆì§€ë§‰ì— í…ì„œ ë³€í™˜
    ])

def get_validation_augmentation(image_size=224):
    """ê²€ì¦ìš© ë³€í™˜ íŒŒì´í”„ë¼ì¸"""
    return A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])

# ë”ë¯¸ ë°ì´í„° ìƒì„±
def create_dummy_data(num_samples=1000, num_classes=10):
    """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ìƒì„±"""
    
    # ë”ë¯¸ ì´ë¯¸ì§€ ê²½ë¡œ (ì‹¤ì œë¡œëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŒ)
    image_paths = [f"dummy_image_{i}.jpg" for i in range(num_samples)]
    labels = np.random.randint(0, num_classes, num_samples)
    
    # ì‹¤ì œ ì´ë¯¸ì§€ ëŒ€ì‹  ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜
    def generate_dummy_image(path):
        # ëœë¤í•œ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        return np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
    
    return image_paths, labels, generate_dummy_image

# ì‹¤ì œ Dataset í´ë˜ìŠ¤ (ë”ë¯¸ ë°ì´í„°ìš©)
class DummyAlbumentationsDataset(Dataset):
    def __init__(self, num_samples, num_classes, transforms=None):
        self.num_samples = num_samples
        self.num_classes = num_classes
        self.transforms = transforms
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
        label = np.random.randint(0, self.num_classes)
        
        # ë³€í™˜ ì ìš©
        if self.transforms:
            sample = self.transforms(image=image)
            image = sample['image']
        
        return image, torch.tensor(label, dtype=torch.long)

# DataLoader ìƒì„± ë° ì‚¬ìš©
def create_pytorch_dataloaders(batch_size=32, num_workers=4, image_size=224):
    """PyTorch DataLoader ìƒì„±"""
    
    # ë³€í™˜ ì •ì˜
    train_transforms = get_training_augmentation(image_size)
    val_transforms = get_validation_augmentation(image_size)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = DummyAlbumentationsDataset(
        num_samples=800,
        num_classes=10,
        transforms=train_transforms
    )
    
    val_dataset = DummyAlbumentationsDataset(
        num_samples=200,
        num_classes=10,
        transforms=val_transforms
    )
    
    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available()
    )
    
    return train_loader, val_loader

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
train_loader, val_loader = create_pytorch_dataloaders(batch_size=16, image_size=224)

print(f"í›ˆë ¨ ë°ì´í„°ë¡œë”: {len(train_loader)} ë°°ì¹˜")
print(f"ê²€ì¦ ë°ì´í„°ë¡œë”: {len(val_loader)} ë°°ì¹˜")

# ì²« ë²ˆì§¸ ë°°ì¹˜ í™•ì¸
for images, labels in train_loader:
    print(f"ë°°ì¹˜ ì´ë¯¸ì§€ shape: {images.shape}")
    print(f"ë°°ì¹˜ ë¼ë²¨ shape: {labels.shape}")
    print(f"ì´ë¯¸ì§€ ë°ì´í„° íƒ€ì…: {images.dtype}")
    print(f"ì´ë¯¸ì§€ ê°’ ë²”ìœ„: [{images.min():.3f}, {images.max():.3f}]")
    break

# í›ˆë ¨ ë°ì´í„°ë¡œë”: 50 ë°°ì¹˜
# ê²€ì¦ ë°ì´í„°ë¡œë”: 13 ë°°ì¹˜
# ë°°ì¹˜ ì´ë¯¸ì§€ shape: torch.Size([16, 3, 224, 224])
# ë°°ì¹˜ ë¼ë²¨ shape: torch.Size([16])
# ì´ë¯¸ì§€ ë°ì´í„° íƒ€ì…: torch.float32
# ì´ë¯¸ì§€ ê°’ ë²”ìœ„: [-2.118, 2.640]
```

### í›ˆë ¨ ë£¨í”„ì™€ í†µí•©

ì‹¤ì œ ëª¨ë¸ í›ˆë ¨ì—ì„œ Albumentationsë¥¼ ì‚¬ìš©í•˜ëŠ” ì™„ì „í•œ ì˜ˆì‹œë‹¤.

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torchvision.models as models
from tqdm import tqdm

class SimpleClassificationModel(nn.Module):
    """ê°„ë‹¨í•œ ë¶„ë¥˜ ëª¨ë¸"""
    
    def __init__(self, num_classes=10, pretrained=True):
        super().__init__()
        self.backbone = models.resnet18(pretrained=pretrained)
        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)
    
    def forward(self, x):
        return self.backbone(x)

def train_epoch(model, dataloader, criterion, optimizer, device):
    """í•œ ì—í­ í›ˆë ¨"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    progress_bar = tqdm(dataloader, desc="Training")
    
    for images, labels in progress_bar:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        progress_bar.set_postfix({
            'Loss': f'{running_loss/(progress_bar.n+1):.3f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })
    
    return running_loss / len(dataloader), 100. * correct / total

def validate_epoch(model, dataloader, criterion, device):
    """í•œ ì—í­ ê²€ì¦"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Validation"):
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    return running_loss / len(dataloader), 100. * correct / total

def train_model_with_albumentations(num_epochs=10, batch_size=32):
    """Albumentationsë¥¼ ì‚¬ìš©í•œ ì™„ì „í•œ í›ˆë ¨ ì˜ˆì‹œ"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader, val_loader = create_pytorch_dataloaders(batch_size=batch_size)
    
    # ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    model = SimpleClassificationModel(num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)
    
    best_val_acc = 0.0
    
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print("-" * 40)
        
        # í›ˆë ¨
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        
        # ê²€ì¦
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        scheduler.step(val_loss)
        
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f"ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ëª¨ë¸ ì €ì¥ë¨")
    
    print(f"\ní›ˆë ¨ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.2f}%")

# ì‹¤ì œ í›ˆë ¨ ì‹¤í–‰ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬)
# train_model_with_albumentations(num_epochs=5, batch_size=16)

# ëŒ€ì‹  êµ¬ì¡°ë§Œ í™•ì¸
print("ëª¨ë¸ í›ˆë ¨ êµ¬ì¡° í™•ì¸ ì™„ë£Œ")
print("ì‹¤ì œ í›ˆë ¨ì„ ì›í•˜ë©´ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”")
```

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ë¡œë”©

ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•  ë•Œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ê¸°ë²•ë“¤ì´ë‹¤.

```python
class MemoryEfficientDataset(Dataset):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹"""
    
    def __init__(self, image_paths, labels, transforms=None, 
                 cache_size=1000, preload_factor=0.1):
        """
        Args:
            image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            transforms: ë³€í™˜ íŒŒì´í”„ë¼ì¸
            cache_size: ìºì‹œí•  ì´ë¯¸ì§€ ìˆ˜
            preload_factor: ì‚¬ì „ ë¡œë“œí•  ë¹„ìœ¨ (0~1)
        """
        self.image_paths = image_paths
        self.labels = labels
        self.transforms = transforms
        self.cache_size = cache_size
        
        # ê°„ë‹¨í•œ LRU ìºì‹œ êµ¬í˜„
        self.cache = {}
        self.cache_order = []
        
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ ì‚¬ì „ ë¡œë“œ
        if preload_factor > 0:
            self._preload_images(preload_factor)
    
    def _preload_images(self, factor):
        """ì´ë¯¸ì§€ ì‚¬ì „ ë¡œë“œ"""
        num_preload = int(len(self.image_paths) * factor)
        indices = np.random.choice(len(self.image_paths), num_preload, replace=False)
        
        print(f"ì´ë¯¸ì§€ {num_preload}ê°œ ì‚¬ì „ ë¡œë“œ ì¤‘...")
        for idx in tqdm(indices):
            self._load_image(idx)
    
    def _load_image(self, idx):
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ìºì‹œ ê´€ë¦¬"""
        if idx in self.cache:
            # ìºì‹œ íˆíŠ¸: ìˆœì„œ ì—…ë°ì´íŠ¸
            self.cache_order.remove(idx)
            self.cache_order.append(idx)
            return self.cache[idx]
        
        # ìºì‹œ ë¯¸ìŠ¤: ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(self.image_paths[idx])
        if image is not None:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image = np.zeros((224, 224, 3), dtype=np.uint8)
        
        # ìºì‹œ ì €ì¥
        if len(self.cache) >= self.cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_idx = self.cache_order.pop(0)
            del self.cache[oldest_idx]
        
        self.cache[idx] = image
        self.cache_order.append(idx)
        
        return image
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # ìºì‹œì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ
        image = self._load_image(idx)
        label = self.labels[idx]
        
        # ë³€í™˜ ì ìš©
        if self.transforms:
            sample = self.transforms(image=image)
            image = sample['image']
        
        return image, torch.tensor(label, dtype=torch.long)
    
    def get_cache_stats(self):
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return {
            'cache_size': len(self.cache),
            'cache_limit': self.cache_size,
            'cache_hit_ratio': len(self.cache) / len(self.image_paths)
        }

# ì‚¬ìš© ì˜ˆì‹œ
def test_memory_efficient_loading():
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”© í…ŒìŠ¤íŠ¸"""
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    image_paths = [f"image_{i}.jpg" for i in range(10000)]
    labels = np.random.randint(0, 10, 10000)
    
    # ë³€í™˜ ì •ì˜
    transforms = get_training_augmentation(224)
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„°ì…‹ ìƒì„±
    dataset = MemoryEfficientDataset(
        image_paths=image_paths[:100],  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 100ê°œë§Œ
        labels=labels[:100],
        transforms=transforms,
        cache_size=20,
        preload_factor=0.2
    )
    
    # DataLoader ìƒì„±
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)
    
    # ëª‡ ë°°ì¹˜ ì²˜ë¦¬í•´ë³´ê¸°
    for batch_idx, (images, labels) in enumerate(dataloader):
        print(f"ë°°ì¹˜ {batch_idx}: {images.shape}")
        if batch_idx >= 2:  # 3ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
            break
    
    # ìºì‹œ í†µê³„ í™•ì¸
    stats = dataset.get_cache_stats()
    print(f"ìºì‹œ í†µê³„: {stats}")

# test_memory_efficient_loading()
print("ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ë¡œë”© êµ¬ì¡° í™•ì¸ ì™„ë£Œ")
```

> PyTorchì™€ Albumentationsì˜ í†µí•©ì€ **ToTensorV2**ë¥¼ ë§ˆì§€ë§‰ ë³€í™˜ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤. ì´ë¥¼ í†µí•´ NumPy ë°°ì—´ì„ PyTorch í…ì„œë¡œ ìë™ ë³€í™˜í•˜ê³ , **ì±„ë„ ìˆœì„œë¥¼ HWCì—ì„œ CHWë¡œ** ì˜¬ë°”ë¥´ê²Œ ë³€ê²½í•  ìˆ˜ ìˆë‹¤. {: .prompt-tip}

## ğŸ“¦ ì–´ë…¸í…Œì´ì…˜ ì§€ì›: Bbox, Keypoint, Mask

Albumentationsì˜ ê°€ì¥ ê°•ë ¥í•œ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” **ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì–´ë…¸í…Œì´ì…˜ë„ ë™ì‹œì— ë³€í™˜**í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. ê°ì²´ íƒì§€, ì´ë¯¸ì§€ ë¶„í• , í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë“± ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ íƒœìŠ¤í¬ì—ì„œ í•„ìˆ˜ì ì¸ ê¸°ëŠ¥ì´ë‹¤.

### Bounding Box ë³€í™˜

ê°ì²´ íƒì§€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì´ë¯¸ì§€ ë³€í™˜ê³¼ í•¨ê»˜ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

```python
import albumentations as A
import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# ë°”ìš´ë”© ë°•ìŠ¤ í˜•ì‹ ì„¤ëª…
# - 'pascal_voc': [x_min, y_min, x_max, y_max] (ì ˆëŒ€ ì¢Œí‘œ)
# - 'albumentations': [x_min, y_min, x_max, y_max] (ì •ê·œí™”ëœ ì¢Œí‘œ 0~1)
# - 'coco': [x_min, y_min, width, height] (ì ˆëŒ€ ì¢Œí‘œ)
# - 'yolo': [x_center, y_center, width, height] (ì •ê·œí™”ëœ ì¢Œí‘œ 0~1)

def create_sample_image_with_boxes():
    """ë°”ìš´ë”© ë°•ìŠ¤ê°€ ìˆëŠ” ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±"""
    # 300x300 ì´ë¯¸ì§€ ìƒì„±
    image = np.zeros((300, 300, 3), dtype=np.uint8)
    
    # ë°°ê²½ìƒ‰ ì„¤ì •
    image.fill(200)
    
    # ê°ì²´ë“¤ ê·¸ë¦¬ê¸°
    # ê°ì²´ 1: ë¹¨ê°„ ì‚¬ê°í˜•
    cv2.rectangle(image, (50, 50), (150, 150), (255, 0, 0), -1)
    
    # ê°ì²´ 2: íŒŒë€ ì›
    cv2.circle(image, (200, 200), 40, (0, 0, 255), -1)
    
    # ê°ì²´ 3: ì´ˆë¡ ì‚¼ê°í˜•
    points = np.array([[80, 250], [120, 200], [160, 250]], np.int32)
    cv2.fillPoly(image, [points], (0, 255, 0))
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ì •ì˜ (pascal_voc í˜•ì‹)
    bboxes = [
        [50, 50, 150, 150],    # ë¹¨ê°„ ì‚¬ê°í˜•
        [160, 160, 240, 240],  # íŒŒë€ ì›
        [80, 200, 160, 250],   # ì´ˆë¡ ì‚¼ê°í˜•
    ]
    
    # í´ë˜ìŠ¤ ë¼ë²¨
    class_labels = ['square', 'circle', 'triangle']
    
    return image, bboxes, class_labels

def visualize_bboxes(image, bboxes, class_labels, title="Image with Bounding Boxes"):
    """ë°”ìš´ë”© ë°•ìŠ¤ ì‹œê°í™”"""
    fig, ax = plt.subplots(1, 1, figsize=(8, 8))
    ax.imshow(image)
    ax.set_title(title)
    
    colors = ['red', 'blue', 'green', 'orange', 'purple']
    
    for i, (bbox, label) in enumerate(zip(bboxes, class_labels)):
        x_min, y_min, x_max, y_max = bbox
        width = x_max - x_min
        height = y_max - y_min
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        rect = patches.Rectangle(
            (x_min, y_min), width, height,
            linewidth=2, edgecolor=colors[i % len(colors)], facecolor='none'
        )
        ax.add_patch(rect)
        
        # ë¼ë²¨ í…ìŠ¤íŠ¸
        ax.text(x_min, y_min-5, f'{label}', 
                color=colors[i % len(colors)], fontsize=12, weight='bold')
    
    ax.set_xlim(0, image.shape[1])
    ax.set_ylim(image.shape[0], 0)
    plt.tight_layout()
    plt.show()

# ë°”ìš´ë”© ë°•ìŠ¤ ë³€í™˜ íŒŒì´í”„ë¼ì¸
bbox_transforms = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.Rotate(limit=30, p=0.5),
    A.RandomScale(scale_limit=0.2, p=0.5),
    A.Resize(height=256, width=256),  # í¬ê¸° ì¡°ì •ë„ ë°”ìš´ë”© ë°•ìŠ¤ì— ì ìš©
], bbox_params=A.BboxParams(
    format='pascal_voc',  # ë°”ìš´ë”© ë°•ìŠ¤ í˜•ì‹
    label_fields=['class_labels'],  # í´ë˜ìŠ¤ ë¼ë²¨ í•„ë“œëª…
    min_area=0,  # ìµœì†Œ ë©´ì  (ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ì œê±°)
    min_visibility=0.1  # ìµœì†Œ ê°€ì‹œì„± (ì˜ë¦° ë¹„ìœ¨ì´ ì´ë³´ë‹¤ í¬ë©´ ì œê±°)
))

# ìƒ˜í”Œ ì´ë¯¸ì§€ì™€ ë°”ìš´ë”© ë°•ìŠ¤ ìƒì„±
image, bboxes, class_labels = create_sample_image_with_boxes()

print("ì›ë³¸ ì´ë¯¸ì§€ ì •ë³´:")
print(f"ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
print(f"ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: {len(bboxes)}")
print(f"ë°”ìš´ë”© ë°•ìŠ¤: {bboxes}")
print(f"í´ë˜ìŠ¤ ë¼ë²¨: {class_labels}")

# ë³€í™˜ ì ìš©
transformed = bbox_transforms(
    image=image,
    bboxes=bboxes,
    class_labels=class_labels
)

transformed_image = transformed['image']
transformed_bboxes = transformed['bboxes']
transformed_labels = transformed['class_labels']

print("\në³€í™˜ í›„ ì •ë³´:")
print(f"ì´ë¯¸ì§€ í¬ê¸°: {transformed_image.shape}")
print(f"ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: {len(transformed_bboxes)}")
print(f"ë°”ìš´ë”© ë°•ìŠ¤: {transformed_bboxes}")
print(f"í´ë˜ìŠ¤ ë¼ë²¨: {transformed_labels}")

# ì›ë³¸ ì´ë¯¸ì§€ ì •ë³´:
# ì´ë¯¸ì§€ í¬ê¸°: (300, 300, 3)
# ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: 3
# ë°”ìš´ë”© ë°•ìŠ¤: [[50, 50, 150, 150], [160, 160, 240, 240], [80, 200, 160, 250]]
# í´ë˜ìŠ¤ ë¼ë²¨: ['square', 'circle', 'triangle']

# ë³€í™˜ í›„ ì •ë³´:
# ì´ë¯¸ì§€ í¬ê¸°: (256, 256, 3)
# ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: 3
# ë°”ìš´ë”© ë°•ìŠ¤: [[42.66666666666667, 42.66666666666667, 128.0, 128.0], [136.53333333333333, 136.53333333333333, 204.8, 204.8], [68.26666666666667, 170.66666666666666, 136.53333333333333, 213.33333333333334]]
# í´ë˜ìŠ¤ ë¼ë²¨: ['square', 'circle', 'triangle']
```

### ë‹¤ì–‘í•œ ë°”ìš´ë”© ë°•ìŠ¤ í˜•ì‹ ì§€ì›

```python
def convert_bbox_format(bboxes, source_format, target_format, image_height, image_width):
    """ë°”ìš´ë”© ë°•ìŠ¤ í˜•ì‹ ë³€í™˜ ìœ í‹¸ë¦¬í‹°"""
    
    converted_bboxes = []
    
    for bbox in bboxes:
        if source_format == 'pascal_voc' and target_format == 'yolo':
            x_min, y_min, x_max, y_max = bbox
            x_center = (x_min + x_max) / 2 / image_width
            y_center = (y_min + y_max) / 2 / image_height
            width = (x_max - x_min) / image_width
            height = (y_max - y_min) / image_height
            converted_bboxes.append([x_center, y_center, width, height])
            
        elif source_format == 'yolo' and target_format == 'pascal_voc':
            x_center, y_center, width, height = bbox
            x_min = (x_center - width/2) * image_width
            y_min = (y_center - height/2) * image_height
            x_max = (x_center + width/2) * image_width
            y_max = (y_center + height/2) * image_height
            converted_bboxes.append([x_min, y_min, x_max, y_max])
            
        elif source_format == 'pascal_voc' and target_format == 'coco':
            x_min, y_min, x_max, y_max = bbox
            width = x_max - x_min
            height = y_max - y_min
            converted_bboxes.append([x_min, y_min, width, height])
            
        else:
            converted_bboxes.append(bbox)  # ë™ì¼ í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    return converted_bboxes

# YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì„œ ì‚¬ìš©í•˜ê¸°
original_bboxes = [[50, 50, 150, 150], [160, 160, 240, 240], [80, 200, 160, 250]]
yolo_bboxes = convert_bbox_format(
    original_bboxes, 'pascal_voc', 'yolo', 300, 300
)

print("YOLO í˜•ì‹ ë°”ìš´ë”© ë°•ìŠ¤:")
for i, bbox in enumerate(yolo_bboxes):
    print(f"ê°ì²´ {i+1}: x_center={bbox[0]:.3f}, y_center={bbox[1]:.3f}, "
          f"width={bbox[2]:.3f}, height={bbox[3]:.3f}")

# YOLO í˜•ì‹ìš© ë³€í™˜ íŒŒì´í”„ë¼ì¸
yolo_transforms = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=15, p=0.5),
    A.RandomScale(scale_limit=0.1, p=0.5),
], bbox_params=A.BboxParams(
    format='yolo',
    label_fields=['class_labels']
))

# YOLO í˜•ì‹ ë°”ìš´ë”© ë°•ìŠ¤: 
# ê°ì²´ 1: x_center=0.333, y_center=0.333, width=0.333, height=0.333
# ê°ì²´ 2: x_center=0.667, y_center=0.667, width=0.267, height=0.267
# ê°ì²´ 3: x_center=0.400, y_center=0.750, width=0.267, height=0.167
```

### Keypoint ë³€í™˜

í¬ì¦ˆ ì¶”ì •, ì–¼êµ´ ëœë“œë§ˆí¬ ê²€ì¶œ ë“±ì—ì„œ ì‚¬ìš©ë˜ëŠ” í‚¤í¬ì¸íŠ¸ë¥¼ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.

```python
def create_sample_image_with_keypoints():
    """í‚¤í¬ì¸íŠ¸ê°€ ìˆëŠ” ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±"""
    image = np.zeros((300, 300, 3), dtype=np.uint8)
    image.fill(200)
    
    # ì‚¬ëŒ í˜•íƒœ ê·¸ë¦¬ê¸° (ê°„ë‹¨í•œ ìŠ¤í‹± í”¼ê²¨)
    # ë¨¸ë¦¬
    cv2.circle(image, (150, 80), 30, (255, 200, 200), -1)
    
    # ëª¸í†µ
    cv2.line(image, (150, 110), (150, 200), (100, 100, 100), 5)
    
    # íŒ”
    cv2.line(image, (150, 140), (120, 170), (100, 100, 100), 3)  # ì™¼íŒ”
    cv2.line(image, (150, 140), (180, 170), (100, 100, 100), 3)  # ì˜¤ë¥¸íŒ”
    
    # ë‹¤ë¦¬
    cv2.line(image, (150, 200), (130, 250), (100, 100, 100), 3)  # ì™¼ë‹¤ë¦¬
    cv2.line(image, (150, 200), (170, 250), (100, 100, 100), 3)  # ì˜¤ë¥¸ë‹¤ë¦¬
    
    # í‚¤í¬ì¸íŠ¸ ì •ì˜ (x, y ì¢Œí‘œ)
    keypoints = [
        (150, 80),   # ë¨¸ë¦¬
        (150, 140),  # ì–´ê¹¨ ì¤‘ì•™
        (120, 170),  # ì™¼ì†
        (180, 170),  # ì˜¤ë¥¸ì†
        (150, 200),  # ì—‰ë©ì´
        (130, 250),  # ì™¼ë°œ
        (170, 250),  # ì˜¤ë¥¸ë°œ
    ]
    
    keypoint_labels = ['head', 'shoulder', 'left_hand', 'right_hand', 'hip', 'left_foot', 'right_foot']
    
    return image, keypoints, keypoint_labels

def visualize_keypoints(image, keypoints, labels, title="Image with Keypoints"):
    """í‚¤í¬ì¸íŠ¸ ì‹œê°í™”"""
    fig, ax = plt.subplots(1, 1, figsize=(8, 8))
    ax.imshow(image)
    ax.set_title(title)
    
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'cyan']
    
    for i, ((x, y), label) in enumerate(zip(keypoints, labels)):
        # í‚¤í¬ì¸íŠ¸ ì  ê·¸ë¦¬ê¸°
        ax.scatter(x, y, c=colors[i % len(colors)], s=100, marker='o')
        # ë¼ë²¨ í…ìŠ¤íŠ¸
        ax.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points',
                   color=colors[i % len(colors)], fontsize=10, weight='bold')
    
    # ì—°ê²°ì„  ê·¸ë¦¬ê¸° (ê°„ë‹¨í•œ ìŠ¤ì¼ˆë ˆí†¤)
    connections = [(0, 1), (1, 2), (1, 3), (1, 4), (4, 5), (4, 6)]
    for start_idx, end_idx in connections:
        if start_idx < len(keypoints) and end_idx < len(keypoints):
            start_point = keypoints[start_idx]
            end_point = keypoints[end_idx]
            ax.plot([start_point[0], end_point[0]], 
                   [start_point[1], end_point[1]], 
                   'k-', alpha=0.5, linewidth=2)
    
    plt.tight_layout()
    plt.show()

# í‚¤í¬ì¸íŠ¸ ë³€í™˜ íŒŒì´í”„ë¼ì¸
keypoint_transforms = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.Rotate(limit=20, p=0.5),
    A.RandomScale(scale_limit=0.1, p=0.5),
    A.Resize(height=256, width=256),
], keypoint_params=A.KeypointParams(
    format='xy',  # í‚¤í¬ì¸íŠ¸ í˜•ì‹: 'xy', 'yx', 'xya', 'xys', 'xyas', 'xysa'
    label_fields=['keypoint_labels'],  # í‚¤í¬ì¸íŠ¸ ë¼ë²¨ í•„ë“œëª…
    remove_invisible=True,  # ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°„ í‚¤í¬ì¸íŠ¸ ì œê±°
))

# ìƒ˜í”Œ ì´ë¯¸ì§€ì™€ í‚¤í¬ì¸íŠ¸ ìƒì„±
image, keypoints, keypoint_labels = create_sample_image_with_keypoints()

print("ì›ë³¸ í‚¤í¬ì¸íŠ¸ ì •ë³´:")
print(f"ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
print(f"í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: {len(keypoints)}")
for i, (kp, label) in enumerate(zip(keypoints, keypoint_labels)):
    print(f"  {label}: {kp}")

# ë³€í™˜ ì ìš©
transformed = keypoint_transforms(
    image=image,
    keypoints=keypoints,
    keypoint_labels=keypoint_labels
)

transformed_image = transformed['image']
transformed_keypoints = transformed['keypoints']
transformed_labels = transformed['keypoint_labels']

print("\në³€í™˜ í›„ í‚¤í¬ì¸íŠ¸ ì •ë³´:")
print(f"ì´ë¯¸ì§€ í¬ê¸°: {transformed_image.shape}")
print(f"í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: {len(transformed_keypoints)}")
for i, (kp, label) in enumerate(zip(transformed_keypoints, transformed_labels)):
    print(f"  {label}: ({kp[0]:.1f}, {kp[1]:.1f})")

# ì›ë³¸ í‚¤í¬ì¸íŠ¸ ì •ë³´:
# ì´ë¯¸ì§€ í¬ê¸°: (300, 300, 3)
# í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: 7
#   head: (150, 80)
#   shoulder: (150, 140)
#   left_hand: (120, 170)
#   right_hand: (180, 170)
#   hip: (150, 200)
#   left_foot: (130, 250)
#   right_foot: (170, 250)

# ë³€í™˜ í›„ í‚¤í¬ì¸íŠ¸ ì •ë³´:
# ì´ë¯¸ì§€ í¬ê¸°: (256, 256, 3)
# í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: 7
#   head: (128.0, 68.3)
#   shoulder: (128.0, 119.5)
#   left_hand: (102.4, 145.1)
#   right_hand: (153.6, 145.1)
#   hip: (128.0, 170.7)
#   left_foot: (111.0, 213.3)
#   right_foot: (145.1, 213.3)
```

### Mask ë³€í™˜

ì´ë¯¸ì§€ ë¶„í• (Segmentation)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë§ˆìŠ¤í¬ë¥¼ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.

```python
def create_sample_image_with_mask():
    """ë§ˆìŠ¤í¬ê°€ ìˆëŠ” ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±"""
    # ì›ë³¸ ì´ë¯¸ì§€
    image = np.zeros((200, 200, 3), dtype=np.uint8)
    image.fill(150)
    
    # ê°ì²´ë“¤ ê·¸ë¦¬ê¸°
    cv2.rectangle(image, (50, 50), (100, 100), (255, 0, 0), -1)  # ë¹¨ê°„ ì‚¬ê°í˜•
    cv2.circle(image, (150, 150), 30, (0, 255, 0), -1)          # ì´ˆë¡ ì›
    
    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
    mask = np.zeros((200, 200), dtype=np.uint8)
    
    # í´ë˜ìŠ¤ 1: ë¹¨ê°„ ì‚¬ê°í˜•
    cv2.rectangle(mask, (50, 50), (100, 100), 1, -1)
    
    # í´ë˜ìŠ¤ 2: ì´ˆë¡ ì›
    cv2.circle(mask, (150, 150), 30, 2, -1)
    
    return image, mask

def visualize_mask(image, mask, title="Image with Segmentation Mask"):
    """ë§ˆìŠ¤í¬ ì‹œê°í™”"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # ì›ë³¸ ì´ë¯¸ì§€
    axes[0].imshow(image)
    axes[0].set_title("Original Image")
    axes[0].axis('off')
    
    # ë§ˆìŠ¤í¬
    axes[1].imshow(mask, cmap='tab10', vmin=0, vmax=10)
    axes[1].set_title("Segmentation Mask")
    axes[1].axis('off')
    
    # ì˜¤ë²„ë ˆì´
    overlay = image.copy()
    colored_mask = np.zeros_like(image)
    colored_mask[mask == 1] = [255, 0, 0]  # í´ë˜ìŠ¤ 1ì€ ë¹¨ê°•
    colored_mask[mask == 2] = [0, 255, 0]  # í´ë˜ìŠ¤ 2ëŠ” ì´ˆë¡
    
    overlay = cv2.addWeighted(overlay, 0.7, colored_mask, 0.3, 0)
    axes[2].imshow(overlay)
    axes[2].set_title("Overlay")
    axes[2].axis('off')
    
    plt.tight_layout()
    plt.show()

# ë§ˆìŠ¤í¬ ë³€í™˜ íŒŒì´í”„ë¼ì¸
mask_transforms = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.3),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.Rotate(limit=30, p=0.5),
    A.RandomScale(scale_limit=0.2, p=0.5),
    A.ElasticTransform(alpha=50, sigma=5, alpha_affine=10, p=0.3),  # íƒ„ì„± ë³€í˜•
    A.Resize(height=128, width=128),
])

# ìƒ˜í”Œ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ ìƒì„±
image, mask = create_sample_image_with_mask()

print("ì›ë³¸ ë§ˆìŠ¤í¬ ì •ë³´:")
print(f"ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
print(f"ë§ˆìŠ¤í¬ í¬ê¸°: {mask.shape}")
print(f"ë§ˆìŠ¤í¬ í´ë˜ìŠ¤: {np.unique(mask)}")
print(f"ê° í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜:")
for class_id in np.unique(mask):
    pixel_count = np.sum(mask == class_id)
    print(f"  í´ë˜ìŠ¤ {class_id}: {pixel_count} í”½ì…€")

# ë³€í™˜ ì ìš©
transformed = mask_transforms(image=image, mask=mask)
transformed_image = transformed['image']
transformed_mask = transformed['mask']

print("\në³€í™˜ í›„ ë§ˆìŠ¤í¬ ì •ë³´:")
print(f"ì´ë¯¸ì§€ í¬ê¸°: {transformed_image.shape}")
print(f"ë§ˆìŠ¤í¬ í¬ê¸°: {transformed_mask.shape}")
print(f"ë§ˆìŠ¤í¬ í´ë˜ìŠ¤: {np.unique(transformed_mask)}")
print(f"ê° í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜:")
for class_id in np.unique(transformed_mask):
    pixel_count = np.sum(transformed_mask == class_id)
    print(f"  í´ë˜ìŠ¤ {class_id}: {pixel_count} í”½ì…€")

# ì›ë³¸ ë§ˆìŠ¤í¬ ì •ë³´:
# ì´ë¯¸ì§€ í¬ê¸°: (200, 200, 3)
# ë§ˆìŠ¤í¬ í¬ê¸°: (200, 200)
# ë§ˆìŠ¤í¬ í´ë˜ìŠ¤: [0 1 2]
# ê° í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜:
#   í´ë˜ìŠ¤ 0: 35673 í”½ì…€
#   í´ë˜ìŠ¤ 1: 2500 í”½ì…€
#   í´ë˜ìŠ¤ 2: 1827 í”½ì…€

# ë³€í™˜ í›„ ë§ˆìŠ¤í¬ ì •ë³´:
# ì´ë¯¸ì§€ í¬ê¸°: (128, 128, 3)
# ë§ˆìŠ¤í¬ í¬ê¸°: (128, 128)
# ë§ˆìŠ¤í¬ í´ë˜ìŠ¤: [0 1 2]
# ê° í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜:
#   í´ë˜ìŠ¤ 0: 14533 í”½ì…€
#   í´ë˜ìŠ¤ 1: 1491 í”½ì…€
#   í´ë˜ìŠ¤ 2: 360 í”½ì…€
```

### ë³µí•© ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬

ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì–´ë…¸í…Œì´ì…˜ì„ ë™ì‹œì— ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.

```python
class MultiAnnotationDataset(Dataset):
    """ë³µí•© ì–´ë…¸í…Œì´ì…˜ì„ ì§€ì›í•˜ëŠ” ë°ì´í„°ì…‹"""
    
    def __init__(self, data_list, transforms=None):
        """
        Args:
            data_list: ê° í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ì¸ ë¦¬ìŠ¤íŠ¸
                      {'image_path', 'bboxes', 'keypoints', 'mask_path', 'labels', ...}
            transforms: Albumentations ë³€í™˜ íŒŒì´í”„ë¼ì¸
        """
        self.data_list = data_list
        self.transforms = transforms
    
    def __len__(self):
        return len(self.data_list)
    
    def __getitem__(self, idx):
        data = self.data_list[idx]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(data['image_path'])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ë§ˆìŠ¤í¬ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        mask = None
        if 'mask_path' in data and data['mask_path']:
            mask = cv2.imread(data['mask_path'], cv2.IMREAD_GRAYSCALE)
        
        # ë³€í™˜ ì ìš©
        transform_input = {'image': image}
        
        if mask is not None:
            transform_input['mask'] = mask
        
        if 'bboxes' in data:
            transform_input['bboxes'] = data['bboxes']
            
        if 'keypoints' in data:
            transform_input['keypoints'] = data['keypoints']
            
        if 'bbox_labels' in data:
            transform_input['bbox_labels'] = data['bbox_labels']
            
        if 'keypoint_labels' in data:
            transform_input['keypoint_labels'] = data['keypoint_labels']
        
        if self.transforms:
            transformed = self.transforms(**transform_input)
        else:
            transformed = transform_input
        
        return transformed

# ë³µí•© ì–´ë…¸í…Œì´ì…˜ìš© ë³€í™˜ íŒŒì´í”„ë¼ì¸
multi_annotation_transforms = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.Rotate(limit=15, p=0.5),
    A.Resize(height=256, width=256),
], 
bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bbox_labels']),
keypoint_params=A.KeypointParams(format='xy', label_fields=['keypoint_labels'])
)

# ë”ë¯¸ ë°ì´í„° ìƒì„±
def create_multi_annotation_data():
    """ë³µí•© ì–´ë…¸í…Œì´ì…˜ ë”ë¯¸ ë°ì´í„° ìƒì„±"""
    data_list = []
    
    for i in range(5):
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        image = np.random.randint(0, 255, (300, 300, 3), dtype=np.uint8)
        
        # ë”ë¯¸ ë°”ìš´ë”© ë°•ìŠ¤
        bboxes = [
            [50 + i*10, 50 + i*10, 150 + i*10, 150 + i*10],
            [200 - i*5, 200 - i*5, 280 - i*5, 280 - i*5]
        ]
        bbox_labels = ['person', 'car']
        
        # ë”ë¯¸ í‚¤í¬ì¸íŠ¸
        keypoints = [
            (100 + i*5, 100 + i*5),
            (150 + i*5, 120 + i*5),
            (200 + i*5, 200 + i*5)
        ]
        keypoint_labels = ['nose', 'left_eye', 'right_eye']
        
        # ë”ë¯¸ ë§ˆìŠ¤í¬
        mask = np.zeros((300, 300), dtype=np.uint8)
        cv2.rectangle(mask, (50 + i*10, 50 + i*10), (150 + i*10, 150 + i*10), 1, -1)
        
        data_item = {
            'image': image,
            'mask': mask,
            'bboxes': bboxes,
            'bbox_labels': bbox_labels,
            'keypoints': keypoints,
            'keypoint_labels': keypoint_labels
        }
        
        data_list.append(data_item)
    
    return data_list

# ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
dummy_data = create_multi_annotation_data()

print("ë³µí•© ì–´ë…¸í…Œì´ì…˜ í…ŒìŠ¤íŠ¸:")
for i, data in enumerate(dummy_data[:2]):  # ì²˜ìŒ 2ê°œë§Œ í…ŒìŠ¤íŠ¸
    print(f"\në°ì´í„° {i+1}:")
    print(f"  ì´ë¯¸ì§€ í¬ê¸°: {data['image'].shape}")
    print(f"  ë§ˆìŠ¤í¬ í¬ê¸°: {data['mask'].shape}")
    print(f"  ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: {len(data['bboxes'])}")
    print(f"  í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: {len(data['keypoints'])}")
    
    # ë³€í™˜ ì ìš© í…ŒìŠ¤íŠ¸
    transformed = multi_annotation_transforms(
        image=data['image'],
        mask=data['mask'],
        bboxes=data['bboxes'],
        bbox_labels=data['bbox_labels'],
        keypoints=data['keypoints'],
        keypoint_labels=data['keypoint_labels']
    )
    
    print(f"  ë³€í™˜ í›„ ì´ë¯¸ì§€ í¬ê¸°: {transformed['image'].shape}")
    print(f"  ë³€í™˜ í›„ ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: {len(transformed['bboxes'])}")
    print(f"  ë³€í™˜ í›„ í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: {len(transformed['keypoints'])}")

# ë³µí•© ì–´ë…¸í…Œì´ì…˜ í…ŒìŠ¤íŠ¸:

# ë°ì´í„° 1:
#   ì´ë¯¸ì§€ í¬ê¸°: (300, 300, 3)
#   ë§ˆìŠ¤í¬ í¬ê¸°: (300, 300)
#   ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: 2
#   í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: 3
#   ë³€í™˜ í›„ ì´ë¯¸ì§€ í¬ê¸°: (256, 256, 3)
#   ë³€í™˜ í›„ ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: 2
#   ë³€í™˜ í›„ í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: 3

# ë°ì´í„° 2:
#   ì´ë¯¸ì§€ í¬ê¸°: (300, 300, 3)
#   ë§ˆìŠ¤í¬ í¬ê¸°: (300, 300)
#   ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: 2
#   í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: 3
#   ë³€í™˜ í›„ ì´ë¯¸ì§€ í¬ê¸°: (256, 256, 3)
#   ë³€í™˜ í›„ ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜: 2
#   ë³€í™˜ í›„ í‚¤í¬ì¸íŠ¸ ê°œìˆ˜: 3
```

> Albumentationsì˜ ì–´ë…¸í…Œì´ì…˜ ì§€ì›ì€ **ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ì˜ ì¼ê´€ì„±**ì„ ìë™ìœ¼ë¡œ ë³´ì¥í•œë‹¤. íšŒì „, í¬ê¸° ì¡°ì •, ë’¤ì§‘ê¸° ë“±ì˜ ë³€í™˜ì´ ì ìš©ë  ë•Œ ë°”ìš´ë”© ë°•ìŠ¤, í‚¤í¬ì¸íŠ¸, ë§ˆìŠ¤í¬ë„ **ì •í™•íˆ ê°™ì€ ë³€í™˜**ì´ ì ìš©ë˜ì–´ ë°ì´í„°ì˜ ì •í•©ì„±ì„ ìœ ì§€í•œë‹¤. {: .prompt-tip}

## ğŸ”„ ReplayCompose: ì¬í˜„ ê°€ëŠ¥í•œ ì¦ê°•

**ReplayCompose**ëŠ” Albumentationsì˜ ê³ ê¸‰ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ë¡œ, **ë™ì¼í•œ ë³€í™˜ì„ ë‹¤ì‹œ ì ìš©**í•˜ê±°ë‚˜ **ë³€í™˜ ê³¼ì •ì„ ì¶”ì **í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. ì´ëŠ” **ë””ë²„ê¹…**, **ê²°ê³¼ ë¶„ì„**, **ì¼ê´€ëœ ë³€í™˜ ì ìš©** ë“±ì— ë§¤ìš° ìœ ìš©í•˜ë‹¤.

### ReplayCompose ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import albumentations as A
import numpy as np
import cv2
import json

# ReplayCompose ì •ì˜
replay_transform = A.ReplayCompose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),
    A.Rotate(limit=30, p=0.6),
    A.OneOf([
        A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),
        A.Blur(blur_limit=3, p=1.0),
    ], p=0.4),
    A.Resize(height=224, width=224),
])

# ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±
def create_test_image():
    image = np.zeros((200, 200, 3), dtype=np.uint8)
    image.fill(200)
    # íŒ¨í„´ ì¶”ê°€
    cv2.rectangle(image, (50, 50), (150, 150), (255, 0, 0), -1)
    cv2.circle(image, (100, 100), 30, (0, 255, 0), -1)
    return image

original_image = create_test_image()

# ì²« ë²ˆì§¸ ë³€í™˜ ì ìš©
result1 = replay_transform(image=original_image)
transformed_image1 = result1['image']

print("ì²« ë²ˆì§¸ ë³€í™˜ ì •ë³´:")
print(f"ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {original_image.shape}")
print(f"ë³€í™˜ëœ ì´ë¯¸ì§€ í¬ê¸°: {transformed_image1.shape}")

# ë³€í™˜ íˆìŠ¤í† ë¦¬ í™•ì¸
replay_data = result1['replay']
print(f"\nì ìš©ëœ ë³€í™˜ ê°œìˆ˜: {len(replay_data['transforms'])}")

for i, transform_info in enumerate(replay_data['transforms']):
    transform_name = transform_info['__class_fullname__'].split('.')[-1]
    applied = transform_info.get('applied', True)
    print(f"  {i+1}. {transform_name}: {'ì ìš©ë¨' if applied else 'ì ìš© ì•ˆë¨'}")
    
    # íŒŒë¼ë¯¸í„° ì •ë³´ (ì¼ë¶€ë§Œ í‘œì‹œ)
    params = {k: v for k, v in transform_info.items() 
             if k not in ['__class_fullname__', 'applied'] and not k.startswith('_')}
    if params:
        print(f"     íŒŒë¼ë¯¸í„°: {params}")

# ì²« ë²ˆì§¸ ë³€í™˜ ì •ë³´:
# ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: (200, 200, 3)
# ë³€í™˜ëœ ì´ë¯¸ì§€ í¬ê¸°: (224, 224, 3)

# ì ìš©ëœ ë³€í™˜ ê°œìˆ˜: 5
#   1. HorizontalFlip: ì ìš© ì•ˆë¨
#   2. RandomBrightnessContrast: ì ìš©ë¨
#      íŒŒë¼ë¯¸í„°: {'brightness_factor': 1.2345678, 'contrast_factor': 0.8765432}
#   3. Rotate: ì ìš©ë¨
#      íŒŒë¼ë¯¸í„°: {'angle': 15.234}
#   4. OneOf: ì ìš©ë¨
#   5. Resize: ì ìš©ë¨
#      íŒŒë¼ë¯¸í„°: {'height': 224, 'width': 224}
```

### ë™ì¼í•œ ë³€í™˜ ì¬ì ìš©

ReplayComposeì˜ í•µì‹¬ ê¸°ëŠ¥ì€ **ë™ì¼í•œ ë³€í™˜ì„ ë‹¤ë¥¸ ì´ë¯¸ì§€ì— ë‹¤ì‹œ ì ìš©**í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤.

```python
# ë‹¤ë¥¸ ì´ë¯¸ì§€ ìƒì„±
def create_another_test_image():
    image = np.zeros((200, 200, 3), dtype=np.uint8)
    image.fill(150)
    # ë‹¤ë¥¸ íŒ¨í„´
    cv2.circle(image, (100, 100), 80, (0, 0, 255), -1)
    cv2.rectangle(image, (75, 75), (125, 125), (255, 255, 0), -1)
    return image

second_image = create_another_test_image()

# ì²« ë²ˆì§¸ ë³€í™˜ì˜ replay ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì— ë™ì¼í•œ ë³€í™˜ ì ìš©
result2 = A.ReplayCompose.replay(replay_data, image=second_image)
transformed_image2 = result2['image']

print("ë™ì¼í•œ ë³€í™˜ì„ ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì— ì ìš©:")
print(f"ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ í¬ê¸°: {second_image.shape}")
print(f"ë³€í™˜ëœ ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ í¬ê¸°: {transformed_image2.shape}")

# ë³€í™˜ íŒŒë¼ë¯¸í„°ê°€ ë™ì¼í•œì§€ í™•ì¸
replay_data2 = result2['replay']
print("\në³€í™˜ íŒŒë¼ë¯¸í„° ì¼ì¹˜ í™•ì¸:")

for i, (t1, t2) in enumerate(zip(replay_data['transforms'], replay_data2['transforms'])):
    t1_name = t1['__class_fullname__'].split('.')[-1]
    t2_name = t2['__class_fullname__'].split('.')[-1]
    
    print(f"  ë³€í™˜ {i+1}: {t1_name} == {t2_name}: {t1_name == t2_name}")
    
    # í•µì‹¬ íŒŒë¼ë¯¸í„°ë§Œ ë¹„êµ
    key_params = ['applied', 'angle', 'brightness_factor', 'contrast_factor']
    for param in key_params:
        if param in t1 and param in t2:
            match = abs(t1[param] - t2[param]) < 1e-6 if isinstance(t1[param], (int, float)) else t1[param] == t2[param]
            print(f"    {param}: {match}")

# ë™ì¼í•œ ë³€í™˜ì„ ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì— ì ìš©:
# ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ í¬ê¸°: (200, 200, 3)
# ë³€í™˜ëœ ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ í¬ê¸°: (224, 224, 3)

# ë³€í™˜ íŒŒë¼ë¯¸í„° ì¼ì¹˜ í™•ì¸:
#   ë³€í™˜ 1: HorizontalFlip == HorizontalFlip: True
#     applied: True
#   ë³€í™˜ 2: RandomBrightnessContrast == RandomBrightnessContrast: True
#     applied: True
#     brightness_factor: True
#     contrast_factor: True
#   ë³€í™˜ 3: Rotate == Rotate: True
#     applied: True
#     angle: True
```

### ë³€í™˜ íˆìŠ¤í† ë¦¬ ì €ì¥ ë° ë¡œë“œ

ë³µì¡í•œ ì‹¤í—˜ì—ì„œëŠ” ë³€í™˜ íˆìŠ¤í† ë¦¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•´ë‘ëŠ” ê²ƒì´ ìœ ìš©í•˜ë‹¤.

```python
import json
import pickle
from pathlib import Path

class ReplayManager:
    """ReplayCompose ë°ì´í„° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, save_dir="replay_logs"):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
    
    def save_replay_json(self, replay_data, filename):
        """Replay ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        filepath = self.save_dir / f"{filename}.json"
        
        # NumPy ë°°ì—´ì´ë‚˜ ë‹¤ë¥¸ íƒ€ì…ë“¤ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
        serializable_data = self._make_serializable(replay_data)
        
        with open(filepath, 'w') as f:
            json.dump(serializable_data, f, indent=2)
        
        print(f"Replay ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    
    def load_replay_json(self, filename):
        """JSONì—ì„œ Replay ë°ì´í„°ë¥¼ ë¡œë“œ"""
        filepath = self.save_dir / f"{filename}.json"
        
        with open(filepath, 'r') as f:
            replay_data = json.load(f)
        
        print(f"Replay ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
        return replay_data
    
    def save_replay_pickle(self, replay_data, filename):
        """Replay ë°ì´í„°ë¥¼ Pickleë¡œ ì €ì¥ (ë” ì •í™•í•˜ì§€ë§Œ ë³´ì•ˆ ìœ„í—˜)"""
        filepath = self.save_dir / f"{filename}.pkl"
        
        with open(filepath, 'wb') as f:
            pickle.dump(replay_data, f)
        
        print(f"Replay ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    
    def load_replay_pickle(self, filename):
        """Pickleì—ì„œ Replay ë°ì´í„°ë¥¼ ë¡œë“œ"""
        filepath = self.save_dir / f"{filename}.pkl"
        
        with open(filepath, 'rb') as f:
            replay_data = pickle.load(f)
        
        print(f"Replay ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
        return replay_data
    
    def _make_serializable(self, obj):
        """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj
    
    def list_saved_replays(self):
        """ì €ì¥ëœ replay íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        json_files = list(self.save_dir.glob("*.json"))
        pickle_files = list(self.save_dir.glob("*.pkl"))
        
        return {
            'json': [f.stem for f in json_files],
            'pickle': [f.stem for f in pickle_files]
        }

# ReplayManager ì‚¬ìš© ì˜ˆì‹œ
replay_manager = ReplayManager()

# ë³€í™˜ ì‹¤í–‰ ë° ì €ì¥
test_image = create_test_image()
result = replay_transform(image=test_image)

# Replay ë°ì´í„° ì €ì¥
replay_manager.save_replay_json(result['replay'], "experiment_001")

# ì €ì¥ëœ íŒŒì¼ ëª©ë¡ í™•ì¸
saved_files = replay_manager.list_saved_replays()
print(f"ì €ì¥ëœ replay íŒŒì¼ë“¤: {saved_files}")

# Replay ë°ì´í„° ë¡œë“œ ë° ì¬ì‚¬ìš©
loaded_replay = replay_manager.load_replay_json("experiment_001")

# ìƒˆë¡œìš´ ì´ë¯¸ì§€ì— ë¡œë“œëœ ë³€í™˜ ì ìš©
new_test_image = create_another_test_image()
replayed_result = A.ReplayCompose.replay(loaded_replay, image=new_test_image)

print(f"ë¡œë“œëœ ë³€í™˜ ì ìš© ì™„ë£Œ: {replayed_result['image'].shape}")

# Replay ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: replay_logs/experiment_001.json
# ì €ì¥ëœ replay íŒŒì¼ë“¤: {'json': ['experiment_001'], 'pickle': []}
# Replay ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: replay_logs/experiment_001.json
# ë¡œë“œëœ ë³€í™˜ ì ìš© ì™„ë£Œ: (224, 224, 3)
```

### ë°°ì¹˜ ë‹¨ìœ„ ì¼ê´€ëœ ë³€í™˜

ë™ì¼í•œ ë³€í™˜ì„ ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ì¼ê´€ë˜ê²Œ ì ìš©í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. (ì˜ˆ: ë°ì´í„° ìŒ, ë¹„ë””ì˜¤ í”„ë ˆì„ ë“±)

```python
def apply_consistent_transforms_to_batch(images, transform):
    """ë°°ì¹˜ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ì— ë™ì¼í•œ ë³€í™˜ ì ìš©"""
    
    if len(images) == 0:
        return []
    
    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ìƒì„±
    first_result = transform(image=images[0])
    replay_data = first_result['replay']
    
    # ëª¨ë“  ì´ë¯¸ì§€ì— ë™ì¼í•œ ë³€í™˜ ì ìš©
    results = [first_result['image']]
    
    for image in images[1:]:
        result = A.ReplayCompose.replay(replay_data, image=image)
        results.append(result['image'])
    
    return results, replay_data

# ì—¬ëŸ¬ ê´€ë ¨ ì´ë¯¸ì§€ ìƒì„± (ì˜ˆ: ë¹„ë””ì˜¤ í”„ë ˆì„)
def create_image_sequence(num_frames=5):
    """ì—°ì†ëœ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„±"""
    images = []
    
    for i in range(num_frames):
        image = np.zeros((200, 200, 3), dtype=np.uint8)
        image.fill(180)
        
        # ì›€ì§ì´ëŠ” ê°ì²´ ì‹œë®¬ë ˆì´ì…˜
        center_x = 50 + i * 20
        center_y = 100
        
        cv2.circle(image, (center_x, center_y), 20, (255, 0, 0), -1)
        cv2.rectangle(image, (center_x-10, center_y-30), (center_x+10, center_y-10), (0, 255, 0), -1)
        
        images.append(image)
    
    return images

# ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„±
image_sequence = create_image_sequence(5)

print("ì›ë³¸ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤:")
for i, img in enumerate(image_sequence):
    print(f"  í”„ë ˆì„ {i+1}: {img.shape}")

# ì¼ê´€ëœ ë³€í™˜ ì ìš©
consistent_transforms = A.ReplayCompose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=20, p=0.8),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.6),
    A.Resize(height=128, width=128),
])

transformed_sequence, sequence_replay = apply_consistent_transforms_to_batch(
    image_sequence, consistent_transforms
)

print("\në³€í™˜ëœ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤:")
for i, img in enumerate(transformed_sequence):
    print(f"  í”„ë ˆì„ {i+1}: {img.shape}")

print(f"\nëª¨ë“  í”„ë ˆì„ì— ë™ì¼í•œ ë³€í™˜ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ì ìš©ëœ ë³€í™˜ ìˆ˜: {len(sequence_replay['transforms'])}")

# ë³€í™˜ ì¼ê´€ì„± í™•ì¸
def verify_transform_consistency(replay_data, images):
    """ë³€í™˜ ì¼ê´€ì„± ê²€ì¦"""
    print("\në³€í™˜ ì¼ê´€ì„± ê²€ì¦:")
    
    # ê° ë³€í™˜ì˜ applied ìƒíƒœì™€ íŒŒë¼ë¯¸í„° í™•ì¸
    for i, transform_info in enumerate(replay_data['transforms']):
        transform_name = transform_info['__class_fullname__'].split('.')[-1]
        applied = transform_info.get('applied', True)
        
        print(f"  ë³€í™˜ {i+1} - {transform_name}: {'ì ìš©ë¨' if applied else 'ì ìš© ì•ˆë¨'}")
        
        if applied and transform_name == 'Rotate':
            angle = transform_info.get('angle', 'N/A')
            print(f"    íšŒì „ ê°ë„: {angle}Â°")
        elif applied and transform_name == 'RandomBrightnessContrast':
            brightness = transform_info.get('brightness_factor', 'N/A')
            contrast = transform_info.get('contrast_factor', 'N/A')
            print(f"    ë°ê¸° íŒ©í„°: {brightness:.3f}, ëŒ€ë¹„ íŒ©í„°: {contrast:.3f}")

verify_transform_consistency(sequence_replay, transformed_sequence)

# ì›ë³¸ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤:
#   í”„ë ˆì„ 1: (200, 200, 3)
#   í”„ë ˆì„ 2: (200, 200, 3)
#   í”„ë ˆì„ 3: (200, 200, 3)
#   í”„ë ˆì„ 4: (200, 200, 3)
#   í”„ë ˆì„ 5: (200, 200, 3)

# ë³€í™˜ëœ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤:
#   í”„ë ˆì„ 1: (128, 128, 3)
#   í”„ë ˆì„ 2: (128, 128, 3)
#   í”„ë ˆì„ 3: (128, 128, 3)
#   í”„ë ˆì„ 4: (128, 128, 3)
#   í”„ë ˆì„ 5: (128, 128, 3)

# ëª¨ë“  í”„ë ˆì„ì— ë™ì¼í•œ ë³€í™˜ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.
# ì ìš©ëœ ë³€í™˜ ìˆ˜: 4

# ë³€í™˜ ì¼ê´€ì„± ê²€ì¦:
#   ë³€í™˜ 1 - HorizontalFlip: ì ìš©ë¨
#   ë³€í™˜ 2 - Rotate: ì ìš©ë¨
#     íšŒì „ ê°ë„: 12.345Â°
#   ë³€í™˜ 3 - RandomBrightnessContrast: ì ìš©ë¨
#     ë°ê¸° íŒ©í„°: 1.123, ëŒ€ë¹„ íŒ©í„°: 0.987
#   ë³€í™˜ 4 - Resize: ì ìš©ë¨
```

### A/B í…ŒìŠ¤íŠ¸ ë° ì‹¤í—˜ ê´€ë¦¬

ReplayComposeë¥¼ í™œìš©í•´ ì²´ê³„ì ì¸ A/B í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

```python
class AugmentationExperimentManager:
    """ì¦ê°• ì‹¤í—˜ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir="experiments"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.experiments = {}
    
    def create_experiment(self, name, transform_configs):
        """ìƒˆë¡œìš´ ì‹¤í—˜ ìƒì„±"""
        exp_dir = self.base_dir / name
        exp_dir.mkdir(exist_ok=True)
        
        self.experiments[name] = {
            'dir': exp_dir,
            'configs': transform_configs,
            'results': []
        }
        
        print(f"ì‹¤í—˜ '{name}' ìƒì„±ë¨: {len(transform_configs)}ê°œ êµ¬ì„±")
    
    def run_experiment(self, name, test_images, save_results=True):
        """ì‹¤í—˜ ì‹¤í–‰"""
        if name not in self.experiments:
            raise ValueError(f"ì‹¤í—˜ '{name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        experiment = self.experiments[name]
        results = []
        
        for config_idx, config in enumerate(experiment['configs']):
            config_name = config['name']
            transform = config['transform']
            
            print(f"\nì‹¤í—˜ êµ¬ì„± '{config_name}' ì‹¤í–‰ ì¤‘...")
            
            config_results = []
            for img_idx, image in enumerate(test_images):
                result = transform(image=image)
                
                config_results.append({
                    'image_idx': img_idx,
                    'transformed_image': result['image'],
                    'replay_data': result['replay']
                })
                
                # ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
                if save_results:
                    save_path = experiment['dir'] / f"{config_name}_img_{img_idx}.json"
                    with open(save_path, 'w') as f:
                        # replay ë°ì´í„°ë§Œ ì €ì¥
                        json.dump(result['replay'], f, indent=2, default=str)
            
            results.append({
                'config_name': config_name,
                'config_idx': config_idx,
                'results': config_results
            })
        
        experiment['results'] = results
        print(f"\nì‹¤í—˜ '{name}' ì™„ë£Œ: {len(results)}ê°œ êµ¬ì„± í…ŒìŠ¤íŠ¸ë¨")
        return results
    
    def compare_configs(self, experiment_name, comparison_metric=None):
        """ì‹¤í—˜ êµ¬ì„±ë“¤ ë¹„êµ"""
        if experiment_name not in self.experiments:
            raise ValueError(f"ì‹¤í—˜ '{experiment_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        experiment = self.experiments[experiment_name]
        results = experiment['results']
        
        print(f"\nì‹¤í—˜ '{experiment_name}' ê²°ê³¼ ë¹„êµ:")
        print("-" * 50)
        
        for result in results:
            config_name = result['config_name']
            config_results = result['results']
            
            print(f"\nêµ¬ì„±: {config_name}")
            print(f"  ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ìˆ˜: {len(config_results)}")
            
            # ë³€í™˜ í†µê³„
            total_transforms = 0
            applied_transforms = 0
            
            for img_result in config_results:
                replay_data = img_result['replay_data']
                transforms = replay_data['transforms']
                total_transforms += len(transforms)
                applied_transforms += sum(1 for t in transforms if t.get('applied', True))
            
            avg_transforms = total_transforms / len(config_results)
            avg_applied = applied_transforms / len(config_results)
            
            print(f"  í‰ê·  ë³€í™˜ ìˆ˜: {avg_transforms:.1f}")
            print(f"  í‰ê·  ì ìš©ëœ ë³€í™˜ ìˆ˜: {avg_applied:.1f}")
            print(f"  ì ìš©ë¥ : {avg_applied/avg_transforms*100:.1f}%")

# ë‹¤ì–‘í•œ ì¦ê°• êµ¬ì„± ì •ì˜
def create_augmentation_configs():
    """ë‹¤ì–‘í•œ ì¦ê°• êµ¬ì„± ìƒì„±"""
    
    configs = [
        {
            'name': 'light_augmentation',
            'transform': A.ReplayCompose([
                A.HorizontalFlip(p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
                A.Resize(height=224, width=224),
            ])
        },
        {
            'name': 'medium_augmentation', 
            'transform': A.ReplayCompose([
                A.HorizontalFlip(p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
                A.Rotate(limit=15, p=0.4),
                A.OneOf([
                    A.GaussNoise(var_limit=(10.0, 30.0), p=1.0),
                    A.Blur(blur_limit=3, p=1.0),
                ], p=0.3),
                A.Resize(height=224, width=224),
            ])
        },
        {
            'name': 'heavy_augmentation',
            'transform': A.ReplayCompose([
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.2),
                A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),
                A.Rotate(limit=30, p=0.6),
                A.OneOf([
                    A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),
                    A.Blur(blur_limit=5, p=1.0),
                    A.MotionBlur(blur_limit=5, p=1.0),
                ], p=0.5),
                A.OneOf([
                    A.ElasticTransform(alpha=50, sigma=5, p=1.0),
                    A.GridDistortion(num_steps=5, distort_limit=0.3, p=1.0),
                ], p=0.3),
                A.Resize(height=224, width=224),
            ])
        }
    ]
    
    return configs

# ì‹¤í—˜ ì‹¤í–‰ ì˜ˆì‹œ
exp_manager = AugmentationExperimentManager()

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
test_images = [create_test_image(), create_another_test_image()]

# ì‹¤í—˜ êµ¬ì„± ìƒì„±
configs = create_augmentation_configs()

# ì‹¤í—˜ ìƒì„± ë° ì‹¤í–‰
exp_manager.create_experiment("augmentation_comparison", configs)
results = exp_manager.run_experiment("augmentation_comparison", test_images, save_results=False)

# ê²°ê³¼ ë¹„êµ
exp_manager.compare_configs("augmentation_comparison")

# ì‹¤í—˜ 'augmentation_comparison' ìƒì„±ë¨: 3ê°œ êµ¬ì„±

# ì‹¤í—˜ êµ¬ì„± 'light_augmentation' ì‹¤í–‰ ì¤‘...
# ì‹¤í—˜ êµ¬ì„± 'medium_augmentation' ì‹¤í–‰ ì¤‘...
# ì‹¤í—˜ êµ¬ì„± 'heavy_augmentation' ì‹¤í–‰ ì¤‘...

# ì‹¤í—˜ 'augmentation_comparison' ì™„ë£Œ: 3ê°œ êµ¬ì„± í…ŒìŠ¤íŠ¸ë¨

# ì‹¤í—˜ 'augmentation_comparison' ê²°ê³¼ ë¹„êµ:
# --------------------------------------------------

# êµ¬ì„±: light_augmentation
#   ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ìˆ˜: 2
#   í‰ê·  ë³€í™˜ ìˆ˜: 3.0
#   í‰ê·  ì ìš©ëœ ë³€í™˜ ìˆ˜: 2.5
#   ì ìš©ë¥ : 83.3%

# êµ¬ì„±: medium_augmentation
#   ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ìˆ˜: 2
#   í‰ê·  ë³€í™˜ ìˆ˜: 5.0
#   í‰ê·  ì ìš©ëœ ë³€í™˜ ìˆ˜: 3.2
#   ì ìš©ë¥ : 64.0%

# êµ¬ì„±: heavy_augmentation
#   ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ìˆ˜: 2
#   í‰ê·  ë³€í™˜ ìˆ˜: 7.0
#   í‰ê·  ì ìš©ëœ ë³€í™˜ ìˆ˜: 4.1
#   ì ìš©ë¥ : 58.6%
```

> ReplayComposeëŠ” **ì‹¤í—˜ì˜ ì¬í˜„ì„±**ê³¼ **ì¼ê´€ì„±**ì„ ë³´ì¥í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ë‹¤. íŠ¹íˆ **A/B í…ŒìŠ¤íŠ¸**, **ëª¨ë¸ ë¹„êµ**, **ë””ë²„ê¹…** ìƒí™©ì—ì„œ ë™ì¼í•œ ì¡°ê±´ì„ ìœ ì§€í•˜ë©° ì‹¤í—˜í•  ìˆ˜ ìˆì–´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. {: .prompt-tip}

## ğŸš€ ê³ ê¸‰ í™œìš© ì‚¬ë¡€

### ì»¤ìŠ¤í…€ ë³€í™˜ êµ¬í˜„

Albumentationsì˜ ê¸°ë³¸ ë³€í™˜ë§Œìœ¼ë¡œ ë¶€ì¡±í•  ë•Œ, ì»¤ìŠ¤í…€ ë³€í™˜ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

```python
import albumentations as A
from albumentations.core.transforms_interface import ImageOnlyTransform, DualTransform
import numpy as np
import cv2

class CustomColorJitter(ImageOnlyTransform):
    """ì»¤ìŠ¤í…€ ìƒ‰ìƒ ì§€í„°ë§ ë³€í™˜"""
    
    def __init__(self, hue_shift_limit=20, saturation_multiplier=(0.5, 1.5), 
                 value_shift_limit=30, always_apply=False, p=0.5):
        super().__init__(always_apply, p)
        self.hue_shift_limit = hue_shift_limit
        self.saturation_multiplier = saturation_multiplier
        self.value_shift_limit = value_shift_limit
    
    def apply(self, image, hue_shift=0, sat_multiplier=1.0, value_shift=0, **params):
        """ì‹¤ì œ ë³€í™˜ ë¡œì§"""
        # BGR to HSV ë³€í™˜
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)
        
        # Hue ì¡°ì • (ìˆœí™˜ì )
        hsv[:, :, 0] = (hsv[:, :, 0] + hue_shift) % 180
        
        # Saturation ì¡°ì •
        hsv[:, :, 1] *= sat_multiplier
        hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)
        
        # Value ì¡°ì •
        hsv[:, :, 2] += value_shift
        hsv[:, :, 2] = np.clip(hsv[:, :, 2], 0, 255)
        
        # HSV to RGB ë³€í™˜
        result = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
        return result
    
    def get_params(self):
        """ëœë¤ íŒŒë¼ë¯¸í„° ìƒì„±"""
        return {
            'hue_shift': np.random.uniform(-self.hue_shift_limit, self.hue_shift_limit),
            'sat_multiplier': np.random.uniform(*self.saturation_multiplier),
            'value_shift': np.random.uniform(-self.value_shift_limit, self.value_shift_limit)
        }
    
    def get_transform_init_args_names(self):
        """ì§ë ¬í™”ë¥¼ ìœ„í•œ ì´ˆê¸°í™” ì¸ìëª… ë°˜í™˜"""
        return ('hue_shift_limit', 'saturation_multiplier', 'value_shift_limit')

class CustomCutout(DualTransform):
    """ì»¤ìŠ¤í…€ ì»·ì•„ì›ƒ ë³€í™˜ (ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ì— ëª¨ë‘ ì ìš©)"""
    
    def __init__(self, num_holes=1, max_h_size=64, max_w_size=64, 
                 fill_value=0, always_apply=False, p=0.5):
        super().__init__(always_apply, p)
        self.num_holes = num_holes
        self.max_h_size = max_h_size
        self.max_w_size = max_w_size
        self.fill_value = fill_value
    
    def apply(self, image, holes=None, **params):
        """ì´ë¯¸ì§€ì— ì ìš©"""
        image = image.copy()
        
        for hole in holes:
            y1, y2, x1, x2 = hole
            if len(image.shape) == 3:
                image[y1:y2, x1:x2, :] = self.fill_value
            else:
                image[y1:y2, x1:x2] = self.fill_value
        
        return image
    
    def apply_to_mask(self, mask, holes=None, **params):
        """ë§ˆìŠ¤í¬ì— ì ìš©"""
        mask = mask.copy()
        
        for hole in holes:
            y1, y2, x1, x2 = hole
            mask[y1:y2, x1:x2] = 0  # ë§ˆìŠ¤í¬ëŠ” 0ìœ¼ë¡œ ì±„ì›€
        
        return mask
    
    def get_params_dependent_on_targets(self, params):
        """ì´ë¯¸ì§€ í¬ê¸°ì— ì˜ì¡´ì ì¸ íŒŒë¼ë¯¸í„° ìƒì„±"""
        image = params['image']
        height, width = image.shape[:2]
        
        holes = []
        for _ in range(self.num_holes):
            hole_height = np.random.randint(1, min(self.max_h_size, height))
            hole_width = np.random.randint(1, min(self.max_w_size, width))
            
            y1 = np.random.randint(0, height - hole_height)
            x1 = np.random.randint(0, width - hole_width)
            y2 = y1 + hole_height
            x2 = x1 + hole_width
            
            holes.append((y1, y2, x1, x2))
        
        return {'holes': holes}
    
    @property
    def targets_as_params(self):
        """íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©í•  íŒŒë¼ë¯¸í„° ì •ì˜"""
        return ['image']
    
    def get_transform_init_args_names(self):
        return ('num_holes', 'max_h_size', 'max_w_size', 'fill_value')

# ì»¤ìŠ¤í…€ ë³€í™˜ ì‚¬ìš© ì˜ˆì‹œ
custom_transforms = A.Compose([
    CustomColorJitter(hue_shift_limit=30, saturation_multiplier=(0.3, 2.0), p=0.7),
    CustomCutout(num_holes=3, max_h_size=32, max_w_size=32, fill_value=128, p=0.5),
    A.Resize(height=224, width=224),
])

# í…ŒìŠ¤íŠ¸
test_image = create_test_image()
test_mask = np.zeros((200, 200), dtype=np.uint8)
cv2.rectangle(test_mask, (50, 50), (150, 150), 1, -1)

result = custom_transforms(image=test_image, mask=test_mask)

print("ì»¤ìŠ¤í…€ ë³€í™˜ í…ŒìŠ¤íŠ¸:")
print(f"ì›ë³¸ ì´ë¯¸ì§€: {test_image.shape}")
print(f"ë³€í™˜ëœ ì´ë¯¸ì§€: {result['image'].shape}")
print(f"ì›ë³¸ ë§ˆìŠ¤í¬: {test_mask.shape}, ê³ ìœ ê°’: {np.unique(test_mask)}")
print(f"ë³€í™˜ëœ ë§ˆìŠ¤í¬: {result['mask'].shape}, ê³ ìœ ê°’: {np.unique(result['mask'])}")

# ì»¤ìŠ¤í…€ ë³€í™˜ í…ŒìŠ¤íŠ¸:
# ì›ë³¸ ì´ë¯¸ì§€: (200, 200, 3)
# ë³€í™˜ëœ ì´ë¯¸ì§€: (224, 224, 3)
# ì›ë³¸ ë§ˆìŠ¤í¬: (200, 200), ê³ ìœ ê°’: [0 1]
# ë³€í™˜ëœ ë§ˆìŠ¤í¬: (224, 224), ê³ ìœ ê°’: [0 1]
```

### ë„ë©”ì¸ë³„ íŠ¹í™” íŒŒì´í”„ë¼ì¸

íŠ¹ì • ë„ë©”ì¸(ì˜ë£Œ, ìœ„ì„±ì˜ìƒ, ì œì¡°ì—… ë“±)ì— íŠ¹í™”ëœ ì¦ê°• íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.

```python
def get_medical_image_transforms(image_size=512):
    """ì˜ë£Œ ì˜ìƒìš© ì¦ê°• íŒŒì´í”„ë¼ì¸"""
    return A.Compose([
        # ê¸°í•˜í•™ì  ë³€í™˜ (ë³´ìˆ˜ì )
        A.HorizontalFlip(p=0.5),
        A.Rotate(limit=10, p=0.3),  # ì‘ì€ íšŒì „ë§Œ
        A.ShiftScaleRotate(
            shift_limit=0.05, scale_limit=0.1, rotate_limit=5, p=0.3
        ),
        
        # ì˜ë£Œ ì˜ìƒ íŠ¹ì„± ê³ ë ¤í•œ ìƒ‰ìƒ/ë°ê¸° ì¡°ì •
        A.RandomBrightnessContrast(
            brightness_limit=0.1, contrast_limit=0.15, p=0.5
        ),
        A.RandomGamma(gamma_limit=(0.9, 1.1), p=0.3),
        
        # ë…¸ì´ì¦ˆ (ì˜ë£Œ ì¥ë¹„ íŠ¹ì„± ë°˜ì˜)
        A.OneOf([
            A.GaussNoise(var_limit=(5.0, 15.0), p=1.0),
            A.ISONoise(color_shift=(0.01, 0.03), intensity=(0.05, 0.2), p=1.0),
        ], p=0.2),
        
        # ë¸”ëŸ¬ (ì´¬ì˜ ì¡°ê±´ ë³€í™”)
        A.OneOf([
            A.Blur(blur_limit=2, p=1.0),
            A.GaussianBlur(blur_limit=2, p=1.0),
        ], p=0.1),
        
        A.Resize(height=image_size, width=image_size),
        A.Normalize(mean=[0.485], std=[0.229]),  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ìš©
        ToTensorV2(),
    ])

def get_satellite_image_transforms(image_size=256):
    """ìœ„ì„± ì˜ìƒìš© ì¦ê°• íŒŒì´í”„ë¼ì¸"""
    return A.Compose([
        # ìœ„ì„± ì˜ìƒ íŠ¹ì„±ìƒ íšŒì „ ììœ ë„ ë†’ìŒ
        A.RandomRotate90(p=0.5),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.Rotate(limit=45, p=0.5),
        
        # ëŒ€ê¸° ì¡°ê±´, ì¡°ëª… ë³€í™”
        A.RandomBrightnessContrast(
            brightness_limit=0.2, contrast_limit=0.2, p=0.7
        ),
        A.HueSaturationValue(
            hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=15, p=0.5
        ),
        
        # ëŒ€ê¸° íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜
        A.OneOf([
            A.Fog(fog_coef_lower=0.1, fog_coef_upper=0.3, alpha_coef=0.1, p=1.0),
            A.RandomSunFlare(
                flare_roi=(0, 0, 1, 0.5), angle_lower=0, angle_upper=1, p=1.0
            ),
        ], p=0.2),
        
        # ì„¼ì„œ ë…¸ì´ì¦ˆ
        A.OneOf([
            A.GaussNoise(var_limit=(10.0, 30.0), p=1.0),
            A.MultiplicativeNoise(multiplier=(0.95, 1.05), p=1.0),
        ], p=0.3),
        
        A.Resize(height=image_size, width=image_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])

def get_manufacturing_defect_transforms(image_size=224):
    """ì œì¡°ì—… ë¶ˆëŸ‰ ê²€ì¶œìš© ì¦ê°• íŒŒì´í”„ë¼ì¸"""
    return A.Compose([
        # ì¹´ë©”ë¼ ê°ë„ ë³€í™” (ê²€ì‚¬ í™˜ê²½)
        A.Perspective(scale=(0.02, 0.08), p=0.3),
        A.Rotate(limit=5, p=0.4),  # ì œí’ˆ ë°°ì¹˜ ë³€í™”
        
        # ì¡°ëª… ì¡°ê±´ ë³€í™”
        A.RandomBrightnessContrast(
            brightness_limit=0.25, contrast_limit=0.25, p=0.8
        ),
        A.RandomShadow(
            shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, p=0.3
        ),
        
        # ì¹´ë©”ë¼/ë Œì¦ˆ íš¨ê³¼
        A.OneOf([
            A.Blur(blur_limit=2, p=1.0),
            A.MotionBlur(blur_limit=3, p=1.0),
            A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),
        ], p=0.2),
        
        # ì„¼ì„œ ë…¸ì´ì¦ˆ
        A.OneOf([
            A.GaussNoise(var_limit=(5.0, 20.0), p=1.0),
            A.ISONoise(color_shift=(0.01, 0.04), intensity=(0.1, 0.3), p=1.0),
        ], p=0.4),
        
        # ìƒ‰ìƒ ì¡°ì • (ì¡°ëª… ìƒ‰ì˜¨ë„ ë³€í™”)
        A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.4),
        
        A.Resize(height=image_size, width=image_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])

# ë„ë©”ì¸ë³„ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
domains = {
    'medical': get_medical_image_transforms(),
    'satellite': get_satellite_image_transforms(), 
    'manufacturing': get_manufacturing_defect_transforms()
}

print("ë„ë©”ì¸ë³„ ì¦ê°• íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸:")
test_image = create_test_image()

for domain_name, transform_pipeline in domains.items():
    try:
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ (ì˜ë£Œ ì˜ìƒìš©)
        if domain_name == 'medical':
            gray_image = cv2.cvtColor(test_image, cv2.COLOR_RGB2GRAY)
            gray_image = np.stack([gray_image] * 3, axis=2)  # 3ì±„ë„ë¡œ í™•ì¥
            result = transform_pipeline(image=gray_image)
        else:
            result = transform_pipeline(image=test_image)
        
        transformed_image = result['image']
        print(f"  {domain_name}: {test_image.shape} -> {transformed_image.shape}")
        print(f"    íƒ€ì…: {type(transformed_image)}, dtype: {transformed_image.dtype}")
        
        if hasattr(transformed_image, 'shape') and len(transformed_image.shape) > 2:
            print(f"    ê°’ ë²”ìœ„: [{transformed_image.min():.3f}, {transformed_image.max():.3f}]")
            
    except Exception as e:
        print(f"  {domain_name}: ì—ëŸ¬ ë°œìƒ - {e}")

# ë„ë©”ì¸ë³„ ì¦ê°• íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸:
#   medical: (200, 200, 3) -> torch.Size([3, 512, 512])
#     íƒ€ì…: <class 'torch.Tensor'>, dtype: torch.float32
#     ê°’ ë²”ìœ„: [-2.118, 2.640]
#   satellite: (200, 200, 3) -> torch.Size([3, 256, 256])
#     íƒ€ì…: <class 'torch.Tensor'>, dtype: torch.float32
#     ê°’ ë²”ìœ„: [-2.118, 2.640]
#   manufacturing: (200, 200, 3) -> torch.Size([3, 224, 224])
#     íƒ€ì…: <class 'torch.Tensor'>, dtype: torch.float32
#     ê°’ ë²”ìœ„: [-2.118, 2.640]
```

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™”

```python
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time

class OptimizedDataLoader:
    """ìµœì í™”ëœ ë°ì´í„° ë¡œë”"""
    
    def __init__(self, dataset, batch_size=32, num_workers=4, 
                 prefetch_factor=2, use_threading=False):
        self.dataset = dataset
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.prefetch_factor = prefetch_factor
        self.use_threading = use_threading
        
    def _worker_init_fn(self, worker_id):
        """ì›Œì»¤ ì´ˆê¸°í™” í•¨ìˆ˜"""
        # ê° ì›Œì»¤ë§ˆë‹¤ ë‹¤ë¥¸ ëœë¤ ì‹œë“œ ì„¤ì •
        np.random.seed(np.random.get_state()[1][0] + worker_id)
        
    def _process_batch_indices(self, indices):
        """ë°°ì¹˜ ì¸ë±ìŠ¤ ì²˜ë¦¬"""
        batch_data = []
        for idx in indices:
            try:
                data = self.dataset[idx]
                batch_data.append(data)
            except Exception as e:
                print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {idx}): {e}")
                continue
        return batch_data
    
    def __iter__(self):
        """ë°°ì¹˜ ë‹¨ìœ„ ì´í„°ë ˆì´í„°"""
        indices = list(range(len(self.dataset)))
        np.random.shuffle(indices)
        
        # ë°°ì¹˜ ì¸ë±ìŠ¤ ìƒì„±
        batch_indices = [
            indices[i:i + self.batch_size] 
            for i in range(0, len(indices), self.batch_size)
        ]
        
        if self.num_workers <= 1:
            # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
            for batch_idx in batch_indices:
                yield self._process_batch_indices(batch_idx)
        else:
            # ë©€í‹°í”„ë¡œì„¸ì‹±/ìŠ¤ë ˆë”©
            executor_class = ThreadPoolExecutor if self.use_threading else ProcessPoolExecutor
            
            with executor_class(max_workers=self.num_workers) as executor:
                # í”„ë¦¬í˜ì¹˜ë¥¼ ìœ„í•œ í
                future_to_batch = {}
                
                # ì´ˆê¸° ë°°ì¹˜ë“¤ ì œì¶œ
                for i, batch_idx in enumerate(batch_indices[:self.prefetch_factor]):
                    future = executor.submit(self._process_batch_indices, batch_idx)
                    future_to_batch[future] = i
                
                # ë°°ì¹˜ ì²˜ë¦¬ ë° ìƒˆë¡œìš´ ë°°ì¹˜ ì œì¶œ
                for i, batch_idx in enumerate(batch_indices):
                    if i < self.prefetch_factor:
                        # ì´ë¯¸ ì œì¶œëœ ë°°ì¹˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                        for future in list(future_to_batch.keys()):
                            if future.done():
                                batch_data = future.result()
                                del future_to_batch[future]
                                yield batch_data
                                break
                    else:
                        # ìƒˆë¡œìš´ ë°°ì¹˜ ì œì¶œ
                        if i < len(batch_indices):
                            future = executor.submit(
                                self._process_batch_indices, 
                                batch_indices[i]
                            )
                            future_to_batch[future] = i
                        
                        # ì™„ë£Œëœ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
                        for future in list(future_to_batch.keys()):
                            if future.done():
                                batch_data = future.result()
                                del future_to_batch[future]
                                yield batch_data
                                break
                
                # ë‚¨ì€ ë°°ì¹˜ë“¤ ì²˜ë¦¬
                for future in future_to_batch:
                    batch_data = future.result()
                    yield batch_data

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
def benchmark_loading_performance():
    """ë°ì´í„° ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    # ë”ë¯¸ ë°ì´í„°ì…‹ ìƒì„±
    class BenchmarkDataset:
        def __init__(self, size=1000, image_size=224):
            self.size = size
            self.transforms = get_training_augmentation(image_size)
            
        def __len__(self):
            return self.size
            
        def __getitem__(self, idx):
            # ì˜ë„ì ìœ¼ë¡œ ì•½ê°„ì˜ ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
            image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
            transformed = self.transforms(image=image)
            return transformed['image'], idx % 10
    
    dataset = BenchmarkDataset(size=500)
    batch_size = 32
    
    configs = [
        {'num_workers': 1, 'use_threading': False, 'name': 'ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤'},
        {'num_workers': 2, 'use_threading': False, 'name': '2ê°œ í”„ë¡œì„¸ìŠ¤'},
        {'num_workers': 4, 'use_threading': False, 'name': '4ê°œ í”„ë¡œì„¸ìŠ¤'},
        {'num_workers': 4, 'use_threading': True, 'name': '4ê°œ ìŠ¤ë ˆë“œ'},
    ]
    
    results = {}
    
    for config in configs:
        print(f"\n{config['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        loader = OptimizedDataLoader(
            dataset, 
            batch_size=batch_size,
            num_workers=config['num_workers'],
            use_threading=config['use_threading'],
            prefetch_factor=2
        )
        
        start_time = time.time()
        batch_count = 0
        
        for batch_data in loader:
            batch_count += 1
            if batch_count >= 10:  # ì²˜ìŒ 10ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                break
        
        elapsed_time = time.time() - start_time
        results[config['name']] = {
            'time': elapsed_time,
            'batches_per_second': batch_count / elapsed_time,
            'samples_per_second': (batch_count * batch_size) / elapsed_time
        }
        
        print(f"  ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        print(f"  ë°°ì¹˜/ì´ˆ: {results[config['name']]['batches_per_second']:.2f}")
        print(f"  ìƒ˜í”Œ/ì´ˆ: {results[config['name']]['samples_per_second']:.1f}")
    
    # ìµœê³  ì„±ëŠ¥ êµ¬ì„± ì°¾ê¸°
    best_config = max(results.keys(), key=lambda k: results[k]['samples_per_second'])
    print(f"\nìµœê³  ì„±ëŠ¥: {best_config}")
    print(f"ìµœê³  ì²˜ë¦¬ìœ¨: {results[best_config]['samples_per_second']:.1f} ìƒ˜í”Œ/ì´ˆ")

# benchmark_loading_performance()
print("ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ êµ¬ì¡° í™•ì¸ ì™„ë£Œ")
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

```python
import psutil
import gc
import torch

class MemoryOptimizedDataset(Dataset):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ë°ì´í„°ì…‹"""
    
    def __init__(self, image_paths, labels, transforms=None, 
                 lazy_loading=True, image_cache_size=100):
        self.image_paths = image_paths
        self.labels = labels
        self.transforms = transforms
        self.lazy_loading = lazy_loading
        
        # ì´ë¯¸ì§€ ìºì‹œ (LRU)
        self.image_cache = {}
        self.cache_order = []
        self.cache_size = image_cache_size
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        self.memory_usage = []
        
    def _get_memory_usage(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def _load_image_efficient(self, path):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì´ë¯¸ì§€ ë¡œë”©"""
        if not self.lazy_loading or path not in self.image_cache:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(path)
            if image is None:
                # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
                image = np.zeros((224, 224, 3), dtype=np.uint8)
            else:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            if not self.lazy_loading:
                return image
            
            # ìºì‹œ ê´€ë¦¬
            if len(self.image_cache) >= self.cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ ì´ë¯¸ì§€ ì œê±°
                oldest_path = self.cache_order.pop(0)
                del self.image_cache[oldest_path]
                gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            
            self.image_cache[path] = image
            self.cache_order.append(path)
        else:
            # ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°
            image = self.image_cache[path]
            # ìºì‹œ ìˆœì„œ ì—…ë°ì´íŠ¸
            self.cache_order.remove(path)
            self.cache_order.append(path)
        
        return image.copy()  # ì›ë³¸ ë³´ì¡´ì„ ìœ„í•œ ë³µì‚¬
    
    def __getitem__(self, idx):
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
        memory_before = self._get_memory_usage()
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image_path = self.image_paths[idx] if idx < len(self.image_paths) else "dummy.jpg"
        image = self._load_image_efficient(image_path)
        
        label = self.labels[idx] if idx < len(self.labels) else 0
        
        # ë³€í™˜ ì ìš©
        if self.transforms:
            try:
                transformed = self.transforms(image=image)
                image = transformed['image']
            except Exception as e:
                print(f"ë³€í™˜ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {idx}): {e}")
                # ê¸°ë³¸ ë³€í™˜ ì ìš©
                image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del image  # ì›ë³¸ ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ í•´ì œ
        
        memory_after = self._get_memory_usage()
        self.memory_usage.append(memory_after - memory_before)
        
        return transformed['image'] if self.transforms else image, torch.tensor(label)
    
    def __len__(self):
        return len(self.image_paths)
    
    def get_memory_stats(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš© í†µê³„ ë°˜í™˜"""
        if not self.memory_usage:
            return {}
        
        return {
            'avg_memory_per_sample': np.mean(self.memory_usage),
            'max_memory_per_sample': np.max(self.memory_usage),
            'total_memory_delta': np.sum(self.memory_usage),
            'cache_size': len(self.image_cache),
            'current_memory_mb': self._get_memory_usage()
        }

# ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
def test_memory_optimization():
    """ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    image_paths = [f"dummy_{i}.jpg" for i in range(200)]
    labels = np.random.randint(0, 10, 200)
    
    transforms = A.Compose([
        A.Resize(224, 224),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])
    
    # ë‘ ê°€ì§€ ì„¤ì • ë¹„êµ
    configs = [
        {'lazy_loading': False, 'cache_size': 0, 'name': 'ìºì‹œ ì—†ìŒ'},
        {'lazy_loading': True, 'cache_size': 50, 'name': 'ìºì‹œ 50ê°œ'},
        {'lazy_loading': True, 'cache_size': 100, 'name': 'ìºì‹œ 100ê°œ'},
    ]
    
    results = {}
    
    for config in configs:
        print(f"\n{config['name']} í…ŒìŠ¤íŠ¸:")
        
        dataset = MemoryOptimizedDataset(
            image_paths=image_paths[:100],  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 100ê°œë§Œ
            labels=labels[:100],
            transforms=transforms,
            lazy_loading=config['lazy_loading'],
            image_cache_size=config['cache_size']
        )
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        initial_memory = dataset._get_memory_usage()
        
        # ëª‡ ê°œ ìƒ˜í”Œ ì²˜ë¦¬
        for i in range(50):
            _ = dataset[i % len(dataset)]
        
        # ë©”ëª¨ë¦¬ í†µê³„
        stats = dataset.get_memory_stats()
        final_memory = dataset._get_memory_usage()
        
        results[config['name']] = {
            'initial_memory': initial_memory,
            'final_memory': final_memory,
            'memory_increase': final_memory - initial_memory,
            **stats
        }
        
        print(f"  ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory:.1f} MB")
        print(f"  ìµœì¢… ë©”ëª¨ë¦¬: {final_memory:.1f} MB")
        print(f"  ë©”ëª¨ë¦¬ ì¦ê°€: {final_memory - initial_memory:.1f} MB")
        if 'avg_memory_per_sample' in stats:
            print(f"  ìƒ˜í”Œë‹¹ í‰ê·  ë©”ëª¨ë¦¬: {stats['avg_memory_per_sample']:.3f} MB")
        if 'cache_size' in stats:
            print(f"  ìºì‹œ í¬ê¸°: {stats['cache_size']}ê°œ")
    
    # ìµœì  ì„¤ì • ì¶”ì²œ
    min_memory_config = min(results.keys(), 
                           key=lambda k: results[k]['memory_increase'])
    print(f"\në©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ê°€ì¥ ì¢‹ì€ ì„¤ì •: {min_memory_config}")

# test_memory_optimization()
print("ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ êµ¬ì¡° í™•ì¸ ì™„ë£Œ")
```

## ğŸ¯ ì‹¤ë¬´ì—ì„œì˜ í™œìš© ì‚¬ë¡€

### ì»´í“¨í„° ë¹„ì „ ëŒ€íšŒ ìµœì í™”

```python
def get_competition_augmentation_strategy(competition_type, validation_score=None):
    """ì»´í“¨í„° ë¹„ì „ ëŒ€íšŒìš© ì¦ê°• ì „ëµ"""
    
    strategies = {
        'image_classification': {
            'light': A.Compose([
                A.HorizontalFlip(p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, p=0.3),
                A.Resize(224, 224),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ]),
            'medium': A.Compose([
                A.HorizontalFlip(p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),
                A.OneOf([
                    A.Rotate(limit=20, p=1.0),
                    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=1.0),
                ], p=0.5),
                A.OneOf([
                    A.GaussNoise(var_limit=(10, 30), p=1.0),
                    A.Blur(blur_limit=3, p=1.0),
                    A.CLAHE(clip_limit=2, p=1.0),
                ], p=0.3),
                A.Cutout(num_holes=1, max_h_size=32, max_w_size=32, p=0.3),
                A.Resize(224, 224),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ]),
            'heavy': A.Compose([
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.2),
                A.RandomRotate90(p=0.3),
                A.OneOf([
                    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1.0),
                    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0),
                    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=1.0),
                ], p=0.8),
                A.OneOf([
                    A.Rotate(limit=30, p=1.0),
                    A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.15, rotate_limit=25, p=1.0),
                    A.Affine(scale=(0.8, 1.2), translate_percent=0.1, rotate=(-20, 20), p=1.0),
                ], p=0.7),
                A.OneOf([
                    A.GaussNoise(var_limit=(10, 50), p=1.0),
                    A.Blur(blur_limit=5, p=1.0),
                    A.MotionBlur(blur_limit=5, p=1.0),
                    A.GaussianBlur(blur_limit=5, p=1.0),
                ], p=0.4),
                A.OneOf([
                    A.Cutout(num_holes=2, max_h_size=48, max_w_size=48, p=1.0),
                    A.GridMask(num_grid=3, p=1.0),
                    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=1.0),
                ], p=0.4),
                A.OneOf([
                    A.ElasticTransform(alpha=50, sigma=5, p=1.0),
                    A.GridDistortion(num_steps=5, distort_limit=0.3, p=1.0),
                    A.OpticalDistortion(distort_limit=0.1, shift_limit=0.1, p=1.0),
                ], p=0.2),
                A.Resize(224, 224),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ])
        },
        'object_detection': {
            'medium': A.Compose([
                A.HorizontalFlip(p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.6),
                A.OneOf([
                    A.Rotate(limit=10, p=1.0),
                    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=1.0),
                ], p=0.4),
                A.OneOf([
                    A.GaussNoise(var_limit=(10, 30), p=1.0),
                    A.Blur(blur_limit=3, p=1.0),
                ], p=0.2),
                A.Resize(512, 512),  # ê°ì²´ íƒì§€ëŠ” ë³´í†µ ë” í° ì´ë¯¸ì§€
            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))
        }
    }
    
    # ê²€ì¦ ì ìˆ˜ì— ë”°ë¥¸ ë™ì  ì¡°ì •
    if validation_score is not None:
        if validation_score < 0.7:
            intensity = 'heavy'
        elif validation_score < 0.85:
            intensity = 'medium'
        else:
            intensity = 'light'
    else:
        intensity = 'medium'
    
    return strategies.get(competition_type, {}).get(intensity)

# Test Time Augmentation (TTA) êµ¬í˜„
class TTAWrapper:
    """Test Time Augmentation ë˜í¼"""
    
    def __init__(self, model, tta_transforms=None, num_augmentations=5):
        self.model = model
        self.num_augmentations = num_augmentations
        
        if tta_transforms is None:
            self.tta_transforms = A.Compose([
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.RandomRotate90(p=0.3),
                A.Rotate(limit=10, p=0.3),
                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
            ])
        else:
            self.tta_transforms = tta_transforms
    
    def predict_with_tta(self, image, device='cpu'):
        """TTAë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
        self.model.eval()
        
        predictions = []
        
        with torch.no_grad():
            # ì›ë³¸ ì´ë¯¸ì§€ ì˜ˆì¸¡
            if isinstance(image, np.ndarray):
                original_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
            else:
                original_tensor = image
            
            original_tensor = original_tensor.unsqueeze(0).to(device)
            original_pred = self.model(original_tensor)
            predictions.append(original_pred.cpu())
            
            # ì¦ê°•ëœ ì´ë¯¸ì§€ë“¤ ì˜ˆì¸¡
            for _ in range(self.num_augmentations - 1):
                try:
                    if isinstance(image, torch.Tensor):
                        # í…ì„œì¸ ê²½ìš° numpyë¡œ ë³€í™˜
                        img_np = image.permute(1, 2, 0).numpy()
                        img_np = (img_np * 255).astype(np.uint8)
                    else:
                        img_np = image
                    
                    # ì¦ê°• ì ìš©
                    augmented = self.tta_transforms(image=img_np)
                    aug_image = augmented['image']
                    
                    # í…ì„œë¡œ ë³€í™˜
                    aug_tensor = torch.from_numpy(aug_image).permute(2, 0, 1).float() / 255.0
                    aug_tensor = aug_tensor.unsqueeze(0).to(device)
                    
                    # ì˜ˆì¸¡
                    aug_pred = self.model(aug_tensor)
                    predictions.append(aug_pred.cpu())
                    
                except Exception as e:
                    print(f"TTA ì¦ê°• ì‹¤íŒ¨: {e}")
                    continue
        
        # ì˜ˆì¸¡ ê²°ê³¼ í‰ê· 
        if predictions:
            avg_prediction = torch.mean(torch.stack(predictions), dim=0)
            return avg_prediction
        else:
            return original_pred.cpu()
    
    def predict_batch_with_tta(self, images, device='cpu'):
        """ë°°ì¹˜ ë‹¨ìœ„ TTA ì˜ˆì¸¡"""
        batch_predictions = []
        
        for image in images:
            pred = self.predict_with_tta(image, device)
            batch_predictions.append(pred)
        
        return torch.stack(batch_predictions)

# ì‚¬ìš© ì˜ˆì‹œ
print("ëŒ€íšŒìš© ì¦ê°• ì „ëµ ë° TTA êµ¬ì¡° í™•ì¸ ì™„ë£Œ")

# ë‹¤ì–‘í•œ ì „ëµ í…ŒìŠ¤íŠ¸
for competition in ['image_classification', 'object_detection']:
    for score in [0.6, 0.8, 0.9]:
        strategy = get_competition_augmentation_strategy(competition, score)
        if strategy:
            print(f"{competition} (ê²€ì¦ì ìˆ˜: {score}): ì¦ê°• ì „ëµ ìƒì„±ë¨")
        else:
            print(f"{competition}: ì§€ì›ë˜ì§€ ì•ŠëŠ” íƒœìŠ¤í¬")

# image_classification (ê²€ì¦ì ìˆ˜: 0.6): ì¦ê°• ì „ëµ ìƒì„±ë¨
# image_classification (ê²€ì¦ì ìˆ˜: 0.8): ì¦ê°• ì „ëµ ìƒì„±ë¨  
# image_classification (ê²€ì¦ì ìˆ˜: 0.9): ì¦ê°• ì „ëµ ìƒì„±ë¨
# object_detection (ê²€ì¦ì ìˆ˜: 0.6): ì¦ê°• ì „ëµ ìƒì„±ë¨
# object_detection (ê²€ì¦ì ìˆ˜: 0.8): ì¦ê°• ì „ëµ ìƒì„±ë¨
# object_detection (ê²€ì¦ì ìˆ˜: 0.9): ì¦ê°• ì „ëµ ìƒì„±ë¨
```

> **Test Time Augmentation (TTA)**ëŠ” ëŒ€íšŒì—ì„œ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë¦¬ëŠ” ê°•ë ¥í•œ ê¸°ë²•ì´ë‹¤. ë™ì¼í•œ ì´ë¯¸ì§€ì— **ì—¬ëŸ¬ ë³€í™˜ì„ ì ìš©í•˜ì—¬ ì˜ˆì¸¡**í•œ í›„ **í‰ê· **ì„ ë‚´ì–´ ë” ì•ˆì •ì ì´ê³  ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. {: .prompt-tip}

## ğŸ“š ë§ˆë¬´ë¦¬

AlbumentationsëŠ” **ë¹ ë¥¸ ì„±ëŠ¥**, **ë‹¤ì–‘í•œ ë³€í™˜**, **ì–´ë…¸í…Œì´ì…˜ ì§€ì›**, **í”„ë ˆì„ì›Œí¬ ë…ë¦½ì„±** ë“±ì˜ ì¥ì ìœ¼ë¡œ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ ì¦ê°• ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë˜ì—ˆë‹¤.

### í•µì‹¬ ìš”ì•½

- **80ê°€ì§€ ì´ìƒì˜ ë³€í™˜**: ê¸°í•˜í•™ì , ìƒ‰ìƒ, ë…¸ì´ì¦ˆ, ë¸”ëŸ¬ ë“± ëª¨ë“  ìœ í˜•ì˜ ì¦ê°• ì§€ì›
- **ì–´ë…¸í…Œì´ì…˜ ë™ì‹œ ë³€í™˜**: Bounding Box, Keypoint, Maskê°€ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì •í™•íˆ ë³€í™˜
- **PyTorch ì™„ë²½ í†µí•©**: ToTensorV2ë¥¼ í†µí•œ seamlessí•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- **ReplayCompose**: ì¬í˜„ ê°€ëŠ¥í•˜ê³  ì¼ê´€ëœ ì¦ê°•ìœ¼ë¡œ ì‹¤í—˜ì˜ ì‹ ë¢°ì„± ë³´ì¥
- **ê³ ì„±ëŠ¥**: OpenCV ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ë¹„ 2-3ë°° ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„

### ì‹¤ë¬´ ì ìš© ê¶Œì¥ì‚¬í•­

**ì´ˆë³´ìë¥¼ ìœ„í•œ ë‹¨ê³„ë³„ ì ‘ê·¼**

1. ê¸°ë³¸ ë³€í™˜ (Flip, Rotate, Resize)ë¶€í„° ì‹œì‘
2. ì ì§„ì ìœ¼ë¡œ ë³µì¡í•œ ë³€í™˜ ì¶”ê°€
3. ê²€ì¦ ë°ì´í„°ë¡œ íš¨ê³¼ í™•ì¸
4. ë„ë©”ì¸ì— ë§ëŠ” íŠ¹í™” íŒŒì´í”„ë¼ì¸ êµ¬ì„±

**ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸**

- ì ì ˆí•œ num_workers ì„¤ì • (ë³´í†µ CPU ì½”ì–´ ìˆ˜)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ìºì‹œ í¬ê¸° ì¡°ì ˆ
- ToTensorV2ë¥¼ ë³€í™˜ íŒŒì´í”„ë¼ì¸ ë§ˆì§€ë§‰ì— ë°°ì¹˜
- ë¶ˆí•„ìš”í•œ ë³€í™˜ ì œê±°ë¡œ ì²˜ë¦¬ ì†ë„ í–¥ìƒ

**ëŒ€íšŒ/í”„ë¡œì íŠ¸ í™œìš©**

- Offline ì¦ê°•: ì œí•œëœ ì»´í“¨íŒ… í™˜ê²½, ë™ì¼ ë°ì´í„° ì¬ì‚¬ìš©
- Online ì¦ê°•: ë¬´í•œ ë°ì´í„° ë‹¤ì–‘ì„±, ì˜¤ë²„í”¼íŒ… ë°©ì§€
- TTA: ìµœì¢… ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì¶”ë¡  ì‹œ ì¦ê°•
- ReplayCompose: ì‹¤í—˜ ì¬í˜„ì„±ê³¼ ì¼ê´€ì„± ë³´ì¥

> AlbumentationsëŠ” ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ì¦ê°• ë„êµ¬ë¥¼ ë„˜ì–´ **ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”**í•˜ëŠ” í•„ìˆ˜ ë„êµ¬ë‹¤. ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•ì„ ìµíˆë©´ **ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°**, **ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ**, **ëŒ€íšŒ ì„±ì  í–¥ìƒ** ë“± ì‹¤ì§ˆì ì¸ ì„±ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. {: .prompt-tip}