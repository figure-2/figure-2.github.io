---
title: ğŸ­ GANs (Generative Adversarial Networks) - ì ëŒ€ì  ìƒì„± ì‹ ê²½ë§ì˜ ì´í•´
date: 2025-07-17 12:58:00 +0900
categories: 
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

- torch==2.0.1
- torchvision==0.15.2
- matplotlib==3.7.1
- numpy==1.24.3
- PIL==9.5.0

## ğŸš€ TL;DR

> **GANsëŠ” ë‘ ê°œì˜ ì‹ ê²½ë§ì´ ì ëŒ€ì ìœ¼ë¡œ ê²½ìŸí•˜ë©° í•™ìŠµí•˜ëŠ” ìƒì„± ëª¨ë¸**ë¡œ, ìœ„ì¡°ì§€íë²”(Generator)ê³¼ ê²½ì°°(Discriminator)ì˜ ê²½ìŸ êµ¬ì¡°ì™€ ê°™ë‹¤. GeneratorëŠ” ì‹¤ì œì™€ êµ¬ë¶„í•  ìˆ˜ ì—†ëŠ” ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ë ¤ í•˜ê³ , DiscriminatorëŠ” ì§„ì§œì™€ ê°€ì§œë¥¼ êµ¬ë³„í•˜ë ¤ í•œë‹¤. ì´ ê³¼ì •ì—ì„œ **VAEë³´ë‹¤ ì„ ëª…í•œ ì´ë¯¸ì§€**ë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ **mode collapse**, **í•™ìŠµ ë¶ˆì•ˆì •ì„±** ë“±ì˜ ë¬¸ì œê°€ ìˆë‹¤. í•˜ì§€ë§Œ **ì„¤ëª… ê°€ëŠ¥ì„±ê³¼ ë‹¤ì–‘í•œ ì‘ìš©** ê°€ëŠ¥ì„±ìœ¼ë¡œ ì¸í•´ í˜„ì¬ê¹Œì§€ë„ ë„ë¦¬ í™œìš©ë˜ëŠ” í•µì‹¬ ìƒì„± ëª¨ë¸ ê¸°ìˆ ì´ë‹¤.

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [GANs Implementation and Training](https://github.com/yuiyeong/notebooks/blob/main/deep_learning/gans_tutorial.ipynb)

## ğŸ­ GANs(Generative Adversarial Networks)ë€?

**Generative Adversarial Networks(GANs)**ëŠ” í•œêµ­ì–´ë¡œ **ì ëŒ€ì  ìƒì„± ì‹ ê²½ë§**ì´ë¼ê³  ë¶ˆë¦¬ë©°, ë‘ ê°œì˜ ì‹ ê²½ë§ì´ ì„œë¡œ ê²½ìŸí•˜ë©° í•™ìŠµí•˜ëŠ” ìƒì„± ëª¨ë¸ì´ë‹¤.

**Generative**(ìƒì„±), **Adversarial**(ì ëŒ€ì ), **Networks**(ì‹ ê²½ë§)ì˜ ì¡°í•©ìœ¼ë¡œ, ì‹ ê²½ë§ë“¤ì´ ì ëŒ€ì ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì˜ë¯¸í•œë‹¤.

> GANsëŠ” ì–€ ë¥´ì¿¤(Yann LeCun)ì´ "ì§€ë‚œ 10ë…„ê°„ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ê°€ì¥ í¥ë¯¸ë¡œìš´ ì•„ì´ë””ì–´"ë¼ê³  í‰ê°€í•  ì •ë„ë¡œ í˜ì‹ ì ì¸ ê¸°ìˆ ì´ë‹¤. {: .prompt-tip}

### GANs vs VAE: ì ‘ê·¼ ë°©ì‹ì˜ ì°¨ì´

**VAE(Variational Autoencoder)**ì™€ **GANs**ëŠ” ëª¨ë‘ ìƒì„± ëª¨ë¸ì´ì§€ë§Œ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì·¨í•œë‹¤.

- **VAE**: ì…ë ¥ ë¶„í¬ë¥¼ ì§ì ‘ ê·¼ì‚¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì •ê·œí™”(regularization)ë¥¼ í†µí•´ ë°ì´í„° ìƒì„± ë°©ë²•ì„ í•™ìŠµ
- **GANs**: ë¶„í¬ë¥¼ ì§ì ‘ ì¶”ì •í•˜ì§€ ì•Šê³ , í•œ ëª¨ë¸ì´ ë‹¤ë¥¸ ëª¨ë¸ì„ ê°€ì´ë“œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ

ì´ëŸ¬í•œ ì°¨ì´ë¡œ ì¸í•´ GANsëŠ” **ë³µì¡í•œ ëª©ì  í•¨ìˆ˜ ì •ì˜ê°€ ë¶ˆí•„ìš”**í•˜ê³  **êµ¬ì¡°ìƒ íŠ¸ë¦­ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤**ëŠ” ì¥ì ì„ ê°€ì§„ë‹¤.

## ğŸ—ï¸ GANsì˜ êµ¬ì¡°: Generatorì™€ Discriminator

GANsëŠ” ë‘ ê°œì˜ ì‹ ê²½ë§ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

### Generator (ìƒì„±ì)

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: Generator êµ¬ì¡°ë„ - ë…¸ì´ì¦ˆ ë²¡í„°ì—ì„œ ì´ë¯¸ì§€ ìƒì„±ê¹Œì§€ì˜ ê³¼ì •]

**Generator**ëŠ” **ìœ„ì¡°ì§€íë²”**ê³¼ ê°™ì€ ì—­í• ì„ í•œë‹¤. ì‹¤ì œ ëˆì„ ì§ì ‘ ë³¸ ì ì€ ì—†ì§€ë§Œ, ê²½ì°°(Discriminator)ì˜ ë°˜ì‘ë§Œ ë³´ê³  ì–´ë–»ê²Œ ë” ì •êµí•œ ìœ„ì¡°ì§€íë¥¼ ë§Œë“¤ì§€ í•™ìŠµí•œë‹¤.

- **ì…ë ¥**: ê°€ìš°ì‹œì•ˆ ë¶„í¬ë‚˜ ê· ë“± ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•œ ë…¸ì´ì¦ˆ ë²¡í„°
- **ì¶œë ¥**: ì‹¤ì œ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ê°€ì§œ ë°ì´í„° (ì˜ˆ: ì´ë¯¸ì§€)
- **êµ¬ì¡°**: Autoencoderì˜ Decoderì™€ ìœ ì‚¬í•œ êµ¬ì¡°

```python
import torch
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_shape=(1, 28, 28)):
        super(Generator, self).__init__()
        
        self.img_shape = img_shape
        
        def block(in_feat, out_feat, normalize=True):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers
        
        self.model = nn.Sequential(
            *block(latent_dim, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),
            nn.Tanh()
        )
    
    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), *self.img_shape)
        return img

# ì‚¬ìš© ì˜ˆì‹œ
latent_dim = 100
generator = Generator(latent_dim)

# ë…¸ì´ì¦ˆ ë²¡í„° ìƒì„±
z = torch.randn(64, latent_dim)  # ë°°ì¹˜ í¬ê¸° 64
fake_images = generator(z)
print(f"ìƒì„±ëœ ì´ë¯¸ì§€ shape: {fake_images.shape}")  # torch.Size([64, 1, 28, 28])
```

### Discriminator (íŒë³„ì)

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: Discriminator êµ¬ì¡°ë„ - ì´ë¯¸ì§€ë¥¼ ë°›ì•„ ì§„ì§œ/ê°€ì§œ íŒë³„í•˜ëŠ” ê³¼ì •]

**Discriminator**ëŠ” **ê²½ì°°**ê³¼ ê°™ì€ ì—­í• ì„ í•œë‹¤. Generatorê°€ ìƒì„±í•œ ê°€ì§œ ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

- **ì…ë ¥**: ì‹¤ì œ ë°ì´í„° ë˜ëŠ” Generatorê°€ ìƒì„±í•œ ê°€ì§œ ë°ì´í„°
- **ì¶œë ¥**: ì…ë ¥ì´ ì‹¤ì œ ë°ì´í„°ì¼ í™•ë¥  (0~1 ì‚¬ì´ì˜ ê°’)
- **êµ¬ì¡°**: ì¼ë°˜ì ì¸ ë¶„ë¥˜ê¸°ì™€ ë™ì¼í•œ êµ¬ì¡°

```python
class Discriminator(nn.Module):
    def __init__(self, img_shape=(1, 28, 28)):
        super(Discriminator, self).__init__()
        
        self.model = nn.Sequential(
            nn.Linear(int(torch.prod(torch.tensor(img_shape))), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity

# ì‚¬ìš© ì˜ˆì‹œ
discriminator = Discriminator()

# ì‹¤ì œ ì´ë¯¸ì§€ì™€ ê°€ì§œ ì´ë¯¸ì§€ íŒë³„
real_images = torch.randn(64, 1, 28, 28)
fake_images = generator(z)

real_validity = discriminator(real_images)
fake_validity = discriminator(fake_images)

print(f"ì‹¤ì œ ì´ë¯¸ì§€ íŒë³„ ê²°ê³¼: {real_validity.mean().item():.4f}")  # 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì§„ì§œë¡œ íŒë³„
print(f"ê°€ì§œ ì´ë¯¸ì§€ íŒë³„ ê²°ê³¼: {fake_validity.mean().item():.4f}")  # 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°€ì§œë¡œ íŒë³„
```

## ğŸ“Š GANsì˜ í•™ìŠµ ì›ë¦¬

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: GANs í•™ìŠµ ê³¼ì • ì‹œê°í™” - ì´ˆë¡ìƒ‰(Generator ë¶„í¬), íŒŒë€ìƒ‰(Discriminator ê²½ê³„), ê²€ì€ìƒ‰(ì‹¤ì œ ë°ì´í„° ë¶„í¬)]

GANsì˜ í•™ìŠµì€ **ë‘ í”Œë ˆì´ì–´ ì œë¡œì„¬ ê²Œì„**ì˜ í˜•íƒœë¡œ ì§„í–‰ëœë‹¤. GeneratorëŠ” ë¶„í¬ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , DiscriminatorëŠ” ì§„ì§œì™€ ê°€ì§œë¥¼ êµ¬ë³„í•˜ëŠ” ê²½ê³„ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

### í•™ìŠµ ê³¼ì •ì˜ ë™ì—­í•™

1. **ì´ˆê¸° ìƒíƒœ**: GeneratorëŠ” ëœë¤í•œ ë¶„í¬, DiscriminatorëŠ” ë¶€ì •í™•í•œ íŒë³„
2. **Generator ê°œì„ **: Discriminatorë¥¼ ì†ì´ê¸° ìœ„í•´ ë” ì‹¤ì œê°™ì€ ë°ì´í„° ìƒì„±
3. **Discriminator ê°œì„ **: ë” ì •êµí•œ íŒë³„ ëŠ¥ë ¥ íšë“
4. **ê· í˜•ì  ë„ë‹¬**: Generatorê°€ ì‹¤ì œ ë¶„í¬ë¥¼ ì™„ë²½íˆ ëª¨ì‚¬

> GeneratorëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ì§ì ‘ ê´€ì°°í•˜ì§€ ì•Šê³  Discriminatorì˜ í”¼ë“œë°±ë§Œì„ í™œìš©í•˜ê¸° ë•Œë¬¸ì—, **Discriminatorì˜ í’ˆì§ˆì´ GANs ì„±ëŠ¥ì„ ì¢Œìš°í•œë‹¤**. {: .prompt-tip}

## ğŸ¯ GANsì˜ ëª©ì  í•¨ìˆ˜

GANsì˜ ëª©ì  í•¨ìˆ˜ëŠ” **minimax ê²Œì„**ì˜ í˜•íƒœë¡œ ì •ì˜ëœë‹¤.

### ìˆ˜í•™ì  í‘œí˜„

$$ \min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))] $$

ì—¬ê¸°ì„œ:

- **D(x)**: Discriminatorê°€ ì‹¤ì œ ë°ì´í„° xì— ëŒ€í•´ ì¶œë ¥í•˜ëŠ” í™•ë¥ 
- **G(z)**: Generatorê°€ ë…¸ì´ì¦ˆ zë¡œë¶€í„° ìƒì„±í•œ ë°ì´í„°
- **ì²« ë²ˆì§¸ í•­**: ì‹¤ì œ ë°ì´í„°ë¥¼ ì§„ì§œë¡œ íŒë³„í•˜ëŠ” ê²ƒì„ ìµœëŒ€í™”
- **ë‘ ë²ˆì§¸ í•­**: ê°€ì§œ ë°ì´í„°ë¥¼ ê°€ì§œë¡œ íŒë³„í•˜ëŠ” ê²ƒì„ ìµœëŒ€í™”

### Discriminatorì˜ ëª©ì 

DiscriminatorëŠ” ìœ„ ëª©ì  í•¨ìˆ˜ë¥¼ **ìµœëŒ€í™”**í•œë‹¤.

- **ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•´**: $\log D(x)$ë¥¼ ìµœëŒ€í™” â†’ D(x) = 1ë¡œ ë§Œë“¤ë ¤ í•¨
- **ê°€ì§œ ë°ì´í„°ì— ëŒ€í•´**: $\log(1 - D(G(z)))$ë¥¼ ìµœëŒ€í™” â†’ D(G(z)) = 0ìœ¼ë¡œ ë§Œë“¤ë ¤ í•¨

### Generatorì˜ ëª©ì 

GeneratorëŠ” ê°™ì€ ëª©ì  í•¨ìˆ˜ë¥¼ **ìµœì†Œí™”**í•œë‹¤.

- **ê°€ì§œ ë°ì´í„°ì— ëŒ€í•´**: $\log(1 - D(G(z)))$ë¥¼ ìµœì†Œí™” â†’ D(G(z)) = 1ë¡œ ë§Œë“¤ë ¤ í•¨

### ì‹¤ì œ êµ¬í˜„ì—ì„œì˜ ê°œì„ 

ì›ë˜ ëª©ì  í•¨ìˆ˜ëŠ” í•™ìŠµ ì´ˆê¸°ì— **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ**ë¥¼ ì¼ìœ¼í‚¨ë‹¤. ë”°ë¼ì„œ ì‹¤ì œë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •ëœë‹¤.

$$ \max_G \mathbb{E}_{z \sim p_z(z)}[\log D(G(z))] $$

```python
import torch.nn.functional as F

def train_step(generator, discriminator, real_images, latent_dim, device):
    batch_size = real_images.size(0)
    
    # ì‹¤ì œ ì´ë¯¸ì§€ì™€ ê°€ì§œ ì´ë¯¸ì§€ ë ˆì´ë¸”
    real_labels = torch.ones(batch_size, 1).to(device)
    fake_labels = torch.zeros(batch_size, 1).to(device)
    
    # ===== Discriminator í•™ìŠµ =====
    discriminator.zero_grad()
    
    # ì‹¤ì œ ì´ë¯¸ì§€ì— ëŒ€í•œ ì†ì‹¤
    real_outputs = discriminator(real_images)
    real_loss = F.binary_cross_entropy(real_outputs, real_labels)
    
    # ê°€ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ ì†ì‹¤
    z = torch.randn(batch_size, latent_dim).to(device)
    fake_images = generator(z)
    fake_outputs = discriminator(fake_images.detach())  # generator ê¸°ìš¸ê¸° ì°¨ë‹¨
    fake_loss = F.binary_cross_entropy(fake_outputs, fake_labels)
    
    # Discriminator ì „ì²´ ì†ì‹¤
    d_loss = real_loss + fake_loss
    d_loss.backward()
    
    # ===== Generator í•™ìŠµ =====
    generator.zero_grad()
    
    # ìˆ˜ì •ëœ ëª©ì  í•¨ìˆ˜ ì‚¬ìš©
    fake_outputs = discriminator(fake_images)
    g_loss = F.binary_cross_entropy(fake_outputs, real_labels)  # ê°€ì§œë¥¼ ì§„ì§œë¡œ ì†ì´ê¸°
    g_loss.backward()
    
    return d_loss.item(), g_loss.item()
```

## âš ï¸ GANsì˜ ì£¼ìš” ë¬¸ì œì ë“¤

### Mode Collapse (ëª¨ë“œ ë¶•ê´´)

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: Mode Collapse ì˜ˆì‹œ - ë‹¤ì–‘í•œ ìˆ«ì ëŒ€ì‹  0ë§Œ ìƒì„±í•˜ëŠ” ì˜ˆì‹œ]

**Mode Collapse**ëŠ” Generatorê°€ ì‹¤ì œ ë°ì´í„°ì˜ ì¼ë¶€ ëª¨ë“œ(mode)ë§Œ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘ì„±ì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œë‹¤.

```python
# Mode Collapse íƒì§€ ì˜ˆì‹œ
def detect_mode_collapse(generated_samples, threshold=0.1):
    """
    ìƒì„±ëœ ìƒ˜í”Œë“¤ì˜ ë‹¤ì–‘ì„±ì„ ì¸¡ì •í•˜ì—¬ mode collapse íƒì§€
    """
    # ê° ìƒ˜í”Œ ê°„ì˜ í‰ê·  ê±°ë¦¬ ê³„ì‚°
    distances = []
    for i in range(len(generated_samples)):
        for j in range(i+1, len(generated_samples)):
            dist = torch.norm(generated_samples[i] - generated_samples[j])
            distances.append(dist.item())
    
    avg_distance = sum(distances) / len(distances)
    
    if avg_distance < threshold:
        print("âš ï¸ Mode Collapse ì˜ì‹¬! ìƒì„±ëœ ìƒ˜í”Œë“¤ì´ ë„ˆë¬´ ìœ ì‚¬í•©ë‹ˆë‹¤.")
        return True
    else:
        print("âœ… ì ì ˆí•œ ë‹¤ì–‘ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
        return False

# ì‚¬ìš© ì˜ˆì‹œ
z_samples = torch.randn(100, latent_dim)
generated_samples = generator(z_samples)
detect_mode_collapse(generated_samples)
```

> Mode CollapseëŠ” Discriminatorê°€ ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ìƒì„±í•˜ë„ë¡ ì¶©ë¶„í•œ í”¼ë“œë°±ì„ ì œê³µí•˜ì§€ ëª»í•  ë•Œ ë°œìƒí•œë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì •ê·œí™” ê¸°ë²•ê³¼ ëª©ì  í•¨ìˆ˜ ê°œì„ ì´ ì—°êµ¬ë˜ê³  ìˆë‹¤. {: .prompt-warning}

### í•™ìŠµ ë¶ˆì•ˆì •ì„±

GANsëŠ” ë‘ ë„¤íŠ¸ì›Œí¬ì˜ ê· í˜•ì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆë‹¤.

```python
def balanced_training(generator, discriminator, d_optimizer, g_optimizer, 
                     real_images, latent_dim, k=1):
    """
    ê· í˜•ì¡íŒ GANs í•™ìŠµ
    k: Discriminatorë¥¼ ëª‡ ë²ˆ ë” í•™ìŠµì‹œí‚¬ì§€ ê²°ì •
    """
    
    # Discriminatorë¥¼ kë²ˆ í•™ìŠµ
    for _ in range(k):
        d_loss, _ = train_step(generator, discriminator, real_images, latent_dim, device)
        d_optimizer.step()
    
    # Generatorë¥¼ 1ë²ˆ í•™ìŠµ
    _, g_loss = train_step(generator, discriminator, real_images, latent_dim, device)
    g_optimizer.step()
    
    return d_loss, g_loss
```

> ì‹¤ì œë¡œëŠ” K=1ì„ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤. Discriminatorê°€ ë„ˆë¬´ ì•ì„œê°€ë©´ Generatorì—ê²Œ ìœ ì˜ë¯¸í•œ ê¸°ìš¸ê¸°ë¥¼ ì œê³µí•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. {: .prompt-tip}

## ğŸš€ GANsì˜ ê°œì„  ë°©í–¥ê³¼ ë³€í˜•ë“¤

### ë‹¤ì–‘í•œ ëª©ì  í•¨ìˆ˜

ê¸°ë³¸ GANsëŠ” **Jensen-Shannon Divergence**ë¥¼ ìµœì í™”í•˜ëŠ” ê²ƒê³¼ ë™ì¹˜ë‹¤. ì´ë¥¼ ë‹¤ë¥¸ ê±°ë¦¬ ì¸¡ë„ë¡œ ë°”ê¾¸ì–´ ê°œì„ í•  ìˆ˜ ìˆë‹¤.

- **WGAN**: Wasserstein Distance ì‚¬ìš©
- **LSGAN**: Least Squares Loss ì‚¬ìš©
- **f-GAN**: ì¼ë°˜í™”ëœ f-divergence í”„ë ˆì„ì›Œí¬

### êµ¬ì¡°ì  ê°œì„ 

- **DCGAN**: Convolutional layersë¥¼ í™œìš©í•œ ì•ˆì •ì  í•™ìŠµ
- **Progressive GAN**: ì ì§„ì ìœ¼ë¡œ í•´ìƒë„ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” í•™ìŠµ
- **StyleGAN**: ìŠ¤íƒ€ì¼ì„ ì œì–´í•  ìˆ˜ ìˆëŠ” ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±

## ğŸ¨ GANs vs VAE: ê²°ê³¼ë¬¼ ë¹„êµ

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: GANsì™€ VAE ìƒì„± ê²°ê³¼ ë¹„êµ ì´ë¯¸ì§€]

### GANsì˜ íŠ¹ì§•

- **ì¥ì **: ì„ ëª…í•˜ê³  ëšœë ·í•œ ì´ë¯¸ì§€ ìƒì„±
- **ë‹¨ì **: ë•Œë¡œëŠ” ë¹„í˜„ì‹¤ì ì¸ ì™œê³¡ì´ë‚˜ ì•„í‹°íŒ©íŠ¸ í¬í•¨

### VAEì˜ íŠ¹ì§•

- **ì¥ì **: ë¶€ë“œëŸ½ê³  ì•ˆì •ì ì¸ ì´ë¯¸ì§€, ì ì¬ ê³µê°„ì˜ ì—°ì†ì„±
- **ë‹¨ì **: ìƒëŒ€ì ìœ¼ë¡œ íë¦¿í•œ ê²°ê³¼ë¬¼

```python
# ë‘ ëª¨ë¸ì˜ ê²°ê³¼ ë¹„êµ ì½”ë“œ
def compare_models(vae, gan_generator, test_input):
    """
    VAEì™€ GANì˜ ìƒì„± ê²°ê³¼ë¥¼ ë¹„êµ
    """
    with torch.no_grad():
        # VAE ê²°ê³¼
        vae_output, _, _ = vae(test_input)
        
        # GAN ê²°ê³¼  
        z = torch.randn(test_input.size(0), latent_dim)
        gan_output = gan_generator(z)
        
        # ì‹œê°í™” ì½”ë“œ
        fig, axes = plt.subplots(2, 4, figsize=(12, 6))
        
        for i in range(4):
            # VAE ê²°ê³¼
            axes[0, i].imshow(vae_output[i].squeeze(), cmap='gray')
            axes[0, i].set_title('VAE')
            axes[0, i].axis('off')
            
            # GAN ê²°ê³¼
            axes[1, i].imshow(gan_output[i].squeeze(), cmap='gray')
            axes[1, i].set_title('GAN')
            axes[1, i].axis('off')
        
        plt.tight_layout()
        plt.show()

# ì‚¬ìš© ì˜ˆì‹œ
# compare_models(vae_model, generator, test_images)
```

## ğŸŒŸ GANsì˜ ì‹¤ì œ í™œìš© ì‚¬ë¡€

### ì´ë¯¸ì§€ ìƒì„± ë° í¸ì§‘

- **DeepFake**: ì–¼êµ´ ë°”ê¾¸ê¸° ê¸°ìˆ 
- **Style Transfer**: ì˜ˆìˆ  ì‘í’ˆ ìŠ¤íƒ€ì¼ ë³€í™˜
- **Super Resolution**: ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ê³ í•´ìƒë„ë¡œ ë³€í™˜

### ë°ì´í„° ì¦ê°•

- **ì˜ë£Œ ì˜ìƒ**: í¬ê·€ ì§ˆë³‘ ë°ì´í„° ìƒì„±ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ë¶€ì¡± í•´ê²°
- **ììœ¨ì£¼í–‰**: ë‹¤ì–‘í•œ ì£¼í–‰ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ìƒì„±

### ì°½ì‘ ì§€ì›

- **ê²Œì„ ì‚°ì—…**: ê²Œì„ ë‚´ ìºë¦­í„°, ë°°ê²½ ìë™ ìƒì„±
- **íŒ¨ì…˜**: ìƒˆë¡œìš´ ë””ìì¸ íŒ¨í„´ ìƒì„±
- **ìŒì•…**: ìƒˆë¡œìš´ ë©œë¡œë”” ìƒì„±

## ğŸ’¡ GANs êµ¬í˜„ ì‹œ ì‹¤ë¬´ íŒ

### í•™ìŠµ ì•ˆì •í™” ê¸°ë²•

```python
# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
def get_lr_scheduler(optimizer, step_size=50, gamma=0.5):
    return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# ëª¨ë¸ì— ì ìš©
generator.apply(weights_init)
discriminator.apply(weights_init)
```

### í•™ìŠµ ëª¨ë‹ˆí„°ë§

```python
def monitor_training(d_losses, g_losses, epoch):
    """
    í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§
    """
    # ì†ì‹¤ ê¸°ë¡
    print(f"Epoch {epoch}: D_loss={d_losses[-1]:.4f}, G_loss={g_losses[-1]:.4f}")
    
    # ì†ì‹¤ ê· í˜• ì²´í¬
    if len(d_losses) > 10:
        recent_d_loss = sum(d_losses[-10:]) / 10
        recent_g_loss = sum(g_losses[-10:]) / 10
        
        if recent_d_loss < 0.1:
            print("âš ï¸ Discriminatorê°€ ë„ˆë¬´ ê°•í•¨ - Generator í•™ìŠµë¥  ì¦ê°€ ê³ ë ¤")
        elif recent_g_loss < 0.1:
            print("âš ï¸ Generatorê°€ ë„ˆë¬´ ê°•í•¨ - Discriminator í•™ìŠµë¥  ì¦ê°€ ê³ ë ¤")
```

## ğŸ”® GANsì˜ ë¯¸ë˜ì™€ ìµœì‹  ë™í–¥

GANsëŠ” í˜„ì¬ê¹Œì§€ë„ í™œë°œíˆ ì—°êµ¬ë˜ê³  ìˆìœ¼ë©°, **Diffusion Model**ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ í’ˆì§ˆê³¼ ì†ë„ ì¸¡ë©´ì—ì„œ ë”ìš± ë°œì „í•˜ê³  ìˆë‹¤.

### ìµœì‹  ì—°êµ¬ ë°©í–¥

- **ì¡°ê±´ë¶€ ìƒì„±**: íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„° ìƒì„±
- **ë‹¤ì¤‘ ëª¨ë‹¬**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€, ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹° ì—°ê²°
- **íš¨ìœ¨ì„± ê°œì„ **: ì ì€ ë°ì´í„°ë¡œë„ ê³ í’ˆì§ˆ ê²°ê³¼ ìƒì„±

> GANsëŠ” ë‹¨ìˆœíˆ ì´ë¯¸ì§€ ìƒì„±ì„ ë„˜ì–´ì„œ **ì°½ì˜ì  AI**, **ë°ì´í„° í”„ë¼ì´ë²„ì‹œ**, **ì‹œë®¬ë ˆì´ì…˜** ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•µì‹¬ ê¸°ìˆ ë¡œ ìë¦¬ì¡ê³  ìˆë‹¤. {: .prompt-tip}

GANsëŠ” ìƒì„± ëª¨ë¸ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¾¼ í˜ì‹ ì ì¸ ê¸°ìˆ ë¡œ, ì´ë¡ ì  ì´í•´ì™€ í•¨ê»˜ ì‹¤ì œ êµ¬í˜„ì„ í†µí•´ ê·¸ ê°€ëŠ¥ì„±ì„ ì²´í—˜í•´ë³¼ ìˆ˜ ìˆë‹¤. ë¹„ë¡ í•™ìŠµì˜ ì–´ë ¤ì›€ì´ ìˆì§€ë§Œ, ì ì ˆí•œ ê¸°ë²•ë“¤ì„ í™œìš©í•˜ë©´ ë†€ë¼ìš´ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë„êµ¬ë‹¤.