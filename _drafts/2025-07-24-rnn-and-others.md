---
title: "ğŸ§  RNNê³¼ ê·¸ ì¹œêµ¬ë“¤: ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆ"
date: 2025-07-24 14:13:00 +0900
categories: 
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

- torch==2.0.0+
- numpy==1.24.0
- matplotlib==3.7.0

## ğŸš€ TL;DR

> - **RNN(Recurrent Neural Network)**ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ, ì´ì „ ì‹œê°„ì˜ ì •ë³´ë¥¼ í˜„ì¬ ì‹œê°„ì— ì „ë‹¬í•˜ì—¬ ìˆœì°¨ì  ë°ì´í„°ë¥¼ í•™ìŠµí•œë‹¤
> - ìì—°ì–´ ì²˜ë¦¬ëŠ” ëŒ€ë¶€ë¶„ **ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°**ë¡œ ì´ë£¨ì–´ì§€ë©°, ì¸ì½”ë”ëŠ” ì…ë ¥ì„ ì´í•´í•˜ê³  ë””ì½”ë”ëŠ” ì¶œë ¥ì„ ìƒì„±í•œë‹¤
> - RNNì˜ ì¹˜ëª…ì  ë‹¨ì ì¸ **Gradient Vanishing/Exploding ë¬¸ì œ**ë¡œ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ê°€ ì–´ë µë‹¤
> - **LSTM**ì€ Cell Stateì™€ 3ê°œì˜ Gate(Forget, Input, Output)ë¥¼ í†µí•´ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤
> - **GRU**ëŠ” LSTMì„ ë‹¨ìˆœí™”í•œ êµ¬ì¡°ë¡œ, 2ê°œì˜ Gate(Update, Reset)ë§Œ ì‚¬ìš©í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë†’ì˜€ë‹¤
> - PyTorchëŠ” ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì„ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ë‚´ì¥ ëª¨ë“ˆì„ ì œê³µí•œë‹¤ {: .prompt-tip}

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- w.i.p.

## ğŸ”„ ì‹œí€€ìŠ¤ì™€ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°

### ì‹œí€€ìŠ¤(Sequence)ë€?

ì‹œí€€ìŠ¤ëŠ” **ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°**ë¥¼ ì˜ë¯¸í•œë‹¤. ìì—°ì–´ëŠ” ë‹¨ì–´ë“¤ì´ íŠ¹ì • ìˆœì„œë¡œ ë°°ì—´ë˜ì–´ ì˜ë¯¸ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ëŒ€í‘œì ì¸ ì‹œí€€ìŠ¤ ë°ì´í„°ë‹¤. "ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤"ì™€ "ë°¥ì„ ë‚˜ëŠ” ë¨¹ëŠ”ë‹¤"ëŠ” ê°™ì€ ë‹¨ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆì§€ë§Œ ìˆœì„œê°€ ë‹¤ë¥´ë©´ ì˜ë¯¸ë‚˜ ìì—°ìŠ¤ëŸ¬ì›€ì´ ë‹¬ë¼ì§„ë‹¤.

### ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ì˜ ì´í•´

ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ëŠ” **Sequence-to-Sequence(Seq2Seq)** ëª¨ë¸ì˜ í•µì‹¬ì´ë‹¤. ì´ êµ¬ì¡°ë¥¼ ë²ˆì—­ ì‘ì—…ìœ¼ë¡œ ë¹„ìœ í•˜ë©´:

- **ì¸ì½”ë”(Encoder)**: ì…ë ¥ ë¬¸ì¥ì„ ì´í•´í•˜ëŠ” ë¶€ë¶„ (ìì—°ì–´ ì´í•´, NLU)
- **ë””ì½”ë”(Decoder)**: ì´í•´í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œë ¥ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ë¶€ë¶„ (ìì—°ì–´ ìƒì„±, NLG)

ì˜ˆë¥¼ ë“¤ì–´, "She is eating a green apple"ì„ ì¤‘êµ­ì–´ë¡œ ë²ˆì—­í•  ë•Œ:

1. ì¸ì½”ë”ê°€ ì˜ì–´ ë¬¸ì¥ì„ ì´í•´í•˜ì—¬ **ì»¨í…ìŠ¤íŠ¸ ë²¡í„°(Context Vector)**ë¡œ ì••ì¶•
2. ë””ì½”ë”ê°€ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ ë°›ì•„ ì¤‘êµ­ì–´ ë¬¸ì¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨]

### í•µì‹¬ ê°œë… ì •ë¦¬

- **ì»¨í…ìŠ¤íŠ¸ ë²¡í„°**: ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì •ë³´ë¥¼ ì••ì¶•í•œ ê³ ì • ê¸¸ì´ ë²¡í„°
- **ì˜¤í† ì¸ì½”ë”©(Auto-encoding)**: ì…ë ¥ì„ ì´í•´í•˜ëŠ” ë°©ì‹ (BERT ë“±ì—ì„œ ì‚¬ìš©)
- **ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ(Auto-regressive)**: ìˆœì°¨ì ìœ¼ë¡œ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë°©ì‹ (GPT ë“±ì—ì„œ ì‚¬ìš©)

> ìì—°ì–´ ì²˜ë¦¬ì˜ ê±°ì˜ ëª¨ë“  ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ëŠ” ì¸ì½”ë”ë§Œ, ë””ì½”ë”ë§Œ, ë˜ëŠ” ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡° ì¤‘ í•˜ë‚˜ë¥¼ ë”°ë¥¸ë‹¤. ì´ëŠ” ìì—°ì–´ ì²˜ë¦¬ì˜ ê°€ì¥ ê¸°ì´ˆ ì¤‘ì˜ ê¸°ì´ˆë‹¤! {: .prompt-tip}

## ğŸ” RNN (Recurrent Neural Network)

### RNNì´ í•„ìš”í•œ ì´ìœ 

ì–¸ì–´ëŠ” **ì‹œê³„ì—´ì  íŠ¹ì„±**ì„ ê°€ì§„ë‹¤. ìš°ë¦¬ê°€ ë§í•˜ê±°ë‚˜ ê¸€ì„ ì“¸ ë•Œ, ë‹¨ì–´ë“¤ì´ ìˆœì„œëŒ€ë¡œ ë‚˜íƒ€ë‚˜ë©° ì´ì „ ë‹¨ì–´ê°€ ë‹¤ìŒ ë‹¨ì–´ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤. RNNì€ ì´ëŸ¬í•œ ìˆœì°¨ì  ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆë‹¤.

ì´ë¯¸ì§€ì™€ ë‹¬ë¦¬ í…ìŠ¤íŠ¸ëŠ” ìˆœì„œê°€ ì¤‘ìš”í•˜ë‹¤. "ê°œê°€ ê³ ì–‘ì´ë¥¼ ì«“ëŠ”ë‹¤"ì™€ "ê³ ì–‘ì´ê°€ ê°œë¥¼ ì«“ëŠ”ë‹¤"ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤. RNNì€ ì´ëŸ° ìˆœì„œ ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

### RNNì˜ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬


RNNì˜ í•µì‹¬ì€ **ì´ì „ ì‹œê°„ì˜ ì •ë³´ë¥¼ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì „ë‹¬**í•˜ëŠ” ê²ƒì´ë‹¤.

#### ìˆ˜í•™ì  í‘œí˜„

í˜„ì¬ ì‹œê°„ tì—ì„œì˜ hidden stateëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤:

$$ h_t = g_1(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t + b_h) $$

$$ y_t = g_2(W_{hy} \cdot h_t + b_y) $$

ì—¬ê¸°ì„œ:

- $h_t$: í˜„ì¬ ì‹œê°„ì˜ hidden state (RNNì˜ ë©”ëª¨ë¦¬ ì—­í• )
- $h_{t-1}$: ì´ì „ ì‹œê°„ì˜ hidden state
- $x_t$: í˜„ì¬ ì‹œê°„ì˜ ì…ë ¥
- $y_t$: í˜„ì¬ ì‹œê°„ì˜ ì¶œë ¥
- $W_{hh}, W_{xh}, W_{hy}$: í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜
- $g_1$: í™œì„±í™” í•¨ìˆ˜ (ì£¼ë¡œ tanh)
- $g_2$: ì¶œë ¥ í™œì„±í™” í•¨ìˆ˜ (íƒœìŠ¤í¬ì— ë”°ë¼ ë‹¤ë¦„)

#### Python ì½”ë“œë¡œ êµ¬í˜„

```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        
        # RNN ë ˆì´ì–´ ì •ì˜
        self.rnn = nn.RNN(input_size, hidden_size, 
                          nonlinearity='tanh', batch_first=True)
        # ì¶œë ¥ì„ ìœ„í•œ ì„ í˜• ë³€í™˜
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # ì´ˆê¸° hidden stateë¥¼ 0ìœ¼ë¡œ ì„¤ì •
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        
        # RNNì— ì…ë ¥ ì „ë‹¬
        out, hidden = self.rnn(x, h0)
        
        # ë§ˆì§€ë§‰ ì‹œê°„ ë‹¨ê³„ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©
        out = self.fc(out[:, -1, :])
        return out

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ ì˜ˆì‹œ
model = SimpleRNN(input_size=10, hidden_size=20, output_size=2)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# ë”ë¯¸ ë°ì´í„°ë¡œ í•™ìŠµ
for epoch in range(100):
    # ì…ë ¥ ë°ì´í„° (ë°°ì¹˜í¬ê¸°=32, ì‹œí€€ìŠ¤ê¸¸ì´=10, ì…ë ¥ì°¨ì›=10)
    inputs = torch.randn(32, 10, 10)
    targets = torch.randint(0, 2, (32,))
    
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')
# ì¶œë ¥: Epoch 0, Loss: 0.7234
# ì¶œë ¥: Epoch 10, Loss: 0.5123
# ...
```

> RNNì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ëŠ” ì‹œê°„ ë‹¨ê³„ë³„ë¡œ ê³µìœ ëœë‹¤. ì¦‰, ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ì •ë³´ë¥¼ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•œë‹¤! {: .prompt-tip}

### RNNì˜ ì¢…ë¥˜

RNNì€ ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê´€ê³„ì— ë”°ë¼ 4ê°€ì§€ í˜•íƒœë¡œ êµ¬ë¶„ëœë‹¤:

#### 1. One-to-One

- ì „í†µì ì¸ ì‹ ê²½ë§ê³¼ ë™ì¼ (ì…ë ¥ í•˜ë‚˜, ì¶œë ¥ í•˜ë‚˜)
- ì˜ˆì‹œ: ì´ë¯¸ì§€ ë¶„ë¥˜

#### 2. One-to-Many

- í•˜ë‚˜ì˜ ì…ë ¥ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ì¶œë ¥ ìƒì„±
- ì˜ˆì‹œ: ì´ë¯¸ì§€ ìº¡ì…”ë‹ (ì´ë¯¸ì§€ â†’ ì„¤ëª… ë¬¸ì¥)

#### 3. Many-to-One

- ì—¬ëŸ¬ ì…ë ¥ì„ ë°›ì•„ í•˜ë‚˜ì˜ ì¶œë ¥ ìƒì„±
- ì˜ˆì‹œ: ê°ì • ë¶„ì„ (ë¬¸ì¥ â†’ ê¸ì •/ë¶€ì •)

#### 4. Many-to-Many

- ì—¬ëŸ¬ ì…ë ¥ì„ ë°›ì•„ ì—¬ëŸ¬ ì¶œë ¥ ìƒì„±
- ì˜ˆì‹œ: ê¸°ê³„ ë²ˆì—­, í˜•íƒœì†Œ ë¶„ì„

### RNNì˜ í•™ìŠµ: BPTT

RNNì€ **Backpropagation Through Time (BPTT)**ë¥¼ í†µí•´ í•™ìŠµí•œë‹¤. ì´ëŠ” ì‹œê°„ ìˆœì„œë¥¼ ì—­ìœ¼ë¡œ ë”°ë¼ê°€ë©´ì„œ ì˜¤ì°¨ë¥¼ ì „íŒŒì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤.

```python
# BPTT ê°œë… ì‹œê°í™”
def visualize_bptt():
    """
    ì‹œê°„ t=3ì—ì„œ ë°œìƒí•œ ì˜¤ì°¨ê°€ 
    t=2, t=1, t=0ìœ¼ë¡œ ì—­ì „íŒŒë˜ëŠ” ê³¼ì •
    """
    sequence_length = 4
    for t in reversed(range(sequence_length)):
        print(f"ì‹œê°„ {t}: ì˜¤ì°¨ ì—­ì „íŒŒ ì¤‘...")
        # gradient = compute_gradient_at_time_t()
        # update_weights(gradient)
# ì¶œë ¥: ì‹œê°„ 3: ì˜¤ì°¨ ì—­ì „íŒŒ ì¤‘...
# ì¶œë ¥: ì‹œê°„ 2: ì˜¤ì°¨ ì—­ì „íŒŒ ì¤‘...
# ì¶œë ¥: ì‹œê°„ 1: ì˜¤ì°¨ ì—­ì „íŒŒ ì¤‘...
# ì¶œë ¥: ì‹œê°„ 0: ì˜¤ì°¨ ì—­ì „íŒŒ ì¤‘...
```

## âš ï¸ RNNì˜ ì¹˜ëª…ì  ë‹¨ì : Gradient Vanishing/Exploding

### ë¬¸ì œì˜ ì›ì¸

RNNì´ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ë•Œ ë°œìƒí•˜ëŠ” ê°€ì¥ í° ë¬¸ì œëŠ” **ê¸°ìš¸ê¸° ì†Œì‹¤(Gradient Vanishing)** ë˜ëŠ” **ê¸°ìš¸ê¸° í­ë°œ(Gradient Exploding)**ì´ë‹¤.

ì´ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ì´ìœ :

1. RNNì€ í™œì„±í™” í•¨ìˆ˜ë¡œ tanhë¥¼ ì‚¬ìš©
2. tanhì˜ ë¯¸ë¶„ê°’ì€ 0ê³¼ 1 ì‚¬ì´
3. BPTT ê³¼ì •ì—ì„œ ì´ ê°’ë“¤ì´ ê³„ì† ê³±í•´ì§
4. ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì›Œì§ (Vanishing) ë˜ëŠ” ë¬´í•œëŒ€ë¡œ ë°œì‚° (Exploding)

### ìˆ˜í•™ì  ì´í•´

tanh í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ ë²”ìœ„ê°€ (0, 1)ì´ë¯€ë¡œ, ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ nì¼ ë•Œ:

$$ \frac{\partial L}{\partial h_0} = \prod_{t=1}^{n} \frac{\partial h_t}{\partial h_{t-1}} $$

ë§Œì•½ ê° ë¯¸ë¶„ê°’ì´ 0.5ë¼ë©´, 20ë‹¨ê³„ í›„ì—ëŠ”: $$ 0.5^{20} \approx 0.00000095 $$

ì´ë ‡ê²Œ ì‘ì€ ê°’ìœ¼ë¡œëŠ” ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ê°€ ê±°ì˜ ë¶ˆê°€ëŠ¥í•˜ë‹¤!

### ì‹¤í—˜: ê¸´ ì‹œí€€ìŠ¤ì—ì„œ RNN ì„±ëŠ¥ ì €í•˜ í™•ì¸

```python
def test_long_sequence():
    # ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (ê¸¸ì´ 10000)
    sequence_length = 10000
    x = torch.randn(1, sequence_length, 10)
    
    # RNN ëª¨ë¸ë¡œ í•™ìŠµ
    rnn_model = SimpleRNN(10, 20, 2)
    
    # í•™ìŠµ ê³¼ì •ì—ì„œ gradient norm ì¸¡ì •
    optimizer = torch.optim.Adam(rnn_model.parameters())
    
    for epoch in range(100):
        optimizer.zero_grad()
        output = rnn_model(x)
        loss = output.sum()
        loss.backward()
        
        # gradient norm ê³„ì‚°
        total_norm = 0
        for p in rnn_model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        
        print(f"Epoch {epoch}: Gradient Norm = {total_norm:.6f}")
        # ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ gradient normì´ ë§¤ìš° ì‘ì•„ì§
        
        optimizer.step()
```

> RNNì€ ì§§ì€ ì‹œí€€ìŠ¤ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ë§Œ, ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§„ë‹¤. ì´ê²ƒì´ LSTMê³¼ GRUê°€ ë“±ì¥í•œ ì´ìœ ë‹¤! {: .prompt-warning}

## ğŸ§® LSTM (Long Short-Term Memory)

### LSTMì˜ í•µì‹¬ ì•„ì´ë””ì–´

LSTMì€ RNNì˜ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **"ì¤‘ìš”í•œ ì •ë³´ëŠ” ê¸°ì–µí•˜ê³ , ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ìŠì–´ë²„ë¦¬ì"**ëŠ” ê²ƒì´ë‹¤.

ì¸ê°„ë„ ëª¨ë“  ì •ë³´ë¥¼ ê¸°ì–µí•˜ì§€ ì•ŠëŠ”ë‹¤. ì¤‘ìš”í•œ ê²ƒë§Œ ì„ íƒì ìœ¼ë¡œ ê¸°ì–µí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìŠì–´ë²„ë¦°ë‹¤. LSTMë„ ì´ì™€ ê°™ì€ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•œë‹¤.

### LSTMì˜ êµ¬ì¡°

LSTMì€ **Cell State**ì™€ **3ê°œì˜ Gate**ë¡œ êµ¬ì„±ëœë‹¤:

#### 1. Cell State ($C_t$)

- LSTMì˜ ì¥ê¸° ê¸°ì–µ ì €ì¥ì†Œ
- ì •ë³´ê°€ ê±°ì˜ ë³€í•˜ì§€ ì•Šê³  ì „ë‹¬ë¨

#### 2. Forget Gate ($f_t$)

- ì´ì „ Cell Stateì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ìŠì„ì§€ ê²°ì •
- ìˆ˜ì‹: $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ 0(ì™„ì „íˆ ìŠê¸°)ê³¼ 1(ì™„ì „íˆ ê¸°ì–µ) ì‚¬ì´ ê°’ ì¶œë ¥

#### 3. Input Gate ($i_t$)

- ìƒˆë¡œìš´ ì •ë³´ ì¤‘ ì–´ë–¤ ê²ƒì„ Cell Stateì— ì €ì¥í• ì§€ ê²°ì •
- ìˆ˜ì‹: $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
- í›„ë³´ Cell State: $\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$

#### 4. Output Gate ($o_t$)

- Cell Stateì˜ ì–´ëŠ ë¶€ë¶„ì„ ì¶œë ¥í• ì§€ ê²°ì •
- ìˆ˜ì‹: $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$

### LSTMì˜ ì „ì²´ ë™ì‘ ê³¼ì •

```python
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        # ì¶œë ¥ ë ˆì´ì–´
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # ì´ˆê¸° hidden stateì™€ cell state
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        
        # LSTM ì „ë‹¬ (ì…ë ¥ê³¼ ì´ˆê¸° ìƒíƒœ ëª¨ë‘ ì „ë‹¬)
        out, (hidden, cell) = self.lstm(x, (h0, c0))
        
        # ë§ˆì§€ë§‰ ì¶œë ¥
        out = self.fc(out[:, -1, :])
        return out

# LSTM í•™ìŠµ ì˜ˆì‹œ
lstm_model = LSTM(10, 20, 2)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.01)

# ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ í•™ìŠµ
long_sequence = torch.randn(32, 100, 10)  # ê¸¸ì´ 100ì˜ ì‹œí€€ìŠ¤
targets = torch.randint(0, 2, (32,))

for epoch in range(300):
    optimizer.zero_grad()
    outputs = lstm_model(long_sequence)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    
    if epoch % 30 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')
# ì¶œë ¥: Epoch 0, Loss: 0.6931
# ì¶œë ¥: Epoch 30, Loss: 0.4123
# ì¶œë ¥: Epoch 60, Loss: 0.2341
# ... LSTMì€ ê¸´ ì‹œí€€ìŠ¤ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë¨
```

### Cell State ì—…ë°ì´íŠ¸ ê³¼ì •

Cell StateëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì—…ë°ì´íŠ¸ëœë‹¤:

$$ C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t $$

- $f_t \odot C_{t-1}$: Forget Gateë¥¼ í†µí•´ ì´ì „ ì •ë³´ ì¤‘ ì¼ë¶€ë¥¼ ìŠìŒ
- $i_t \odot \tilde{C}_t$: Input Gateë¥¼ í†µí•´ ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€

Hidden StateëŠ”: $$ h_t = o_t \odot \tanh(C_t) $$

[ì‹œê°ì  í‘œí˜„ ë„£ê¸°: LSTM ê²Œì´íŠ¸ ë™ì‘ ê³¼ì • ë‹¤ì´ì–´ê·¸ë¨]

> LSTMì˜ í•µì‹¬ì€ Cell Stateê°€ ê±°ì˜ ë³€í•˜ì§€ ì•Šê³  ì „ë‹¬ë˜ë¯€ë¡œ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œê°€ ì™„í™”ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í•„ìš”í•œ ì •ë³´ë§Œ ì„ íƒì ìœ¼ë¡œ ìœ ì§€í•œë‹¤! {: .prompt-tip}

## ğŸšª GRU (Gated Recurrent Unit)

### GRUì˜ ë“±ì¥ ë°°ê²½

GRUëŠ” ì¡°ê²½í˜„ êµìˆ˜ë‹˜ì´ ê°œë°œí•œ ëª¨ë¸ë¡œ, LSTMì˜ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”í–ˆë‹¤. "LSTMì€ ì¢‹ì§€ë§Œ ë„ˆë¬´ ë³µì¡í•˜ë‹¤. ë” ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ ìˆ˜ ì—†ì„ê¹Œ?"ë¼ëŠ” ì§ˆë¬¸ì—ì„œ ì‹œì‘ë˜ì—ˆë‹¤.

### GRU vs LSTM ë¹„êµ

|íŠ¹ì§•|LSTM|GRU|
|---|---|---|
|Gate ê°œìˆ˜|3ê°œ (Forget, Input, Output)|2ê°œ (Update, Reset)|
|State ê°œìˆ˜|2ê°œ (Hidden, Cell)|1ê°œ (Hiddenë§Œ)|
|íŒŒë¼ë¯¸í„° ìˆ˜|ë§ìŒ|ì ìŒ|
|ê³„ì‚° ë³µì¡ë„|ë†’ìŒ|ë‚®ìŒ|
|ì¥ê¸° ì˜ì¡´ì„±|ë§¤ìš° ìš°ìˆ˜|ìš°ìˆ˜|

### GRUì˜ êµ¬ì¡°

#### 1. Update Gate ($z_t$)

- LSTMì˜ Forget Gateì™€ Input Gateë¥¼ í•©ì¹œ ì—­í• 
- ì´ì „ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í•˜ê³ , ìƒˆ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë°›ì„ì§€ ê²°ì •
- ìˆ˜ì‹: $z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$

#### 2. Reset Gate ($r_t$)

- ì´ì „ hidden stateë¥¼ ì–¼ë§ˆë‚˜ ë¬´ì‹œí• ì§€ ê²°ì •
- ìˆ˜ì‹: $r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$

### GRU êµ¬í˜„

```python
class GRU(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(GRU, self).__init__()
        self.hidden_size = hidden_size
        
        # GRU ë ˆì´ì–´
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        # ì¶œë ¥ ë ˆì´ì–´
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # ì´ˆê¸° hidden stateë§Œ í•„ìš” (cell state ì—†ìŒ)
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        
        # GRU ì „ë‹¬
        out, hidden = self.gru(x, h0)
        
        # ë§ˆì§€ë§‰ ì¶œë ¥
        out = self.fc(out[:, -1, :])
        return out

# GRU í•™ìŠµ
gru_model = GRU(10, 20, 2)
# ë‚˜ë¨¸ì§€ í•™ìŠµ ì½”ë“œëŠ” LSTMê³¼ ë™ì¼
```

### Hidden State ì—…ë°ì´íŠ¸

GRUì˜ hidden state ì—…ë°ì´íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

1. í›„ë³´ hidden state ê³„ì‚°: $$ \tilde{h}_t = \tanh(W \cdot [r_t \odot h_{t-1}, x_t]) $$
    
2. ìµœì¢… hidden state: $$ h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t $$
    

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ: RNN vs LSTM vs GRU

### ì‹¤í—˜: ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì„±ëŠ¥ ë¹„êµ

```python
def compare_models_on_long_sequence():
    # ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ ìƒì„± (ê¸¸ì´ 20000)
    sequence_length = 20000
    batch_size = 100
    
    # ëœë¤ ì…ë ¥ ìƒì„±
    X = torch.randn(batch_size, sequence_length, 1)
    # íƒ€ê²Ÿ: í‰ê· ì´ 0ë³´ë‹¤ í¬ë©´ 1, ì•„ë‹ˆë©´ 0
    y = (X.mean(dim=1) > 0).long().squeeze()
    
    # ì„¸ ëª¨ë¸ ìƒì„±
    rnn = SimpleRNN(1, 50, 2)
    lstm = LSTM(1, 50, 2)
    gru = GRU(1, 50, 2)
    
    models = {'RNN': rnn, 'LSTM': lstm, 'GRU': gru}
    results = {}
    
    # ê° ëª¨ë¸ í•™ìŠµ
    for name, model in models.items():
        optimizer = torch.optim.Adam(model.parameters())
        criterion = nn.CrossEntropyLoss()
        losses = []
        
        for epoch in range(100):
            optimizer.zero_grad()
            outputs = model(X)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
            
            if epoch % 20 == 0:
                print(f'{name} - Epoch {epoch}: Loss = {loss.item():.4f}')
        
        results[name] = losses
    
    # ê²°ê³¼ ì‹œê°í™”
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(10, 6))
    for name, losses in results.items():
        plt.plot(losses, label=name)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Long Sequence Learning: RNN vs LSTM vs GRU')
    plt.legend()
    plt.grid(True)
    plt.show()

# ì‹¤í–‰ ê²°ê³¼:
# RNN - Epoch 0: Loss = 0.6931
# RNN - Epoch 20: Loss = 0.5234
# RNN - Epoch 80: Loss = 0.2145  # ìˆ˜ë ´ì´ ëŠë¦¼
# 
# LSTM - Epoch 0: Loss = 0.6681
# LSTM - Epoch 20: Loss = 0.3421
# LSTM - Epoch 80: Loss = 0.1023  # ë” ë‚®ì€ loss
# 
# GRU - Epoch 0: Loss = 0.6623
# GRU - Epoch 20: Loss = 0.3123
# GRU - Epoch 80: Loss = 0.0934   # ê°€ì¥ ë‚®ì€ loss
```

### íŠ¹ì„± ë¹„êµí‘œ

|íŠ¹ì„±|RNN|LSTM|GRU|
|---|---|---|---|
|**Long-term Dependency**|Poor|Excellent|Good|
|**Gradient Vanishing**|Severe|Minimal|Minimal|
|**Computational Complexity**|Low|High|Medium|
|**Parameter Count**|Least|Most|Medium|
|**Training Speed**|Fast|Slow|Medium|
|**Memory Usage**|Low|High|Medium|

## ğŸ¯ ë‹¤ì–‘í•œ RNN êµ¬ì¡° ì‹¤ìŠµ

### 1. One-to-One: ì œê³± ì˜ˆì¸¡

```python
def one_to_one_example():
    """ì…ë ¥ ìˆ«ìì˜ ì œê³±ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸"""
    # ë°ì´í„° ìƒì„±
    X = torch.randint(1, 5, (1000,)).float().unsqueeze(-1).unsqueeze(-1)
    y = (X.squeeze() ** 2).long()
    
    # ëª¨ë¸ í•™ìŠµ
    model = SimpleRNN(1, 10, 20)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters())
    
    for epoch in range(1000):
        optimizer.zero_grad()
        outputs = model(X).squeeze()
        loss = criterion(outputs.float(), y.float())
        loss.backward()
        optimizer.step()
        
        if epoch % 100 == 0:
            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')
    
    # í…ŒìŠ¤íŠ¸
    test_input = torch.tensor([[[2.0]]])
    prediction = model(test_input)
    print(f'ì…ë ¥: 2.0, ì˜ˆì¸¡: {prediction.item():.2f}, ì •ë‹µ: 4.0')
    # ì¶œë ¥: ì…ë ¥: 2.0, ì˜ˆì¸¡: 3.98, ì •ë‹µ: 4.0
```

### 2. One-to-Many: ë°°ìˆ˜ ìƒì„±

```python
class OneToManyRNN(nn.Module):
    """í•˜ë‚˜ì˜ ìˆ«ìë¥¼ ë°›ì•„ ê·¸ ë°°ìˆ˜ 10ê°œë¥¼ ìƒì„±"""
    def __init__(self, input_size, hidden_size, output_size, seq_length):
        super().__init__()
        self.hidden_size = hidden_size
        self.seq_length = seq_length
        
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # ì…ë ¥ì„ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ë°˜ë³µ
        x = x.repeat(1, self.seq_length, 1)
        
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.rnn(x, h0)
        
        # ëª¨ë“  ì‹œê°„ ë‹¨ê³„ì—ì„œ ì¶œë ¥
        out = self.fc(out)
        return out

# ì‚¬ìš© ì˜ˆì‹œ
model = OneToManyRNN(1, 20, 1, 10)
input_num = torch.tensor([[[3.0]]])  # ì…ë ¥: 3
# ëª©í‘œ ì¶œë ¥: [3, 6, 9, 12, 15, 18, 21, 24, 27, 30]
```

### 3. Many-to-One: ê°ì • ë¶„ì„

```python
class ManyToOneRNN(nn.Module):
    """ì—¬ëŸ¬ ë‹¨ì–´ë¥¼ ë°›ì•„ í•˜ë‚˜ì˜ ê°ì • ë ˆì´ë¸” ì¶œë ¥"""
    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        embedded = self.embedding(x)
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.rnn(embedded, h0)
        
        # ë§ˆì§€ë§‰ ì‹œê°„ ë‹¨ê³„ë§Œ ì‚¬ìš©
        out = self.fc(out[:, -1, :])
        return torch.sigmoid(out)

# ê°ì • ë¶„ì„ ì˜ˆì‹œ
model = ManyToOneRNN(vocab_size=1000, embedding_dim=50, 
                      hidden_size=100, output_size=1)
# ì…ë ¥: ë¬¸ì¥ì˜ ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤
# ì¶œë ¥: 0(ë¶€ì •) ë˜ëŠ” 1(ê¸ì •)
```

### 4. Many-to-Many: í˜•íƒœì†Œ ë¶„ì„

```python
class ManyToManyRNN(nn.Module):
    """ê° ë‹¨ì–´ì— ëŒ€í•œ í’ˆì‚¬ íƒœê¹…"""
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_tags):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_tags)
    
    def forward(self, x):
        embedded = self.embedding(x)
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.rnn(embedded, h0)
        
        # ëª¨ë“  ì‹œê°„ ë‹¨ê³„ì—ì„œ ì¶œë ¥
        out = self.fc(out)
        return out

# í’ˆì‚¬ íƒœê¹… ì˜ˆì‹œ
model = ManyToManyRNN(vocab_size=1000, embedding_dim=50, 
                       hidden_size=100, num_tags=10)
# ì…ë ¥: ["ë‚˜ëŠ”", "ë°¥ì„", "ë¨¹ëŠ”ë‹¤"] â†’ [23, 45, 67]
# ì¶œë ¥: ["ëŒ€ëª…ì‚¬", "ëª…ì‚¬", "ë™ì‚¬"] â†’ [1, 2, 3]
```

## ğŸ”¬ Gradient ë¬¸ì œ ì‹¤í—˜

### Gradient Vanishing ì‹œê°í™”

```python
def visualize_gradient_vanishing():
    """RNNì—ì„œ gradient vanishing í˜„ìƒ ê´€ì°°"""
    import matplotlib.pyplot as plt
    
    sequence_lengths = [10, 50, 100, 500, 1000]
    models = {'RNN': SimpleRNN, 'LSTM': LSTM, 'GRU': GRU}
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for idx, (name, Model) in enumerate(models.items()):
        gradient_norms = []
        
        for seq_len in sequence_lengths:
            model = Model(10, 20, 2)
            X = torch.randn(1, seq_len, 10)
            y = torch.tensor([1])
            
            # Forward pass
            output = model(X)
            loss = nn.CrossEntropyLoss()(output, y)
            
            # Backward pass
            loss.backward()
            
            # Calculate gradient norm
            total_norm = 0
            for p in model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** 0.5
            
            gradient_norms.append(total_norm)
            
        axes[idx].plot(sequence_lengths, gradient_norms, 'o-')
        axes[idx].set_xlabel('Sequence Length')
        axes[idx].set_ylabel('Gradient Norm')
        axes[idx].set_title(f'{name} Gradient Flow')
        axes[idx].set_yscale('log')
        axes[idx].grid(True)
    
    plt.tight_layout()
    plt.show()
    
    # RNNì€ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ gradientê°€ ê¸‰ê²©íˆ ê°ì†Œ
    # LSTMê³¼ GRUëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì 
```

## ğŸ’¡ ì‹¤ì „ íŒê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

- **ì§§ì€ ì‹œí€€ìŠ¤ (< 100)**: RNNë„ ì¶©ë¶„í•  ìˆ˜ ìˆìŒ
- **ì¤‘ê°„ ì‹œí€€ìŠ¤ (100-500)**: GRU ì¶”ì²œ (ë¹ ë¥´ê³  íš¨ìœ¨ì )
- **ê¸´ ì‹œí€€ìŠ¤ (> 500)**: LSTM ì¶”ì²œ (ë” ì•ˆì •ì )
- **ì‹¤ì‹œê°„ ì²˜ë¦¬ í•„ìš”**: GRU (ê³„ì‚°ì´ ë¹ ë¦„)
- **ìµœê³  ì„±ëŠ¥ í•„ìš”**: LSTM (ë³µì¡í•˜ì§€ë§Œ ê°•ë ¥)

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
def hyperparameter_guide():
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°"""
    
    # Hidden size ì„ íƒ
    # - ì‘ì€ ë°ì´í„°: 32-64
    # - ì¤‘ê°„ ë°ì´í„°: 64-128
    # - í° ë°ì´í„°: 128-512
    
    # Learning rate
    # - RNN: 0.01-0.001
    # - LSTM/GRU: 0.001-0.0001
    
    # Batch size
    # - ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„ì—ì„œ ìµœëŒ€í•œ í¬ê²Œ
    # - ì¼ë°˜ì ìœ¼ë¡œ 32, 64, 128
    
    # Dropout (ê³¼ì í•© ë°©ì§€)
    model = nn.LSTM(input_size=100, hidden_size=256, 
                    num_layers=2, dropout=0.2, batch_first=True)
    
    # Gradient clipping (gradient exploding ë°©ì§€)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### Bidirectional RNN

ì–‘ë°©í–¥ RNNì€ ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•œë‹¤:

```python
class BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        # bidirectional=True ì„¤ì •
        self.lstm = nn.LSTM(input_size, hidden_size, 
                           batch_first=True, bidirectional=True)
        # hidden_size * 2 (ì–‘ë°©í–¥ì´ë¯€ë¡œ)
        self.fc = nn.Linear(hidden_size * 2, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(2, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(2, x.size(0), self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out
```

## ğŸ“ í•µì‹¬ ì •ë¦¬

RNNê³¼ ê·¸ ë³€í˜•ë“¤ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆë‹¤. ê° ëª¨ë¸ì˜ íŠ¹ì§•ì„ ì •ë¦¬í•˜ë©´:

### RNN

- **ì¥ì **: êµ¬ì¡°ê°€ ê°„ë‹¨í•˜ê³  ë¹ ë¦„
- **ë‹¨ì **: ê¸´ ì‹œí€€ìŠ¤ì—ì„œ gradient vanishing ë¬¸ì œ
- **ì‚¬ìš©ì²˜**: ì§§ì€ ì‹œí€€ìŠ¤, ì‹¤ì‹œê°„ ì²˜ë¦¬

### LSTM

- **ì¥ì **: ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°, ê°€ì¥ ì•ˆì •ì 
- **ë‹¨ì **: ë³µì¡í•˜ê³  ëŠë¦¼, ë§ì€ ë©”ëª¨ë¦¬ í•„ìš”
- **ì‚¬ìš©ì²˜**: ê¸´ ì‹œí€€ìŠ¤, ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ ê²½ìš°

### GRU

- **ì¥ì **: LSTMë³´ë‹¤ ê°„ë‹¨í•˜ë©´ì„œë„ ì„±ëŠ¥ ìš°ìˆ˜
- **ë‹¨ì **: LSTMë³´ë‹¤ ì•½ê°„ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
- **ì‚¬ìš©ì²˜**: ì¤‘ê°„ ê¸¸ì´ ì‹œí€€ìŠ¤, íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°

> í˜„ì¬ëŠ” Transformerê°€ ì£¼ë¥˜ê°€ ë˜ì—ˆì§€ë§Œ, RNN ê³„ì—´ì€ ì—¬ì „íˆ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ì˜ ê¸°ì´ˆì´ë©°, íŠ¹íˆ ë¦¬ì†ŒìŠ¤ê°€ ì œí•œì ì¸ í™˜ê²½ì—ì„œëŠ” ì—¬ì „íˆ ìœ ìš©í•˜ë‹¤. ë˜í•œ Transformerë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œë„ RNNì˜ í•œê³„ì™€ í•´ê²° ë°©ë²•ì„ ì•„ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤! {: .prompt-tip}

## ğŸ”— ì¶”ê°€ í•™ìŠµ ìë£Œ

- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [PyTorch RNN Tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)


## RNN (Recurrent Neural Network) ë„ì‹í™”

### RNN - ìˆ˜ì‹ íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TD
    subgraph "ì‹œê°„ tì—ì„œì˜ RNN ì—°ì‚°"
        Xt[/"ì…ë ¥ x_t"/]
        Ht_1[/"ì´ì „ hidden state h_{t-1}"/]
        
        Xt --> Wxh["W_xh Â· x_t"]
        Ht_1 --> Whh["W_hh Â· h_{t-1}"]
        
        Wxh --> Add1["+"]
        Whh --> Add1
        
        Add1 --> Bh["+ b_h"]
        Bh --> Tanh["tanh()"]
        Tanh --> Ht["h_t (í˜„ì¬ hidden state)"]
        
        Ht --> Why["W_hy Â· h_t"]
        Why --> By["+ b_y"]
        By --> Yt["y_t (ì¶œë ¥)"]
        
        Ht --> Next["ë‹¤ìŒ ì‹œê°„ t+1ë¡œ ì „ë‹¬"]
    end
    
    style Xt fill:#e1f5fe
    style Ht_1 fill:#fff3e0
    style Ht fill:#c8e6c9
    style Yt fill:#ffcdd2
```

### RNN - ì˜ë¯¸ì  ë„ì‹í™”

```mermaid
graph LR
    subgraph "RNNì˜ ì •ë³´ íë¦„"
        subgraph T1["ì‹œê°„ t-1"]
            I1[/"ì…ë ¥ 1"/]
            H1["ê¸°ì–µ 1"]
            O1[/"ì¶œë ¥ 1"/]
            I1 --> H1
            H1 --> O1
        end
        
        subgraph T2["ì‹œê°„ t"]
            I2[/"ì…ë ¥ 2"/]
            H2["ê¸°ì–µ 2"]
            O2[/"ì¶œë ¥ 2"/]
            I2 --> H2
            H2 --> O2
        end
        
        subgraph T3["ì‹œê°„ t+1"]
            I3[/"ì…ë ¥ 3"/]
            H3["ê¸°ì–µ 3"]
            O3[/"ì¶œë ¥ 3"/]
            I3 --> H3
            H3 --> O3
        end
        
        H1 -.->|"ì´ì „ ì •ë³´ ì „ë‹¬"| H2
        H2 -.->|"ì´ì „ ì •ë³´ ì „ë‹¬"| H3
    end
    
    style I1 fill:#e3f2fd
    style I2 fill:#e3f2fd
    style I3 fill:#e3f2fd
    style H1 fill:#fff9c4
    style H2 fill:#fff9c4
    style H3 fill:#fff9c4
    style O1 fill:#ffebee
    style O2 fill:#ffebee
    style O3 fill:#ffebee
```

## LSTM (Long Short-Term Memory) ë„ì‹í™”

### LSTM - ìˆ˜ì‹ íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TD
    subgraph "LSTM Cell at time t"
        Xt[/"ì…ë ¥ x_t"/]
        Ht_1[/"h_{t-1}"/]
        Ct_1[/"C_{t-1}"/]
        
        subgraph Forget["Forget Gate"]
            Concat1["[h_{t-1}, x_t]"]
            Xt --> Concat1
            Ht_1 --> Concat1
            Concat1 --> Wf["W_f Â· [h_{t-1}, x_t] + b_f"]
            Wf --> Sigmoidf["Ïƒ (sigmoid)"]
            Sigmoidf --> ft["f_t âˆˆ [0,1]"]
        end
        
        subgraph Input["Input Gate"]
            Concat2["[h_{t-1}, x_t]"]
            Xt --> Concat2
            Ht_1 --> Concat2
            Concat2 --> Wi["W_i Â· [h_{t-1}, x_t] + b_i"]
            Wi --> Sigmoidi["Ïƒ (sigmoid)"]
            Sigmoidi --> it["i_t âˆˆ [0,1]"]
            
            Concat2 --> WC["W_C Â· [h_{t-1}, x_t] + b_C"]
            WC --> TanhC["tanh"]
            TanhC --> Ct_tilde["CÌƒ_t âˆˆ [-1,1]"]
        end
        
        subgraph Update["Cell State Update"]
            Ct_1 --> Mult1["âŠ™"]
            ft --> Mult1
            
            it --> Mult2["âŠ™"]
            Ct_tilde --> Mult2
            
            Mult1 --> Add["+"]
            Mult2 --> Add
            Add --> Ct["C_t"]
        end
        
        subgraph Output["Output Gate"]
            Concat3["[h_{t-1}, x_t]"]
            Xt --> Concat3
            Ht_1 --> Concat3
            Concat3 --> Wo["W_o Â· [h_{t-1}, x_t] + b_o"]
            Wo --> Sigmoido["Ïƒ (sigmoid)"]
            Sigmoido --> ot["o_t âˆˆ [0,1]"]
            
            Ct --> TanhOut["tanh"]
            TanhOut --> Mult3["âŠ™"]
            ot --> Mult3
            Mult3 --> Ht["h_t"]
        end
        
        Ht --> Yt["ì¶œë ¥ y_t"]
    end
    
    style Xt fill:#e1f5fe
    style Ct_1 fill:#fff3e0
    style Ht_1 fill:#fff3e0
    style Ct fill:#c8e6c9
    style Ht fill:#c8e6c9
    style Yt fill:#ffcdd2
```

### LSTM - ì˜ë¯¸ì  ë„ì‹í™”

```mermaid
graph TB
    subgraph "LSTMì˜ ì •ë³´ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜"
        Input[/"ìƒˆë¡œìš´ ì •ë³´ ì…ë ¥"/]
        
        subgraph Gates["ê²Œì´íŠ¸ ì‹œìŠ¤í…œ"]
            Forget["ğŸ—‘ï¸ Forget Gate<br/>ë¬´ì—‡ì„ ìŠì„ê¹Œ?"]
            InputG["ğŸ“¥ Input Gate<br/>ë¬´ì—‡ì„ ì €ì¥í• ê¹Œ?"]
            OutputG["ğŸ“¤ Output Gate<br/>ë¬´ì—‡ì„ ì¶œë ¥í• ê¹Œ?"]
        end
        
        subgraph Memory["ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ"]
            LongTerm["ğŸ“š Cell State<br/>(ì¥ê¸° ê¸°ì–µ)"]
            ShortTerm["ğŸ’­ Hidden State<br/>(ë‹¨ê¸° ê¸°ì–µ/ì‘ì—… ë©”ëª¨ë¦¬)"]
        end
        
        Input --> Forget
        Input --> InputG
        Input --> OutputG
        
        Forget -->|"ì„ íƒì  ë§ê°"| LongTerm
        InputG -->|"ì„ íƒì  ì €ì¥"| LongTerm
        LongTerm -->|"í•„í„°ë§ëœ ì •ë³´"| OutputG
        OutputG -->|"í˜„ì¬ ê´€ë ¨ ì •ë³´"| ShortTerm
        
        ShortTerm --> Output[/"í˜„ì¬ ì¶œë ¥"/]
        ShortTerm -.->|"ë‹¤ìŒ ì‹œê°„ìœ¼ë¡œ"| NextTime[/"t+1"/]
        LongTerm -.->|"ì¥ê¸° ê¸°ì–µ ì „ë‹¬"| NextTime
    end
    
    style Input fill:#e3f2fd
    style Forget fill:#ffecb3
    style InputG fill:#c5e1a5
    style OutputG fill:#b3e5fc
    style LongTerm fill:#fff9c4
    style ShortTerm fill:#d1c4e9
    style Output fill:#ffcdd2
```

## GRU (Gated Recurrent Unit) ë„ì‹í™”

### GRU - ìˆ˜ì‹ íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TD
    subgraph "GRU Cell at time t"
        Xt[/"ì…ë ¥ x_t"/]
        Ht_1[/"h_{t-1}"/]
        
        subgraph Reset["Reset Gate"]
            ConcatR["[h_{t-1}, x_t]"]
            Xt --> ConcatR
            Ht_1 --> ConcatR
            ConcatR --> Wr["W_r Â· [h_{t-1}, x_t] + b_r"]
            Wr --> SigmoidR["Ïƒ (sigmoid)"]
            SigmoidR --> rt["r_t âˆˆ [0,1]"]
        end
        
        subgraph Update["Update Gate"]
            ConcatZ["[h_{t-1}, x_t]"]
            Xt --> ConcatZ
            Ht_1 --> ConcatZ
            ConcatZ --> Wz["W_z Â· [h_{t-1}, x_t] + b_z"]
            Wz --> SigmoidZ["Ïƒ (sigmoid)"]
            SigmoidZ --> zt["z_t âˆˆ [0,1]"]
        end
        
        subgraph Candidate["Candidate Hidden State"]
            rt --> MultR["âŠ™"]
            Ht_1 --> MultR
            MultR --> ConcatH["[r_t âŠ™ h_{t-1}, x_t]"]
            Xt --> ConcatH
            ConcatH --> Wh["W_h Â· [r_t âŠ™ h_{t-1}, x_t] + b_h"]
            Wh --> TanhH["tanh"]
            TanhH --> Ht_tilde["hÌƒ_t"]
        end
        
        subgraph Final["Final Hidden State"]
            zt --> Inv["1 - z_t"]
            Inv --> MultPrev["âŠ™"]
            Ht_1 --> MultPrev
            
            zt --> MultNew["âŠ™"]
            Ht_tilde --> MultNew
            
            MultPrev --> AddFinal["+"]
            MultNew --> AddFinal
            AddFinal --> Ht["h_t"]
        end
        
        Ht --> Yt["ì¶œë ¥ y_t"]
        Ht --> Next["ë‹¤ìŒ ì‹œê°„ t+1ë¡œ"]
    end
    
    style Xt fill:#e1f5fe
    style Ht_1 fill:#fff3e0
    style Ht fill:#c8e6c9
    style Yt fill:#ffcdd2
```

### GRU - ì˜ë¯¸ì  ë„ì‹í™”

```mermaid
graph LR
    subgraph "GRUì˜ ë‹¨ìˆœí™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬"
        Input[/"í˜„ì¬ ì…ë ¥"/]
        PrevMemory[/"ì´ì „ ê¸°ì–µ"/]
        
        subgraph Control["ì œì–´ ë©”ì»¤ë‹ˆì¦˜"]
            Reset["ğŸ”„ Reset Gate<br/>ì´ì „ ê¸°ì–µì„<br/>ì–¼ë§ˆë‚˜ ë¬´ì‹œ?"]
            Update["âš–ï¸ Update Gate<br/>ìƒˆ ì •ë³´ vs ì´ì „ ì •ë³´<br/>ë¹„ìœ¨ ê²°ì •"]
        end
        
        subgraph Processing["ì •ë³´ ì²˜ë¦¬"]
            Candidate["ğŸ¯ í›„ë³´ ê¸°ì–µ<br/>ë¦¬ì…‹ëœ ì´ì „ ì •ë³´ +<br/>í˜„ì¬ ì…ë ¥"]
            Mixing["ğŸ”€ ì •ë³´ í˜¼í•©<br/>(1-z)Ã—ì´ì „ + zÃ—ìƒˆë¡œìš´"]
        end
        
        Input --> Reset
        Input --> Update
        PrevMemory --> Reset
        PrevMemory --> Update
        
        Reset -->|"ì„ íƒì  ë¦¬ì…‹"| Candidate
        Input --> Candidate
        
        Update -->|"í˜¼í•© ë¹„ìœ¨"| Mixing
        PrevMemory -->|"ìœ ì§€í•  ë¶€ë¶„"| Mixing
        Candidate -->|"ì¶”ê°€í•  ë¶€ë¶„"| Mixing
        
        Mixing --> NewMemory["ğŸ“ ìƒˆë¡œìš´ ê¸°ì–µ"]
        NewMemory --> Output[/"í˜„ì¬ ì¶œë ¥"/]
        NewMemory -.->|"ë‹¤ìŒ ì‹œê°„ìœ¼ë¡œ"| NextTime[/"t+1"/]
    end
    
    style Input fill:#e3f2fd
    style PrevMemory fill:#fff3e0
    style Reset fill:#ffecb3
    style Update fill:#c5e1a5
    style Candidate fill:#f8bbd0
    style Mixing fill:#b39ddb
    style NewMemory fill:#c8e6c9
    style Output fill:#ffcdd2
```

## ì„¸ ëª¨ë¸ì˜ í•µì‹¬ ì°¨ì´ì  ë¹„êµ

```mermaid
graph TD
    subgraph "ëª¨ë¸ë³„ ì •ë³´ íë¦„ ë¹„êµ"
        subgraph RNN["RNN"]
            RNN_In[/"ì…ë ¥"/] --> RNN_H["Hidden State"]
            RNN_H --> RNN_Out[/"ì¶œë ¥"/]
            RNN_H -.-> RNN_Next["ë‹¤ìŒ ì‹œê°„"]
        end
        
        subgraph LSTM["LSTM"]
            LSTM_In[/"ì…ë ¥"/] --> LSTM_Gates["3ê°œ ê²Œì´íŠ¸<br/>(Forget, Input, Output)"]
            LSTM_Gates --> LSTM_Cell["Cell State<br/>(ì¥ê¸° ê¸°ì–µ)"]
            LSTM_Gates --> LSTM_Hidden["Hidden State<br/>(ë‹¨ê¸° ê¸°ì–µ)"]
            LSTM_Hidden --> LSTM_Out[/"ì¶œë ¥"/]
            LSTM_Cell -.-> LSTM_Next["ë‹¤ìŒ ì‹œê°„"]
            LSTM_Hidden -.-> LSTM_Next
        end
        
        subgraph GRU["GRU"]
            GRU_In[/"ì…ë ¥"/] --> GRU_Gates["2ê°œ ê²Œì´íŠ¸<br/>(Reset, Update)"]
            GRU_Gates --> GRU_Hidden["Hidden State<br/>(í†µí•© ê¸°ì–µ)"]
            GRU_Hidden --> GRU_Out[/"ì¶œë ¥"/]
            GRU_Hidden -.-> GRU_Next["ë‹¤ìŒ ì‹œê°„"]
        end
    end
    
    RNN_Note["ë‹¨ìˆœí•˜ì§€ë§Œ<br/>ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ"]
    LSTM_Note["ë³µì¡í•˜ì§€ë§Œ<br/>ê°•ë ¥í•œ ì¥ê¸° ê¸°ì–µ"]
    GRU_Note["ê· í˜•ì¡íŒ<br/>íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥"]
    
    RNN --> RNN_Note
    LSTM --> LSTM_Note
    GRU --> GRU_Note
    
    style RNN_In fill:#e3f2fd
    style LSTM_In fill:#e3f2fd
    style GRU_In fill:#e3f2fd
    style RNN_Out fill:#ffcdd2
    style LSTM_Out fill:#ffcdd2
    style GRU_Out fill:#ffcdd2
```