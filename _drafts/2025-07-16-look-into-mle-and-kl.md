---
title: "ì´ë¯¸ì§€ ìƒì„±ëª¨ë¸ì˜ í•µì‹¬: ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •(MLE)ê³¼ KL Divergence"
date: 2025-07-16 18:19:00 +0900
categories: 
tags:
  - ê¸‰ë°œì§„ê±°ë¶ì´
toc: true
comments: false
mermaid: true
math: true
---
## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

- numpy==1.26.4
- matplotlib==3.10.1
- scipy==1.15.2
- torch==2.6.0
- torchvision==0.21.0
- seaborn==0.13.2

## ğŸš€ TL;DR

- **ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •(MLE)** ì€ ê´€ì¸¡ëœ ë°ì´í„°ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ê¸°ë²•ìœ¼ë¡œ, ìƒì„±ëª¨ë¸ í•™ìŠµì˜ í•µì‹¬ ì›ë¦¬ë‹¤
- **KL Divergence** ëŠ” ë‘ í™•ë¥ ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œë¡œ, ìƒì„±ë¶„í¬ì™€ ì‹¤ì œë¶„í¬ ê°„ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•  ë•Œ ì‚¬ìš©ëœë‹¤
- **VAE** ëŠ” KL Divergenceë¥¼ ì •ê·œí™” í•­ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì ì¬ê³µê°„ì„ ì œì•½í•˜ê³ , ELBO(Evidence Lower BOund)ë¥¼ ìµœëŒ€í™”í•œë‹¤
- **GAN** ì€ íŒë³„ìê°€ MLE ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•˜ê³ , ìƒì„±ìëŠ” Jensen-Shannon Divergence(KL Divergenceì˜ í™•ì¥)ë¥¼ ìµœì†Œí™”í•œë‹¤
- **Diffusion Model** ì€ forward processì™€ reverse process ëª¨ë‘ì—ì„œ KL Divergenceë¥¼ í™œìš©í•˜ì—¬ ë…¸ì´ì¦ˆ ì œê±° ê³¼ì •ì„ í•™ìŠµí•œë‹¤
- ë‘ ê°œë… ëª¨ë‘ **ì •ë³´ì´ë¡ ** ì—ì„œ ì¶œë°œí•˜ì—¬ í˜„ëŒ€ ìƒì„±ëª¨ë¸ì˜ ìˆ˜í•™ì  ê¸°ë°˜ì„ ì œê³µí•˜ë©°, ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‹¤ì–‘í•œ í˜•íƒœë¡œ ë³€í˜•ë˜ì–´ ì‚¬ìš©ëœë‹¤

## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

- [Maximum Likelihood Estimation & KL Divergence in Generative Models](https://github.com/yuiyeong/notebooks/blob/main/deep_learning/mle_kl_divergence_generative_models.ipynb)
## ğŸ¯ ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •(Maximum Likelihood Estimation, MLE)ì´ë€?

**ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •(MLE)** ì€ ì£¼ì–´ì§„ ë°ì´í„°ê°€ íŠ¹ì • í™•ë¥ ë¶„í¬ì—ì„œ ë‚˜ì™”ë‹¤ê³  ê°€ì •í•  ë•Œ, ê·¸ ë°ì´í„°ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” í†µê³„í•™ì  ë°©ë²•ì´ë‹¤.

ì‰½ê²Œ ë§í•´, "ë‚´ê°€ ê´€ì¸¡í•œ ë°ì´í„°ê°€ ê°€ì¥ ê·¸ëŸ´ë“¯í•˜ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë¸ì˜ ì„¤ì •ê°’ì€ ë¬´ì—‡ì¼ê¹Œ?"ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê¸°ë²•ì´ë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ë™ì „ì„ 10ë²ˆ ë˜ì ¸ì„œ ì•ë©´ì´ 7ë²ˆ ë‚˜ì™”ë‹¤ë©´, ì´ ë™ì „ì˜ ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥  pëŠ” ì–¼ë§ˆì¼ê¹Œ? MLEëŠ” ì´ ì§ˆë¬¸ì— "ê´€ì¸¡ëœ ê²°ê³¼ê°€ ê°€ì¥ ì¼ì–´ë‚  ê°€ëŠ¥ì„±ì´ ë†’ì€ pê°’ì„ ì°¾ì"ë¼ê³  ì ‘ê·¼í•œë‹¤.

### ì–¸ì–´ì  í‘œí˜„

MLEì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

- **ê°€ëŠ¥ë„(Likelihood)**: íŠ¹ì • íŒŒë¼ë¯¸í„° Î¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê´€ì¸¡ëœ ë°ì´í„° Xê°€ ë‚˜ì˜¬ í™•ë¥ 
- **ìµœëŒ€ ê°€ëŠ¥ë„**: ê°€ëŠ¥í•œ ëª¨ë“  Î¸ ì¤‘ì—ì„œ ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” Î¸ë¥¼ ì„ íƒ
- **ê°ê´€ì  ì¶”ì •**: ë°ì´í„°ì—ë§Œ ì˜ì¡´í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•˜ëŠ” ê°ê´€ì  ë°©ë²•

[MLE ê°œë…ë„: ë™ì „ ë˜ì§€ê¸° ì˜ˆì‹œì—ì„œ ì•ë©´ í™•ë¥  pì— ë”°ë¥¸ likelihood ê³¡ì„  ê·¸ë˜í”„]

### ìˆ˜í•™ì  í‘œí˜„

ì£¼ì–´ì§„ ë°ì´í„° $$X = {x_1, x_2, ..., x_n}$$ê³¼ íŒŒë¼ë¯¸í„° $$\theta$$ë¥¼ ê°€ì§„ í™•ë¥ ë¶„í¬ì— ëŒ€í•´:

**ê°€ëŠ¥ë„ í•¨ìˆ˜(Likelihood Function)**:

$$ L(\theta) = P(X|\theta) = \prod_{i=1}^{n} P(x_i|\theta) $$

**ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜(Log-Likelihood Function)**:

$$ \ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log P(x_i|\theta) $$

**MLE ì¶”ì •ëŸ‰**:

$$ \hat{\theta}_{MLE} = \arg\max_{\theta} \ell(\theta) $$

> ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ê³±ì…ˆì„ ë§ì…ˆìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°ì„ ë‹¨ìˆœí™”í•˜ê³ , ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ í™•ë³´í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë¡œê·¸ëŠ” ë‹¨ì¡°ì¦ê°€ í•¨ìˆ˜ì´ë¯€ë¡œ ìµœëŒ€ê°’ì˜ ìœ„ì¹˜ëŠ” ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤. {: .prompt-tip}

### ì§ê´€ì  ì´í•´ë¥¼ ìœ„í•œ ì˜ˆì‹œ

ë™ì „ ë˜ì§€ê¸° ì˜ˆì‹œë¥¼ í†µí•´ MLEë¥¼ ì´í•´í•´ë³´ì:

- ë™ì „ì„ 10ë²ˆ ë˜ì ¸ì„œ ì•ë©´ì´ 7ë²ˆ ë‚˜ì™”ë‹¤
- ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì„ pë¼ê³  í•˜ì
- ì´í•­ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •: $$P(k|n,p) = \binom{n}{k}p^k(1-p)^{n-k}$$

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

# ë™ì „ ë˜ì§€ê¸° MLE ì˜ˆì‹œ
n_trials = 10
n_heads = 7

# p ê°’ì˜ ë²”ìœ„
p_values = np.linspace(0.01, 0.99, 100)

# ê° pì— ëŒ€í•œ likelihood ê³„ì‚°
likelihoods = [binom.pmf(n_heads, n_trials, p) for p in p_values]

# MLE ì¶”ì •ê°’ (ì´ë¡ ì ìœ¼ë¡œëŠ” 7/10 = 0.7)
mle_estimate = n_heads / n_trials

plt.figure(figsize=(10, 6))
plt.plot(p_values, likelihoods, 'b-', linewidth=2, label='Likelihood')
plt.axvline(mle_estimate, color='r', linestyle='--', linewidth=2, 
           label=f'MLE estimate: p = {mle_estimate:.1f}')
plt.xlabel('p (ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ )')
plt.ylabel('Likelihood')
plt.title('ë™ì „ ë˜ì§€ê¸° MLE: 10ë²ˆ ì¤‘ 7ë²ˆ ì•ë©´')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"MLE ì¶”ì •ê°’: p = {mle_estimate:.3f}")
print(f"ìµœëŒ€ likelihood: {max(likelihoods):.6f}")
```


### ë‹¤ì–‘í•œ ë¶„í¬ì—ì„œì˜ MLE ì˜ˆì‹œ

#### 1. ì •ê·œë¶„í¬ì˜ MLE

ì •ê·œë¶„í¬ $$N(\mu, \sigma^2)$$ì—ì„œ í‰ê· ê³¼ ë¶„ì‚°ì„ ì¶”ì •í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì˜ˆì‹œë‹¤.

**ì´ë¡ ì  ìœ ë„**:

ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜: $$ \ell(\mu, \sigma^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i - \mu)^2 $$

ë¯¸ë¶„í•˜ì—¬ 0ì´ ë˜ëŠ” ì ì„ ì°¾ìœ¼ë©´:

- $$\hat{\mu}_{MLE} = \frac{1}{n}\sum_{i=1}^{n}x_i$$ (í‘œë³¸ í‰ê· )
- $$\hat{\sigma^2}_{MLE} = \frac{1}{n}\sum_{i=1}^{n}(x_i - \hat{\mu})^2$$ (í‘œë³¸ ë¶„ì‚°)

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from scipy.optimize import minimize_scalar

def gaussian_mle_example():
    """ì •ê·œë¶„í¬ MLE ì˜ˆì‹œ"""
    # ì‹¤ì œ ëª¨ì§‘ë‹¨ íŒŒë¼ë¯¸í„°
    true_mu, true_sigma = 5.0, 2.0
    
    # ë‹¤ì–‘í•œ ìƒ˜í”Œ í¬ê¸°ë¡œ ì‹¤í—˜
    sample_sizes = [10, 50, 200, 1000]
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()
    
    for i, n in enumerate(sample_sizes):
        # ë°ì´í„° ìƒì„±
        data = np.random.normal(true_mu, true_sigma, n)
        
        # MLE ì¶”ì •
        mu_mle = np.mean(data)
        sigma_mle = np.sqrt(np.mean((data - mu_mle)**2))
        
        # ì‹œê°í™”
        x = np.linspace(true_mu - 4*true_sigma, true_mu + 4*true_sigma, 100)
        true_pdf = norm.pdf(x, true_mu, true_sigma)
        estimated_pdf = norm.pdf(x, mu_mle, sigma_mle)
        
        axes[i].hist(data, bins=20, density=True, alpha=0.7, 
                    label=f'ë°ì´í„° (n={n})')
        axes[i].plot(x, true_pdf, 'r-', linewidth=2, 
                    label=f'ì‹¤ì œ: Î¼={true_mu}, Ïƒ={true_sigma}')
        axes[i].plot(x, estimated_pdf, 'b--', linewidth=2,
                    label=f'MLE: Î¼={mu_mle:.2f}, Ïƒ={sigma_mle:.2f}')
        
        axes[i].set_title(f'ìƒ˜í”Œ í¬ê¸°: {n}')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
        
        print(f"n={n}: Î¼_MLE={mu_mle:.3f} (ì˜¤ì°¨: {abs(mu_mle-true_mu):.3f}), "
              f"Ïƒ_MLE={sigma_mle:.3f} (ì˜¤ì°¨: {abs(sigma_mle-true_sigma):.3f})")
    
    plt.tight_layout()
    plt.show()

gaussian_mle_example()
```

#### 2. í¬ì•„ì†¡ ë¶„í¬ì˜ MLE

í¬ì•„ì†¡ ë¶„í¬ëŠ” ë‹¨ìœ„ ì‹œê°„ë‹¹ ë°œìƒí•˜ëŠ” ì´ë²¤íŠ¸ ìˆ˜ë¥¼ ëª¨ë¸ë§í•  ë•Œ ì‚¬ìš©ëœë‹¤.

**ìˆ˜í•™ì  ìœ ë„**:

í¬ì•„ì†¡ ë¶„í¬: $$P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}$$

ë¡œê·¸ ê°€ëŠ¥ë„: $$ \ell(\lambda) = \sum_{i=1}^{n}(x_i \log \lambda - \lambda - \log(x_i!)) $$

ë¯¸ë¶„í•˜ì—¬ 0ì´ ë˜ëŠ” ì : $$ \hat{\lambda}_{MLE} = \frac{1}{n}\sum_{i=1}^{n}x_i $$

```python
from scipy.stats import poisson

def poisson_mle_example():
    """í¬ì•„ì†¡ ë¶„í¬ MLE ì˜ˆì‹œ"""
    # ì‹¤ì œ íŒŒë¼ë¯¸í„° (ì˜ˆ: ì‹œê°„ë‹¹ í‰ê·  ê³ ê° ìˆ˜)
    true_lambda = 3.5
    sample_size = 200
    
    # ë°ì´í„° ìƒì„± (ì‹œê°„ë‹¹ ê³ ê° ìˆ˜ ê´€ì¸¡)
    data = np.random.poisson(true_lambda, sample_size)
    
    # MLE ì¶”ì •
    lambda_mle = np.mean(data)
    
    # ë‹¤ì–‘í•œ Î» ê°’ì— ëŒ€í•œ likelihood ê³„ì‚°
    lambda_range = np.linspace(2, 5, 100)
    log_likelihoods = []
    
    for lam in lambda_range:
        # í¬ì•„ì†¡ ë¶„í¬ì˜ ë¡œê·¸ ê°€ëŠ¥ë„ ê³„ì‚°
        log_likelihood = np.sum(poisson.logpmf(data, lam))
        log_likelihoods.append(log_likelihood)
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # ë°ì´í„° íˆìŠ¤í† ê·¸ë¨ê³¼ ì¶”ì • ë¶„í¬
    unique_values = np.arange(0, max(data) + 1)
    observed_freq = [(data == k).sum() / len(data) for k in unique_values]
    expected_freq_true = [poisson.pmf(k, true_lambda) for k in unique_values]
    expected_freq_mle = [poisson.pmf(k, lambda_mle) for k in unique_values]
    
    ax1.bar(unique_values - 0.2, observed_freq, 0.4, 
           label='ê´€ì¸¡ ë¹ˆë„', alpha=0.7)
    ax1.bar(unique_values + 0.2, expected_freq_mle, 0.4, 
           label=f'MLE ì¶”ì • (Î»={lambda_mle:.2f})', alpha=0.7)
    ax1.plot(unique_values, expected_freq_true, 'ro-', 
            label=f'ì‹¤ì œ ë¶„í¬ (Î»={true_lambda})')
    
    ax1.set_xlabel('ë°œìƒ íšŸìˆ˜')
    ax1.set_ylabel('í™•ë¥ ')
    ax1.set_title('í¬ì•„ì†¡ ë¶„í¬ MLE')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ë¡œê·¸ ê°€ëŠ¥ë„ ê³¡ì„ 
    ax2.plot(lambda_range, log_likelihoods, 'b-', linewidth=2)
    ax2.axvline(lambda_mle, color='r', linestyle='--', 
               label=f'MLE: Î»={lambda_mle:.2f}')
    ax2.axvline(true_lambda, color='g', linestyle='--', 
               label=f'ì‹¤ì œ: Î»={true_lambda}')
    
    ax2.set_xlabel('Î» (ëª¨ìˆ˜)')
    ax2.set_ylabel('ë¡œê·¸ ê°€ëŠ¥ë„')
    ax2.set_title('ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"ì‹¤ì œ Î»: {true_lambda}")
    print(f"MLE ì¶”ì • Î»: {lambda_mle:.3f}")
    print(f"ì¶”ì • ì˜¤ì°¨: {abs(lambda_mle - true_lambda):.3f}")

poisson_mle_example()
```

#### 3. ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ MLE (ì´ì§„ ë¶„ë¥˜ì˜ ê¸°ì´ˆ)

ë² ë¥´ëˆ„ì´ ë¶„í¬ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì˜ ê¸°ë³¸ì´ ë˜ëŠ” ë¶„í¬ë‹¤.

```python
def bernoulli_mle_example():
    """ë² ë¥´ëˆ„ì´ ë¶„í¬ MLE ì˜ˆì‹œ - ì´ì§„ ë¶„ë¥˜ ê´€ì """
    # ì‹¤ì œ ì„±ê³µ í™•ë¥ 
    true_p = 0.7
    sample_size = 100
    
    # ë°ì´í„° ìƒì„± (1: ì„±ê³µ, 0: ì‹¤íŒ¨)
    data = np.random.binomial(1, true_p, sample_size)
    
    # MLE ì¶”ì •
    p_mle = np.mean(data)  # ì„±ê³µ íšŸìˆ˜ / ì „ì²´ ì‹œë„ íšŸìˆ˜
    
    # ë‹¤ì–‘í•œ p ê°’ì— ëŒ€í•œ likelihood ê³„ì‚°
    p_range = np.linspace(0.01, 0.99, 100)
    log_likelihoods = []
    
    for p in p_range:
        # ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ ë¡œê·¸ ê°€ëŠ¥ë„
        log_likelihood = np.sum(data * np.log(p) + (1 - data) * np.log(1 - p))
        log_likelihoods.append(log_likelihood)
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # ë°ì´í„° ì‹œê°í™”
    success_count = np.sum(data)
    failure_count = len(data) - success_count
    
    ax1.bar(['ì‹¤íŒ¨ (0)', 'ì„±ê³µ (1)'], [failure_count, success_count], 
           alpha=0.7, color=['red', 'blue'])
    ax1.set_ylabel('ë¹ˆë„')
    ax1.set_title(f'ë² ë¥´ëˆ„ì´ ì‹œí–‰ ê²°ê³¼ (n={sample_size})')
    ax1.text(0, failure_count/2, f'{failure_count}íšŒ', 
            ha='center', va='center', fontsize=12, fontweight='bold')
    ax1.text(1, success_count/2, f'{success_count}íšŒ', 
            ha='center', va='center', fontsize=12, fontweight='bold')
    
    # ë¡œê·¸ ê°€ëŠ¥ë„ ê³¡ì„ 
    ax2.plot(p_range, log_likelihoods, 'b-', linewidth=2)
    ax2.axvline(p_mle, color='r', linestyle='--', 
               label=f'MLE: p={p_mle:.3f}')
    ax2.axvline(true_p, color='g', linestyle='--', 
               label=f'ì‹¤ì œ: p={true_p}')
    
    ax2.set_xlabel('ì„±ê³µ í™•ë¥  p')
    ax2.set_ylabel('ë¡œê·¸ ê°€ëŠ¥ë„')
    ax2.set_title('ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"ì‹¤ì œ ì„±ê³µ í™•ë¥ : {true_p}")
    print(f"MLE ì¶”ì • í™•ë¥ : {p_mle:.3f}")
    print(f"95% ì‹ ë¢°êµ¬ê°„: [{p_mle - 1.96*np.sqrt(p_mle*(1-p_mle)/sample_size):.3f}, "
          f"{p_mle + 1.96*np.sqrt(p_mle*(1-p_mle)/sample_size):.3f}]")

bernoulli_mle_example()
```

#### 4. ë‹¤í•­ë¶„í¬ì˜ MLE (ë‹¤ì¤‘ ë¶„ë¥˜ì˜ ê¸°ì´ˆ)

ë‹¤í•­ë¶„í¬ëŠ” ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œì˜ ê¸°ë³¸ì´ ë˜ëŠ” ë¶„í¬ë‹¤.

```python
def multinomial_mle_example():
    """ë‹¤í•­ë¶„í¬ MLE ì˜ˆì‹œ - ë‹¤ì¤‘ ë¶„ë¥˜ ê´€ì """
    # ì‹¤ì œ í´ë˜ìŠ¤ í™•ë¥  (3ê°œ í´ë˜ìŠ¤)
    true_probs = np.array([0.5, 0.3, 0.2])
    sample_size = 1000
    
    # ë°ì´í„° ìƒì„±
    data = np.random.multinomial(1, true_probs, sample_size)
    class_counts = np.sum(data, axis=0)
    
    # MLE ì¶”ì • (ê° í´ë˜ìŠ¤ì˜ ìƒëŒ€ ë¹ˆë„)
    probs_mle = class_counts / sample_size
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    classes = ['í´ë˜ìŠ¤ A', 'í´ë˜ìŠ¤ B', 'í´ë˜ìŠ¤ C']
    x_pos = np.arange(len(classes))
    
    # ë¹ˆë„ ë¹„êµ
    width = 0.35
    ax1.bar(x_pos - width/2, true_probs, width, 
           label='ì‹¤ì œ í™•ë¥ ', alpha=0.7, color='skyblue')
    ax1.bar(x_pos + width/2, probs_mle, width, 
           label='MLE ì¶”ì •', alpha=0.7, color='orange')
    
    ax1.set_xlabel('í´ë˜ìŠ¤')
    ax1.set_ylabel('í™•ë¥ ')
    ax1.set_title('ë‹¤í•­ë¶„í¬ MLE')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(classes)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ì¹´ìš´íŠ¸ ì‹œê°í™”
    ax2.bar(classes, class_counts, alpha=0.7, color='green')
    ax2.set_ylabel('ê´€ì¸¡ ë¹ˆë„')
    ax2.set_title(f'í´ë˜ìŠ¤ë³„ ê´€ì¸¡ ë¹ˆë„ (ì´ {sample_size}ê°œ)')
    
    for i, count in enumerate(class_counts):
        ax2.text(i, count + 10, str(count), ha='center', fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    print("ì‹¤ì œ í™•ë¥ :", true_probs)
    print("MLE ì¶”ì • í™•ë¥ :", probs_mle)
    print("ì¶”ì • ì˜¤ì°¨:", np.abs(true_probs - probs_mle))
    
    # ë¡œê·¸ ê°€ëŠ¥ë„ ê³„ì‚°
    log_likelihood = np.sum(class_counts * np.log(probs_mle))
    print(f"ë¡œê·¸ ê°€ëŠ¥ë„: {log_likelihood:.2f}")

multinomial_mle_example()
```

### MLEê°€ ì™œ ì¤‘ìš”í•œê°€?

#### 1. í†µê³„ì  ì„±ì§ˆì˜ ìš°ìˆ˜ì„±

MLEëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°”ëŒì§í•œ í†µê³„ì  ì„±ì§ˆì„ ê°€ì§„ë‹¤:

**ì¼ì¹˜ì„±(Consistency)**: ìƒ˜í”Œ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì°¸ê°’ì— ìˆ˜ë ´ $$ \hat{\theta}_n \xrightarrow{p} \theta_0 \quad \text{as } n \to \infty $$

**ì ê·¼ì  ì •ê·œì„±(Asymptotic Normality)**: ëŒ€í‘œë³¸ì—ì„œ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ $$ \sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} N(0, I^{-1}(\theta_0)) $$

**ì ê·¼ì  íš¨ìœ¨ì„±(Asymptotic Efficiency)**: ê°€ëŠ¥í•œ ìµœì†Œ ë¶„ì‚°ì„ ê°€ì§

```python
def mle_properties_demonstration():
    """MLEì˜ í†µê³„ì  ì„±ì§ˆ ì‹œì—°"""
    true_mu = 0
    true_sigma = 1
    sample_sizes = [10, 50, 100, 500, 1000, 5000]
    n_simulations = 1000
    
    # ê° ìƒ˜í”Œ í¬ê¸°ë³„ë¡œ MLE ì¶”ì •ê°’ë“¤ ìˆ˜ì§‘
    mle_estimates = {}
    
    for n in sample_sizes:
        estimates = []
        for _ in range(n_simulations):
            data = np.random.normal(true_mu, true_sigma, n)
            mu_mle = np.mean(data)
            estimates.append(mu_mle)
        mle_estimates[n] = np.array(estimates)
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # 1. ì¼ì¹˜ì„± í™•ì¸ (ë¶„ì‚°ì´ ì¤„ì–´ë“¦)
    variances = [np.var(mle_estimates[n]) for n in sample_sizes]
    theoretical_variances = [true_sigma**2 / n for n in sample_sizes]
    
    ax1.loglog(sample_sizes, variances, 'bo-', label='ì‹¤ì œ ë¶„ì‚°')
    ax1.loglog(sample_sizes, theoretical_variances, 'r--', label='ì´ë¡ ì  ë¶„ì‚° (ÏƒÂ²/n)')
    ax1.set_xlabel('ìƒ˜í”Œ í¬ê¸°')
    ax1.set_ylabel('MLE ì¶”ì •ëŸ‰ì˜ ë¶„ì‚°')
    ax1.set_title('ì¼ì¹˜ì„±: ë¶„ì‚°ì´ 1/nìœ¼ë¡œ ê°ì†Œ')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ì ê·¼ì  ì •ê·œì„± í™•ì¸ (íˆìŠ¤í† ê·¸ë¨)
    n_large = 1000
    estimates_large = mle_estimates[n_large]
    
    ax2.hist(estimates_large, bins=50, density=True, alpha=0.7, 
            label=f'MLE ë¶„í¬ (n={n_large})')
    
    # ì´ë¡ ì  ì •ê·œë¶„í¬ overlay
    x = np.linspace(estimates_large.min(), estimates_large.max(), 100)
    theoretical_std = true_sigma / np.sqrt(n_large)
    theoretical_pdf = norm.pdf(x, true_mu, theoretical_std)
    ax2.plot(x, theoretical_pdf, 'r-', linewidth=2, 
            label=f'ì´ë¡ ì  N({true_mu}, {theoretical_std:.4f}Â²)')
    
    ax2.axvline(true_mu, color='g', linestyle='--', 
               label='ì°¸ê°’')
    ax2.set_xlabel('MLE ì¶”ì •ê°’')
    ax2.set_ylabel('ë°€ë„')
    ax2.set_title('ì ê·¼ì  ì •ê·œì„±')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("=== MLEì˜ í†µê³„ì  ì„±ì§ˆ í™•ì¸ ===")
    for n in sample_sizes:
        bias = np.mean(mle_estimates[n]) - true_mu
        variance = np.var(mle_estimates[n])
        theoretical_var = true_sigma**2 / n
        print(f"n={n:4d}: í¸í–¥={bias:6.4f}, ë¶„ì‚°={variance:6.4f}, "
              f"ì´ë¡ ì  ë¶„ì‚°={theoretical_var:6.4f}")

mle_properties_demonstration()
```

#### 2. ë¨¸ì‹ ëŸ¬ë‹ì—ì„œì˜ ì—­í• 

MLEëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ ê±°ì˜ ëª¨ë“  ì˜ì—­ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•œë‹¤:

**ì„ í˜• íšŒê·€**: ì”ì°¨ì˜ ì •ê·œë¶„í¬ ê°€ì • í•˜ì—ì„œ ìµœì†Œì œê³±ë²•ì€ MLEì™€ ë™ì¼ **ë¡œì§€ìŠ¤í‹± íšŒê·€**: ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ MLEë¡œ ìœ ë„ **ì‹ ê²½ë§**: Cross-entropy ì†ì‹¤í•¨ìˆ˜ëŠ” MLEì˜ ë³€í˜• **ìƒì„±ëª¨ë¸**: ë°ì´í„° ë¶„í¬ë¥¼ í•™ìŠµí•˜ëŠ” ê¸°ë³¸ ì›ë¦¬

```python
def mle_in_ml_examples():
    """ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ MLE í™œìš© ì˜ˆì‹œ"""
    
    # 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ = ë² ë¥´ëˆ„ì´ ë¶„í¬ MLE
    from sklearn.linear_model import LogisticRegression
    from sklearn.datasets import make_classification
    
    # ë°ì´í„° ìƒì„±
    X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, 
                             n_informative=2, n_clusters_per_class=1, random_state=42)
    
    # ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ
    model = LogisticRegression()
    model.fit(X, y)
    
    # ì˜ˆì¸¡ í™•ë¥  (ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ ëª¨ìˆ˜ p)
    probs = model.predict_proba(X)[:, 1]
    
    # ë¡œê·¸ ê°€ëŠ¥ë„ ê³„ì‚° (ë² ë¥´ëˆ„ì´ ë¶„í¬)
    log_likelihood = np.sum(y * np.log(probs + 1e-15) + 
                           (1 - y) * np.log(1 - probs + 1e-15))
    
    print(f"ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ë¡œê·¸ ê°€ëŠ¥ë„: {log_likelihood:.2f}")
    
    # 2. ì„ í˜• íšŒê·€ = ì •ê·œë¶„í¬ MLE
    from sklearn.linear_model import LinearRegression
    
    # íšŒê·€ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    X_reg = np.random.randn(100, 1)
    y_reg = 2 * X_reg.flatten() + 1 + np.random.randn(100) * 0.5
    
    # ì„ í˜• íšŒê·€ í•™ìŠµ
    reg_model = LinearRegression()
    reg_model.fit(X_reg, y_reg)
    
    # ì˜ˆì¸¡ê³¼ ì”ì°¨
    y_pred = reg_model.predict(X_reg)
    residuals = y_reg - y_pred
    
    # ì”ì°¨ì˜ ë¶„ì‚° ì¶”ì • (MLE)
    sigma_mle = np.sqrt(np.mean(residuals**2))
    
    # ë¡œê·¸ ê°€ëŠ¥ë„ ê³„ì‚° (ì •ê·œë¶„í¬)
    log_likelihood_reg = -0.5 * len(y_reg) * np.log(2 * np.pi * sigma_mle**2) - \
                        np.sum(residuals**2) / (2 * sigma_mle**2)
    
    print(f"ì„ í˜• íšŒê·€ì˜ ë¡œê·¸ ê°€ëŠ¥ë„: {log_likelihood_reg:.2f}")
    print(f"ì¶”ì •ëœ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨: {sigma_mle:.3f}")

mle_in_ml_examples()
```

#### 3. ë² ì´ì§€ì•ˆ ì¶”ë¡ ê³¼ì˜ ê´€ê³„

MLEëŠ” ë² ì´ì§€ì•ˆ ì¶”ë¡ ì—ì„œ **ë¬´ì •ë³´ ì‚¬ì „ë¶„í¬(non-informative prior)** ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ **ìµœëŒ€ ì‚¬í›„í™•ë¥ (MAP) ì¶”ì •**ê³¼ ê°™ë‹¤:

$$ \text{MAP: } \hat{\theta}_{MAP} = \arg\max_\theta p(\theta|X) = \arg\max_\theta p(X|\theta)p(\theta) $$

$$ \text{ë§Œì•½ } p(\theta) \text{ê°€ ìƒìˆ˜ë¼ë©´: } \hat{\theta}_{MAP} = \hat{\theta}_{MLE} $$

#### 4. ì •ë³´ì´ë¡ ì  í•´ì„

MLEëŠ” **ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°(KL Divergence)ì„ ìµœì†Œí™”**í•˜ëŠ” ê²ƒê³¼ ë™ì¹˜ì´ë‹¤. ì´ëŠ” ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ìì„¸íˆ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.

### ìƒì„±ëª¨ë¸ì—ì„œ MLEì˜ ì—­í• 

ìƒì„±ëª¨ë¸ì—ì„œ MLEëŠ” ë‹¤ìŒê³¼ ê°™ì´ í™œìš©ëœë‹¤:

- **ë°ì´í„° ë¶„í¬ í•™ìŠµ**: ì‹¤ì œ ë°ì´í„° ë¶„í¬ $$p_{data}(x)$$ë¥¼ ëª¨ë°©í•˜ëŠ” ëª¨ë¸ ë¶„í¬ $$p_{model}(x;\theta)$$ì˜ íŒŒë¼ë¯¸í„° Î¸ë¥¼ í•™ìŠµ
- **ì†ì‹¤ í•¨ìˆ˜ ì„¤ê³„**: Negative Log-Likelihoodë¥¼ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì‚¬ìš©
- **ìƒì„± í’ˆì§ˆ í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ì´ ì‹¤ì œ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ í‰ê°€

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ê°„ë‹¨í•œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ MLE ì˜ˆì‹œ
class GaussianModel(nn.Module):
    def __init__(self):
        super().__init__()
        # í‰ê· ê³¼ ë¡œê·¸ ë¶„ì‚°ì„ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •
        self.mean = nn.Parameter(torch.tensor(0.0))
        self.log_var = nn.Parameter(torch.tensor(0.0))
    
    def forward(self, x):
        # ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ë¡œê·¸ í™•ë¥ ë°€ë„ ê³„ì‚°
        var = torch.exp(self.log_var)
        log_prob = -0.5 * torch.log(2 * torch.pi * var) - 0.5 * (x - self.mean)**2 / var
        return log_prob
    
    def sample(self, n_samples):
        # í•™ìŠµëœ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
        std = torch.sqrt(torch.exp(self.log_var))
        return torch.normal(self.mean, std, (n_samples,))

# ì‹¤ì œ ë°ì´í„° ìƒì„± (í‰ê· =2, í‘œì¤€í¸ì°¨=1.5ì¸ ê°€ìš°ì‹œì•ˆ)
true_mean, true_std = 2.0, 1.5
real_data = torch.normal(true_mean, true_std, (1000,))

# ëª¨ë¸ ì´ˆê¸°í™” ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
model = GaussianModel()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# MLE í•™ìŠµ
for epoch in range(1000):
    optimizer.zero_grad()
    
    # Negative Log-Likelihood ê³„ì‚° (ì†ì‹¤ í•¨ìˆ˜)
    log_probs = model(real_data)
    nll_loss = -torch.mean(log_probs)  # Negative Log-Likelihood
    
    nll_loss.backward()
    optimizer.step()
    
    if epoch % 200 == 0:
        print(f"Epoch {epoch}, NLL Loss: {nll_loss.item():.4f}")

# í•™ìŠµ ê²°ê³¼
learned_mean = model.mean.item()
learned_std = torch.sqrt(torch.exp(model.log_var)).item()

print(f"\nì‹¤ì œ íŒŒë¼ë¯¸í„°: í‰ê· ={true_mean:.2f}, í‘œì¤€í¸ì°¨={true_std:.2f}")
print(f"í•™ìŠµëœ íŒŒë¼ë¯¸í„°: í‰ê· ={learned_mean:.2f}, í‘œì¤€í¸ì°¨={learned_std:.2f}")
```

## ğŸ“ KL Divergence(ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°)ë€?

**KL Divergence** ëŠ” ë‘ í™•ë¥ ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì •ë³´ì´ë¡ ì  ì§€í‘œì´ë‹¤. "í•œ ë¶„í¬ë¥¼ ë‹¤ë¥¸ ë¶„í¬ë¡œ ê·¼ì‚¬í•  ë•Œ ë°œìƒí•˜ëŠ” ì •ë³´ ì†ì‹¤ëŸ‰"ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.

KL DivergenceëŠ” **ë¹„ëŒ€ì¹­ì ** ì´ë©°, í•­ìƒ 0 ì´ìƒì˜ ê°’ì„ ê°€ì§„ë‹¤. ë‘ ë¶„í¬ê°€ ë™ì¼í•  ë•Œë§Œ 0ì´ ë˜ê³ , ë‹¤ë¥¼ìˆ˜ë¡ í° ê°’ì„ ê°€ì§„ë‹¤.

[KL Divergence ê°œë…ë„: ë‘ ê°€ìš°ì‹œì•ˆ ë¶„í¬ ê°„ì˜ KL Divergence ì‹œê°í™”]

### ì–¸ì–´ì  í‘œí˜„

KL Divergenceì˜ ì§ê´€ì  ì˜ë¯¸:

- **ì •ë³´ ì´ë¡ ì  í•´ì„**: ë¶„í¬ P ëŒ€ì‹  ë¶„í¬ Që¥¼ ì‚¬ìš©í•  ë•Œ ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ëŸ‰
- **ì••ì¶• ê´€ì **: Pë¡œ ì¸ì½”ë”©ëœ ë©”ì‹œì§€ë¥¼ Që¡œ ë””ì½”ë”©í•  ë•Œì˜ ë¹„íš¨ìœ¨ì„±
- **í™•ë¥ ì  í•´ì„**: Pì—ì„œ ìƒì„±ëœ ë°ì´í„°ë¥¼ Që¡œ ì„¤ëª…í•  ë•Œì˜ ë¶€ì •í™•ì„±

### ìˆ˜í•™ì  í‘œí˜„

**ì´ì‚° í™•ë¥ ë¶„í¬**ì— ëŒ€í•œ KL Divergence:

$$ D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} $$

**ì—°ì† í™•ë¥ ë¶„í¬**ì— ëŒ€í•œ KL Divergence:

$$ D_{KL}(P||Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx $$

**ê¸°ëŒ“ê°’ìœ¼ë¡œ í‘œí˜„**:

$$ D_{KL}(P||Q) = \mathbb{E}_{x \sim P} \left[ \log \frac{P(x)}{Q(x)} \right] $$

**ì—”íŠ¸ë¡œí”¼ ê´€ì **:

$$ D_{KL}(P||Q) = \mathbb{E}_{x \sim P}[-\log Q(x)] - \mathbb{E}_{x \sim P}[-\log P(x)] = H(P,Q) - H(P) $$

ì—¬ê¸°ì„œ $$H(P,Q)$$ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼(Cross Entropy), $$H(P)$$ëŠ” Pì˜ ì—”íŠ¸ë¡œí”¼ì´ë‹¤.

> KL Divergenceì˜ í•µì‹¬ ì„±ì§ˆ: ë¹„ëŒ€ì¹­ì„± $$D_{KL}(P||Q) \neq D_{KL}(Q||P)$$, ë¹„ìŒì„± $$D_{KL}(P||Q) \geq 0$$, P=Qì¼ ë•Œë§Œ 0 {: .prompt-tip}

### ì§ê´€ì  ì´í•´ë¥¼ ìœ„í•œ ì˜ˆì‹œ

ë‘ ê°€ìš°ì‹œì•ˆ ë¶„í¬ ê°„ì˜ KL Divergenceë¥¼ ê³„ì‚°í•´ë³´ì:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from scipy.integrate import quad

def gaussian_pdf(x, mu, sigma):
    """ê°€ìš°ì‹œì•ˆ í™•ë¥ ë°€ë„í•¨ìˆ˜"""
    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)

def kl_divergence_gaussian(mu1, sigma1, mu2, sigma2):
    """ë‘ ê°€ìš°ì‹œì•ˆ ë¶„í¬ ê°„ì˜ KL Divergence (í•´ì„ì  í•´)"""
    return np.log(sigma2/sigma1) + (sigma1**2 + (mu1-mu2)**2)/(2*sigma2**2) - 0.5

# ë‘ ê°€ìš°ì‹œì•ˆ ë¶„í¬ ì„¤ì •
mu1, sigma1 = 0, 1    # P(x): í‘œì¤€ì •ê·œë¶„í¬
mu2, sigma2 = 2, 1.5  # Q(x): í‰ê· =2, í‘œì¤€í¸ì°¨=1.5

# KL Divergence ê³„ì‚°
kl_pq = kl_divergence_gaussian(mu1, sigma1, mu2, sigma2)  # D_KL(P||Q)
kl_qp = kl_divergence_gaussian(mu2, sigma2, mu1, sigma1)  # D_KL(Q||P)

print(f"D_KL(P||Q) = {kl_pq:.4f}")
print(f"D_KL(Q||P) = {kl_qp:.4f}")
print(f"ë¹„ëŒ€ì¹­ì„± í™•ì¸: {abs(kl_pq - kl_qp):.4f}")

# ì‹œê°í™”
x = np.linspace(-4, 6, 1000)
p_x = gaussian_pdf(x, mu1, sigma1)
q_x = gaussian_pdf(x, mu2, sigma2)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(x, p_x, 'b-', linewidth=2, label='P(x): N(0,1)')
plt.plot(x, q_x, 'r-', linewidth=2, label='Q(x): N(2,1.5)')
plt.fill_between(x, p_x, alpha=0.3, color='blue')
plt.fill_between(x, q_x, alpha=0.3, color='red')
plt.xlabel('x')
plt.ylabel('í™•ë¥ ë°€ë„')
plt.title('ë‘ ê°€ìš°ì‹œì•ˆ ë¶„í¬')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# KL Divergenceì˜ ê¸°ì—¬ë„ ì‹œê°í™” (P(x) * log(P(x)/Q(x)))
kl_contribution = p_x * np.log(p_x / (q_x + 1e-10))  # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ epsilon
plt.plot(x, kl_contribution, 'g-', linewidth=2)
plt.fill_between(x, kl_contribution, alpha=0.3, color='green')
plt.xlabel('x')
plt.ylabel('P(x) Ã— log(P(x)/Q(x))')
plt.title(f'KL Divergence ê¸°ì—¬ë„\nâˆ« ë©´ì  = D_KL(P||Q) = {kl_pq:.4f}')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### PyTorchë¥¼ ì‚¬ìš©í•œ KL Divergence ê³„ì‚°

```python
import torch
import torch.nn.functional as F
from torch.distributions import Normal, kl_divergence

# ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ KL Divergence ê³„ì‚°
def compute_kl_divergence():
    # ë‘ ê°€ìš°ì‹œì•ˆ ë¶„í¬ ì •ì˜
    p = Normal(torch.tensor(0.0), torch.tensor(1.0))    # N(0,1)
    q = Normal(torch.tensor(2.0), torch.tensor(1.5))    # N(2,1.5)
    
    # PyTorchì˜ ë‚´ì¥ í•¨ìˆ˜ë¡œ KL Divergence ê³„ì‚°
    kl_pq = kl_divergence(p, q)
    kl_qp = kl_divergence(q, p)
    
    print(f"PyTorch KL(P||Q): {kl_pq.item():.4f}")
    print(f"PyTorch KL(Q||P): {kl_qp.item():.4f}")
    
    # ìˆ˜ë™ ê³„ì‚°ìœ¼ë¡œ ê²€ì¦
    mu1, sigma1 = 0.0, 1.0
    mu2, sigma2 = 2.0, 1.5
    
    manual_kl = torch.log(torch.tensor(sigma2/sigma1)) + \
                (sigma1**2 + (mu1-mu2)**2)/(2*sigma2**2) - 0.5
    
    print(f"ìˆ˜ë™ ê³„ì‚° KL(P||Q): {manual_kl.item():.4f}")

compute_kl_divergence()

# ì´ì‚° ë¶„í¬ì—ì„œì˜ KL Divergence ê³„ì‚°
def discrete_kl_divergence():
    # ë‘ ì´ì‚° í™•ë¥ ë¶„í¬ (ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥)
    logits_p = torch.tensor([1.0, 2.0, 0.5])
    logits_q = torch.tensor([0.5, 1.5, 1.0])
    
    p = F.softmax(logits_p, dim=0)
    q = F.softmax(logits_q, dim=0)
    
    # KL Divergence ê³„ì‚°
    kl_div = F.kl_div(torch.log(q), p, reduction='sum')
    
    # ìˆ˜ë™ ê³„ì‚°
    manual_kl = torch.sum(p * torch.log(p / q))
    
    print(f"\nì´ì‚° ë¶„í¬ KL Divergence:")
    print(f"P: {p}")
    print(f"Q: {q}")
    print(f"KL(P||Q): {manual_kl.item():.4f}")

discrete_kl_divergence()
```

### MLEì™€ KL Divergenceì˜ ê¹Šì€ ê´€ê³„

MLEì™€ KL DivergenceëŠ” í‘œë©´ì ìœ¼ë¡œ ë‹¤ë¥¸ ê°œë…ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ì‹¤ì œë¡œëŠ” **ê°™ì€ ëª©í‘œë¥¼ ì¶”êµ¬í•˜ëŠ” ë‘ ê°€ì§€ ë‹¤ë¥¸ ê´€ì **ì´ë‹¤.

#### ìˆ˜í•™ì  ì—°ê²°: MLE = KL Divergence ìµœì†Œí™”

ê²½í—˜ì  ë¶„í¬(empirical distribution) $$\hat{p}_{data}(x)$$ì™€ ëª¨ë¸ ë¶„í¬ $$p_{\theta}(x)$$ ì‚¬ì´ì˜ KL divergenceë¥¼ ìƒê°í•´ë³´ì:

$$ D_{KL}(\hat{p}_{data} || p_{\theta}) = \sum_x \hat{p}_{data}(x) \log \frac{\hat{p}_{data}(x)}{p_{\theta}(x)} $$

$$ = \sum_x \hat{p}_{data}(x) \log \hat{p}_{data}(x) - \sum_x \hat{p}_{data}(x) \log p_{\theta}(x) $$

$$ = H(\hat{p}_{data}) - \mathbb{E}_{\hat{p}_{data}}[\log p_{\theta}(x)] $$

ì—¬ê¸°ì„œ ì²« ë²ˆì§¸ í•­ $$H(\hat{p}_{data})$$ëŠ” ë°ì´í„°ì—ë§Œ ì˜ì¡´í•˜ë¯€ë¡œ ìƒìˆ˜ì´ë‹¤. ë”°ë¼ì„œ KL divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì€ **êµì°¨ ì—”íŠ¸ë¡œí”¼ í•­ì„ ìµœëŒ€í™”**í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤:

$$ \min_{\theta} D_{KL}(\hat{p}_{data} || p_{\theta}) \equiv \max_{\theta} \mathbb{E}_{\hat{p}_{data}}[\log p_{\theta}(x)] $$

ê²½í—˜ì  ë¶„í¬ì—ì„œì˜ ê¸°ëŒ“ê°’ì€ í‘œë³¸ í‰ê· ì´ë¯€ë¡œ:

$$ \max_{\theta} \mathbb{E}_{\hat{p}_{data}}[\log p_{\theta}(x)] = \max_{\theta} \frac{1}{n} \sum_{i=1}^{n} \log p_{\theta}(x_i) $$

ì´ê²ƒì´ ë°”ë¡œ **MLEì˜ ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜**ì´ë‹¤!

> **í•µì‹¬ í†µì°°**: MLEë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ê²½í—˜ì  ë°ì´í„° ë¶„í¬ì™€ ëª¨ë¸ ë¶„í¬ ì‚¬ì´ì˜ KL divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒê³¼ ì •í™•íˆ ê°™ë‹¤. {: .prompt-tip}

```python
def mle_kl_relationship_demo():
    """MLEì™€ KL Divergenceì˜ ê´€ê³„ ì‹œì—°"""
    
    # ì‹¤ì œ ë°ì´í„° ë¶„í¬ ì‹œë®¬ë ˆì´ì…˜
    true_mu, true_sigma = 2.0, 1.0
    data = np.random.normal(true_mu, true_sigma, 1000)
    
    # ê²½í—˜ì  ë¶„í¬ ìƒì„± (íˆìŠ¤í† ê·¸ë¨)
    bins = np.linspace(data.min() - 1, data.max() + 1, 50)
    empirical_counts, _ = np.histogram(data, bins=bins, density=True)
    bin_centers = (bins[:-1] + bins[1:]) / 2
    empirical_probs = empirical_counts * (bins[1] - bins[0])  # í™•ë¥ ë¡œ ì •ê·œí™”
    empirical_probs = empirical_probs / empirical_probs.sum()  # í•©ì´ 1ì´ ë˜ë„ë¡
    
    # ë‹¤ì–‘í•œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ KL divergenceì™€ log-likelihood ê³„ì‚°
    mu_range = np.linspace(0, 4, 50)
    sigma_fixed = 1.0
    
    kl_divergences = []
    log_likelihoods = []
    
    for mu in mu_range:
        # ëª¨ë¸ í™•ë¥  ê³„ì‚°
        model_probs = norm.pdf(bin_centers, mu, sigma_fixed)
        model_probs = model_probs / model_probs.sum()  # ì •ê·œí™”
        
        # KL divergence ê³„ì‚° (empirical || model)
        kl_div = np.sum(empirical_probs * np.log(empirical_probs / (model_probs + 1e-15)))
        kl_divergences.append(kl_div)
        
        # Log-likelihood ê³„ì‚°
        log_likelihood = np.sum(norm.logpdf(data, mu, sigma_fixed))
        log_likelihoods.append(log_likelihood)
    
    # MLE ì¶”ì •ê°’
    mu_mle = np.mean(data)
    
    # ì‹œê°í™”
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))
    
    # 1. ê²½í—˜ì  ë¶„í¬ vs ìµœì  ëª¨ë¸ ë¶„í¬
    x = np.linspace(data.min() - 1, data.max() + 1, 100)
    empirical_pdf = norm.pdf(x, np.mean(data), np.std(data))
    optimal_model_pdf = norm.pdf(x, mu_mle, sigma_fixed)
    
    ax1.hist(data, bins=30, density=True, alpha=0.6, label='ë°ì´í„°')
    ax1.plot(x, empirical_pdf, 'g-', linewidth=2, label='ê²½í—˜ì  ë¶„í¬')
    ax1.plot(x, optimal_model_pdf, 'r--', linewidth=2, label=f'ìµœì  ëª¨ë¸ (Î¼={mu_mle:.2f})')
    ax1.set_xlabel('x')
    ax1.set_ylabel('ë°€ë„')
    ax1.set_title('ë¶„í¬ ë¹„êµ')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. KL Divergence
    ax2.plot(mu_range, kl_divergences, 'b-', linewidth=2)
    ax2.axvline(mu_mle, color='r', linestyle='--', 
               label=f'MLE ìµœì ê°’ (Î¼={mu_mle:.2f})')
    ax2.axvline(true_mu, color='g', linestyle='--', 
               label=f'ì‹¤ì œê°’ (Î¼={true_mu})')
    ax2.set_xlabel('ëª¨ë¸ í‰ê·  Î¼')
    ax2.set_ylabel('KL Divergence')
    ax2.set_title('KL Divergence (ê²½í—˜ì  || ëª¨ë¸)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Log-Likelihood
    ax3.plot(mu_range, log_likelihoods, 'purple', linewidth=2)
    ax3.axvline(mu_mle, color='r', linestyle='--', 
               label=f'MLE ìµœì ê°’ (Î¼={mu_mle:.2f})')
    ax3.axvline(true_mu, color='g', linestyle='--', 
               label=f'ì‹¤ì œê°’ (Î¼={true_mu})')
    ax3.set_xlabel('ëª¨ë¸ í‰ê·  Î¼')
    ax3.set_ylabel('Log-Likelihood')
    ax3.set_title('Log-Likelihood')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ìµœì ê°’ì—ì„œì˜ ì§€í‘œ ë¹„êµ
    optimal_idx = np.argmin(kl_divergences)
    mle_idx = np.argmax(log_likelihoods)
    
    print(f"=== MLEì™€ KL Divergence ìµœì†Œí™”ì˜ ë™ì¹˜ì„± í™•ì¸ ===")
    print(f"KL divergence ìµœì†Œê°’ì—ì„œì˜ Î¼: {mu_range[optimal_idx]:.3f}")
    print(f"Log-likelihood ìµœëŒ€ê°’ì—ì„œì˜ Î¼: {mu_range[mle_idx]:.3f}")
    print(f"ì‹¤ì œ MLE ì¶”ì •ê°’: {mu_mle:.3f}")
    print(f"ë‘ ë°©ë²•ì˜ ì°¨ì´: {abs(mu_range[optimal_idx] - mu_range[mle_idx]):.6f}")

mle_kl_relationship_demo()
```

#### Cross-Entropyì™€ì˜ ê´€ê³„

ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” **Cross-Entropy ì†ì‹¤í•¨ìˆ˜**ëŠ” ì‹¤ì œë¡œ **Negative Log-Likelihood**ì´ë‹¤:

ë¶„ë¥˜ ë¬¸ì œì—ì„œ Cross-Entropy: $$ \text{CrossEntropy} = -\frac{1}{n}\sum_{i=1}^{n} \sum_{c=1}^{C} y_{ic} \log p_{ic} $$

MLEì˜ Negative Log-Likelihood: $$ \text{NLL} = -\frac{1}{n}\sum_{i=1}^{n} \log p_{\theta}(y_i|x_i) $$

ë‘ ì‹ì´ ë™ì¼í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤!

```python
def cross_entropy_mle_demo():
    """Cross-Entropyì™€ MLEì˜ ê´€ê³„ ì‹œì—°"""
    
    # 3-í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œ ì‹œë®¬ë ˆì´ì…˜
    n_samples = 1000
    n_classes = 3
    
    # ì‹¤ì œ ë ˆì´ë¸” (ì›-í•« ì¸ì½”ë”©)
    true_labels = np.random.randint(0, n_classes, n_samples)
    y_true = np.eye(n_classes)[true_labels]
    
    # ëª¨ë¸ ì˜ˆì¸¡ (ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥)
    logits = np.random.randn(n_samples, n_classes)
    y_pred = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
    
    # 1. Cross-Entropy ê³„ì‚°
    cross_entropy = -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))
    
    # 2. Negative Log-Likelihood ê³„ì‚° (ë™ì¼í•œ ê²ƒ)
    nll = -np.mean([np.log(y_pred[i, true_labels[i]] + 1e-15) 
                    for i in range(n_samples)])
    
    # 3. ë‹¤í•­ë¶„í¬ì˜ MLEë¡œ í•´ì„
    # ê° í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ MLEë¡œ ì¶”ì •
    class_counts = np.bincount(true_labels, minlength=n_classes)
    mle_probs = class_counts / n_samples
    
    print(f"=== Cross-Entropyì™€ MLEì˜ ë™ì¹˜ì„± ===")
    print(f"Cross-Entropy: {cross_entropy:.6f}")
    print(f"Negative Log-Likelihood: {nll:.6f}")
    print(f"ì°¨ì´: {abs(cross_entropy - nll):.10f}")
    print()
    print(f"í´ë˜ìŠ¤ë³„ ì‹¤ì œ ë¹ˆë„: {class_counts}")
    print(f"MLE ì¶”ì • í™•ë¥ : {mle_probs}")
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # í´ë˜ìŠ¤ ë¶„í¬
    classes = [f'í´ë˜ìŠ¤ {i}' for i in range(n_classes)]
    ax1.bar(classes, class_counts, alpha=0.7, color='skyblue')
    ax1.set_ylabel('ë¹ˆë„')
    ax1.set_title('í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬')
    
    for i, count in enumerate(class_counts):
        ax1.text(i, count + 10, str(count), ha='center', fontweight='bold')
    
    # ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ ì˜ˆì‹œ (ì²« 100ê°œ ìƒ˜í”Œ)
    sample_indices = range(100)
    ax2.plot(sample_indices, y_pred[:100, 0], 'r-', label='í´ë˜ìŠ¤ 0 í™•ë¥ ', alpha=0.7)
    ax2.plot(sample_indices, y_pred[:100, 1], 'g-', label='í´ë˜ìŠ¤ 1 í™•ë¥ ', alpha=0.7)
    ax2.plot(sample_indices, y_pred[:100, 2], 'b-', label='í´ë˜ìŠ¤ 2 í™•ë¥ ', alpha=0.7)
    
    ax2.set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')
    ax2.set_ylabel('ì˜ˆì¸¡ í™•ë¥ ')
    ax2.set_title('ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥  (ì²« 100ê°œ ìƒ˜í”Œ)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

cross_entropy_mle_demo()
```

#### ì •ë³´ì´ë¡ ì  í•´ì„ì˜ í†µí•©

MLEì™€ KL Divergenceì˜ ê´€ê³„ë¥¼ ì •ë³´ì´ë¡ ì ìœ¼ë¡œ í•´ì„í•˜ë©´:

1. **ì••ì¶• ê´€ì **: MLEëŠ” ë°ì´í„°ë¥¼ ê°€ì¥ íš¨ìœ¨ì ìœ¼ë¡œ ì••ì¶•í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì°¾ëŠ”ë‹¤
2. **ì •ë³´ ì†ì‹¤ ìµœì†Œí™”**: KL divergenceëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë°œìƒí•˜ëŠ” ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•œë‹¤
3. **ì˜ˆì¸¡ ì •í™•ë„**: ë‘ ë°©ë²• ëª¨ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ìµœëŒ€í™”í•œë‹¤

```python
def information_theory_integration():
    """ì •ë³´ì´ë¡ ì  ê´€ì ì—ì„œì˜ MLE-KL í†µí•©"""
    
    # ì‹œë‚˜ë¦¬ì˜¤: ì´ë¯¸ì§€ í”½ì…€ ê°’ ë¶„í¬ ëª¨ë¸ë§
    # ì‹¤ì œ ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ë“¤ (0-255)
    np.random.seed(42)
    
    # ê°€ìƒì˜ "ìì—° ì´ë¯¸ì§€" í”½ì…€ ë¶„í¬ (bimodal)
    true_pixels = np.concatenate([
        np.random.normal(60, 20, 500),   # ì–´ë‘ìš´ ì˜ì—­
        np.random.normal(180, 30, 500)   # ë°ì€ ì˜ì—­
    ])
    true_pixels = np.clip(true_pixels, 0, 255).astype(int)
    
    # ê²½í—˜ì  ë¶„í¬ ìƒì„±
    pixel_counts = np.bincount(true_pixels, minlength=256)
    empirical_dist = pixel_counts / pixel_counts.sum()
    
    # ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ í”¼íŒ…
    models = {
        'Uniform': np.ones(256) / 256,
        'Single Gaussian': None,  # ê³„ì‚° í›„ ì±„ì›€
        'Optimal Model': empirical_dist  # ì™„ë²½í•œ ëª¨ë¸
    }
    
    # ë‹¨ì¼ ê°€ìš°ì‹œì•ˆ ëª¨ë¸ (MLEë¡œ ì¶”ì •)
    mu_mle = np.mean(true_pixels)
    sigma_mle = np.std(true_pixels)
    x_range = np.arange(256)
    gaussian_model = norm.pdf(x_range, mu_mle, sigma_mle)
    gaussian_model = gaussian_model / gaussian_model.sum()
    models['Single Gaussian'] = gaussian_model
    
    # ê° ëª¨ë¸ì— ëŒ€í•œ ì§€í‘œ ê³„ì‚°
    results = {}
    
    for name, model_dist in models.items():
        # KL Divergence
        kl_div = np.sum(empirical_dist * np.log(empirical_dist / (model_dist + 1e-15)))
        
        # Cross-Entropy
        cross_entropy = -np.sum(empirical_dist * np.log(model_dist + 1e-15))
        
        # Entropy of empirical distribution
        entropy_empirical = -np.sum(empirical_dist * np.log(empirical_dist + 1e-15))
        
        # Log-Likelihood (on original data)
        log_likelihood = np.sum([np.log(model_dist[pixel] + 1e-15) for pixel in true_pixels])
        
        # Compression efficiency (bits per pixel)
        bits_per_pixel = -log_likelihood / len(true_pixels) / np.log(2)
        
        results[name] = {
            'KL_Divergence': kl_div,
            'Cross_Entropy': cross_entropy,
            'Log_Likelihood': log_likelihood,
            'Bits_per_Pixel': bits_per_pixel
        }
    
    # ê²°ê³¼ ì¶œë ¥
    print("=== ì •ë³´ì´ë¡ ì  ê´€ì ì—ì„œì˜ ëª¨ë¸ ë¹„êµ ===")
    print(f"ê²½í—˜ì  ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼: {entropy_empirical:.4f} bits")
    print()
    
    for name, metrics in results.items():
        print(f"{name}:")
        print(f"  KL Divergence: {metrics['KL_Divergence']:.4f}")
        print(f"  Cross Entropy: {metrics['Cross_Entropy']:.4f}")
        print(f"  Log-Likelihood: {metrics['Log_Likelihood']:.2f}")
        print(f"  ì••ì¶• íš¨ìœ¨ì„±: {metrics['Bits_per_Pixel']:.4f} bits/pixel")
        print()
    
    # ì‹œê°í™”
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ì‹¤ì œ ë°ì´í„°ì™€ ëª¨ë¸ ë¶„í¬
    ax1.hist(true_pixels, bins=50, density=True, alpha=0.6, label='ì‹¤ì œ ë°ì´í„°')
    ax1.plot(x_range, empirical_dist, 'g-', linewidth=2, label='ê²½í—˜ì  ë¶„í¬')
    ax1.plot(x_range, gaussian_model, 'r--', linewidth=2, label='ê°€ìš°ì‹œì•ˆ ëª¨ë¸')
    ax1.set_xlabel('í”½ì…€ ê°’')
    ax1.set_ylabel('í™•ë¥  ë°€ë„')
    ax1.set_title('ì‹¤ì œ ë¶„í¬ vs ëª¨ë¸ ë¶„í¬')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. KL Divergence ë¹„êµ
    model_names = list(results.keys())
    kl_values = [results[name]['KL_Divergence'] for name in model_names]
    
    ax2.bar(model_names, kl_values, alpha=0.7, color=['red', 'blue', 'green'])
    ax2.set_ylabel('KL Divergence')
    ax2.set_title('ëª¨ë¸ë³„ KL Divergence (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)')
    ax2.tick_params(axis='x', rotation=45)
    
    # 3. ì••ì¶• íš¨ìœ¨ì„±
    compression_values = [results[name]['Bits_per_Pixel'] for name in model_names]
    
    ax3.bar(model_names, compression_values, alpha=0.7, color=['red', 'blue', 'green'])
    ax3.set_ylabel('Bits per Pixel')
    ax3.set_title('ì••ì¶• íš¨ìœ¨ì„± (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)')
    ax3.tick_params(axis='x', rotation=45)
    
    # 4. Cross-Entropy vs KL Divergence ê´€ê³„
    ce_values = [results[name]['Cross_Entropy'] for name in model_names]
    
    ax4.scatter(kl_values, ce_values, s=100, alpha=0.7)
    for i, name in enumerate(model_names):
        ax4.annotate(name, (kl_values[i], ce_values[i]), 
                    xytext=(5, 5), textcoords='offset points')
    
    ax4.set_xlabel('KL Divergence')
    ax4.set_ylabel('Cross Entropy')
    ax4.set_title('KL Divergence vs Cross Entropy')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("=== í•µì‹¬ í†µì°° ===")
    print("1. KL Divergenceê°€ ë‚®ì„ìˆ˜ë¡ ë” ì¢‹ì€ ëª¨ë¸")
    print("2. Cross-Entropy = KL Divergence + ë°ì´í„° ì—”íŠ¸ë¡œí”¼")
    print("3. ì••ì¶• íš¨ìœ¨ì„±ê³¼ ëª¨ë¸ í’ˆì§ˆì€ ë°˜ë¹„ë¡€ ê´€ê³„")
    print("4. MLEëŠ” ì´ ëª¨ë“  ì§€í‘œë¥¼ ë™ì‹œì— ìµœì í™”")

information_theory_integration()
```

ì´ë ‡ê²Œ MLEì™€ KL DivergenceëŠ” ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ê°™ì€ ëª©í‘œë¥¼ ì¶”êµ¬í•˜ëŠ” **ìŒëŒ€ ê°œë…(dual concepts)** ì´ë‹¤. MLEëŠ” "ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ëª¨ë¸"ì„ ì°¾ê³ , KL Divergence ìµœì†Œí™”ëŠ” "ì‹¤ì œ ë¶„í¬ì™€ ê°€ì¥ ê°€ê¹Œìš´ ëª¨ë¸"ì„ ì°¾ëŠ”ë‹¤. ê²°êµ­ ë‘˜ ë‹¤ **ìµœì ì˜ ëª¨ë¸**ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì´ë©°, ìˆ˜í•™ì ìœ¼ë¡œëŠ” ì™„ì „íˆ ë™ì¹˜ì´ë‹¤.

## ğŸ¤– VAEì—ì„œì˜ MLEì™€ KL Divergence í™œìš©

**Variational Autoencoder(VAE)** ëŠ” MLEì™€ KL Divergenceë¥¼ ê°€ì¥ ìš°ì•„í•˜ê²Œ ê²°í•©í•œ ìƒì„±ëª¨ë¸ì´ë‹¤. VAEì˜ í•µì‹¬ì€ **Evidence Lower BOund(ELBO)** ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì¸ë°, ì´ëŠ” ê²°êµ­ ë°ì´í„°ì˜ ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.

[VAE êµ¬ì¡°ë„: ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ì™€ ì ì¬ë³€ìˆ˜ zì˜ íë¦„]

### VAEì˜ ìˆ˜í•™ì  ê¸°ë°˜

VAEëŠ” ë‹¤ìŒê³¼ ê°™ì€ í™•ë¥ ì  ê·¸ë˜í”„ ëª¨ë¸ì„ ê°€ì •í•œë‹¤:

- **ì ì¬ë³€ìˆ˜**: $$z \sim p(z)$$ (ë³´í†µ í‘œì¤€ì •ê·œë¶„í¬)
- **ìƒì„±ê³¼ì •**: $$x \sim p_\theta(x|z)$$ (ë””ì½”ë”)
- **ì¸ì‹ê³¼ì •**: $$z \sim q_\phi(z|x)$$ (ì¸ì½”ë”)

**ëª©í‘œ**: ë°ì´í„°ì˜ ë¡œê·¸ ê°€ëŠ¥ë„ $$\log p_\theta(x)$$ë¥¼ ìµœëŒ€í™”

í•˜ì§€ë§Œ $$p_\theta(x) = \int p_\theta(x|z)p(z)dz$$ëŠ” ì§ì ‘ ê³„ì‚°í•˜ê¸° ì–´ë µë‹¤.

### ELBO ìœ ë„ ê³¼ì •

Jensen's ë¶€ë“±ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ ê°€ëŠ¥ë„ì˜ í•˜í•œ(Lower Bound)ì„ êµ¬í•œë‹¤:

$$ \log p_\theta(x) = \log \int p_\theta(x|z)p(z)dz $$

$$ = \log \mathbb{E}_{q_\phi(z|x)} \left[ \frac{p_\theta(x|z)p(z)}{q_\phi(z|x)} \right] $$

$$ \geq \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p_\theta(x|z)p(z)}{q_\phi(z|x)} \right] $$

$$ = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x)||p(z)) $$

ì´ê²ƒì´ **ELBO(Evidence Lower BOund)** ì´ë‹¤:

$$ \mathcal{L}_{ELBO} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x)||p(z)) $$

### ELBOì˜ ë‘ êµ¬ì„±ìš”ì†Œ

1. **ì¬êµ¬ì„± ì†ì‹¤(Reconstruction Loss)**: $$\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$$
    
    - ë””ì½”ë”ê°€ ì›ë³¸ ë°ì´í„°ë¥¼ ì˜ ë³µì›í•˜ëŠ”ì§€ ì¸¡ì •
    - MLE ê´€ì ì—ì„œ ë°ì´í„° ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€í™”
2. **KL ì •ê·œí™” í•­(KL Regularization)**: $$D_{KL}(q_\phi(z|x)||p(z))$$
    
    - ì¸ì½”ë”ì˜ ì¶œë ¥ë¶„í¬ê°€ ì‚¬ì „ë¶„í¬ì— ê°€ê¹ë„ë¡ ì œì•½
    - ì ì¬ê³µê°„ì˜ êµ¬ì¡°ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal

class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super(VAE, self).__init__()
        
        # ì¸ì½”ë” (recognition network)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # ì ì¬ë³€ìˆ˜ì˜ í‰ê· ê³¼ ë¡œê·¸ë¶„ì‚°
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        
        # ë””ì½”ë” (generative network)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()  # MNISTì˜ ê²½ìš° [0,1] ë²”ìœ„
        )
    
    def encode(self, x):
        """ì¸ì½”ë”: x -> (mu, logvar)"""
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        """ì¬ë§¤ê°œí™” íŠ¸ë¦­: (mu, logvar) -> z"""
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu
    
    def decode(self, z):
        """ë””ì½”ë”: z -> x"""
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar

def vae_loss_function(recon_x, x, mu, logvar, beta=1.0):
    """VAE ì†ì‹¤í•¨ìˆ˜: ELBO = ì¬êµ¬ì„±ì†ì‹¤ + Î²Ã—KLì†ì‹¤"""
    
    # ì¬êµ¬ì„± ì†ì‹¤ (Negative Log-Likelihood)
    # ë² ë¥´ëˆ„ì´ ë¶„í¬ ê°€ì •: BCE Loss
    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')
    
    # KL Divergence: q(z|x) || p(z)
    # ê°€ìš°ì‹œì•ˆ ë¶„í¬ ê°„ì˜ KL divergence í•´ì„ì  í•´
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    # Î²-VAE: KL í•­ì— ê°€ì¤‘ì¹˜ Î² ì ìš©
    total_loss = recon_loss + beta * kl_loss
    
    return total_loss, recon_loss, kl_loss

# VAE í•™ìŠµ ì˜ˆì‹œ
def train_vae_step(model, data, optimizer, beta=1.0):
    model.train()
    optimizer.zero_grad()
    
    # Forward pass
    recon_data, mu, logvar = model(data)
    
    # ì†ì‹¤ ê³„ì‚°
    loss, recon_loss, kl_loss = vae_loss_function(
        recon_data, data, mu, logvar, beta
    )
    
    # Backward pass
    loss.backward()
    optimizer.step()
    
    return {
        'total_loss': loss.item(),
        'recon_loss': recon_loss.item(),
        'kl_loss': kl_loss.item()
    }

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
batch_size, input_dim = 32, 784
model = VAE(input_dim=input_dim)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
dummy_data = torch.randn(batch_size, input_dim).sigmoid()
losses = train_vae_step(model, dummy_data, optimizer)

print("VAE ì†ì‹¤ êµ¬ì„±ìš”ì†Œ:")
print(f"ì¬êµ¬ì„± ì†ì‹¤: {losses['recon_loss']:.2f}")
print(f"KL ì†ì‹¤: {losses['kl_loss']:.2f}")
print(f"ì´ ì†ì‹¤: {losses['total_loss']:.2f}")
```

### Î²-VAE: KL Divergenceì˜ ê°€ì¤‘ì¹˜ ì¡°ì ˆ

Î²-VAEëŠ” KL í•­ì— ê°€ì¤‘ì¹˜ Î²ë¥¼ ë„ì…í•˜ì—¬ ì¬êµ¬ì„± í’ˆì§ˆê³¼ ì ì¬í‘œí˜„ì˜ ë¶„ë¦¬ì„±(disentanglement) ì‚¬ì´ì˜ ê· í˜•ì„ ì¡°ì ˆí•œë‹¤:

$$ \mathcal{L}_{\beta-VAE} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta \cdot D_{KL}(q_\phi(z|x)||p(z)) $$

- **Î² > 1**: KL ì œì•½ì„ ê°•í™”í•˜ì—¬ ë” ë¶„ë¦¬ëœ í‘œí˜„ í•™ìŠµ (í‘œí˜„ë ¥ ê°ì†Œ)
- **Î² < 1**: ì¬êµ¬ì„±ì— ì§‘ì¤‘í•˜ì—¬ ë” í‘œí˜„ë ¥ ìˆëŠ” ì ì¬ë³€ìˆ˜ (ë¶„ë¦¬ì„± ê°ì†Œ)

[Î²ê°’ì— ë”°ë¥¸ ì ì¬ê³µê°„ ë³€í™” ì‹œê°í™”]

## ğŸ­ GANì—ì„œì˜ MLEì™€ Divergence

**Generative Adversarial Network(GAN)** ëŠ” ë‘ ì‹ ê²½ë§ì´ ê²½ìŸí•˜ëŠ” ë¯¸ë‹ˆë§¥ìŠ¤ ê²Œì„ìœ¼ë¡œ ê³µì‹í™”ë˜ë©°, íŒë³„ìëŠ” MLE ê¸°ë°˜ìœ¼ë¡œ, ìƒì„±ìëŠ” ë¶„í¬ ê°„ divergenceë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµëœë‹¤.

[GAN êµ¬ì¡°ë„: ìƒì„±ìì™€ íŒë³„ìì˜ ì ëŒ€ì  í•™ìŠµ ê³¼ì •]

### GANì˜ ìˆ˜í•™ì  ê¸°ë°˜

GANì˜ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¯¸ë‹ˆë§¥ìŠ¤ ê²Œì„ì´ë‹¤:

$$ \min_G \max_D V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1-D(G(z)))] $$

ì—¬ê¸°ì„œ:

- **íŒë³„ì D**: ì‹¤ì œ ë°ì´í„°ì™€ ê°€ì§œ ë°ì´í„°ë¥¼ êµ¬ë¶„ (ì´ì§„ ë¶„ë¥˜ê¸°)
- **ìƒì„±ì G**: ë…¸ì´ì¦ˆ zì—ì„œ ê°€ì§œ ë°ì´í„° ìƒì„±

### íŒë³„ìì—ì„œì˜ MLE

íŒë³„ìëŠ” ì‹¤ì œë¡œ **ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ MLEë¡œ í‘¸ëŠ” ê²ƒ**ì´ë‹¤:

- ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•´: $$\max_D \mathbb{E}_{x \sim p_{data}}[\log D(x)]$$
- ê°€ì§œ ë°ì´í„°ì— ëŒ€í•´: $$\max_D \mathbb{E}_{z \sim p_z}[\log(1-D(G(z)))]$$

ì´ëŠ” ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ MLEì™€ ì •í™•íˆ ê°™ì€ í˜•íƒœì´ë‹¤.

### ìƒì„±ìì™€ Jensen-Shannon Divergence

ìµœì  íŒë³„ì $$D^*$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ìƒì„±ìì˜ ì†ì‹¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ëœë‹¤:

$$ \min_G V(D^*,G) = -\log 4 + 2 \cdot JS(p_{data} || p_g) $$

ì—¬ê¸°ì„œ $$JS(P||Q)$$ëŠ” Jensen-Shannon Divergenceì´ë‹¤:

$$ JS(P||Q) = \frac{1}{2}D_{KL}(P||M) + \frac{1}{2}D_{KL}(Q||M) $$

$$M = \frac{1}{2}(P+Q)$$ëŠ” ë‘ ë¶„í¬ì˜ í‰ê· ì´ë‹¤.

> Jensen-Shannon DivergenceëŠ” KL Divergenceì˜ ëŒ€ì¹­í™”ëœ ë²„ì „ìœ¼ë¡œ, í•­ìƒ 0ê³¼ log 2 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©° ê±°ë¦¬ ì§€í‘œì˜ ì„±ì§ˆì„ ë§Œì¡±í•œë‹¤. {: .prompt-tip}

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class Generator(nn.Module):
    def __init__(self, noise_dim=100, hidden_dim=128, output_dim=784):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(noise_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Tanh()  # [-1, 1] ë²”ìœ„ ì¶œë ¥
        )
    
    def forward(self, z):
        return self.model(z)

class Discriminator(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=128):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()  # [0, 1] í™•ë¥  ì¶œë ¥
        )
    
    def forward(self, x):
        return self.model(x)

def train_gan_step(generator, discriminator, real_data, noise_dim=100):
    batch_size = real_data.size(0)
    device = real_data.device
    
    # ë ˆì´ë¸” ì •ì˜
    real_labels = torch.ones(batch_size, 1, device=device)
    fake_labels = torch.zeros(batch_size, 1, device=device)
    
    # =================
    # íŒë³„ì í•™ìŠµ (MLE)
    # =================
    discriminator.train()
    
    # ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ (MLE)
    real_output = discriminator(real_data)
    d_loss_real = F.binary_cross_entropy(real_output, real_labels)
    
    # ê°€ì§œ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ (MLE)
    noise = torch.randn(batch_size, noise_dim, device=device)
    fake_data = generator(noise).detach()  # ìƒì„±ì ê·¸ë˜ë””ì–¸íŠ¸ ì°¨ë‹¨
    fake_output = discriminator(fake_data)
    d_loss_fake = F.binary_cross_entropy(fake_output, fake_labels)
    
    # íŒë³„ì ì´ ì†ì‹¤ (Negative Log-Likelihood)
    d_loss = d_loss_real + d_loss_fake
    
    # =================
    # ìƒì„±ì í•™ìŠµ (JS Divergence ìµœì†Œí™”)
    # =================
    generator.train()
    
    # ìƒˆë¡œìš´ ê°€ì§œ ë°ì´í„° ìƒì„±
    noise = torch.randn(batch_size, noise_dim, device=device)
    fake_data = generator(noise)
    fake_output = discriminator(fake_data)
    
    # ìƒì„±ì ì†ì‹¤ (íŒë³„ìë¥¼ ì†ì´ë ¤ê³  í•¨)
    g_loss = F.binary_cross_entropy(fake_output, real_labels)
    
    return {
        'd_loss': d_loss.item(),
        'd_loss_real': d_loss_real.item(),
        'd_loss_fake': d_loss_fake.item(),
        'g_loss': g_loss.item(),
        'real_acc': (real_output > 0.5).float().mean().item(),
        'fake_acc': (fake_output <= 0.5).float().mean().item()
    }

# GAN ëª¨ë¸ ì´ˆê¸°í™”
noise_dim = 100
generator = Generator(noise_dim)
discriminator = Discriminator()

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

# ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
batch_size = 32
dummy_real_data = torch.randn(batch_size, 784).tanh()

# í•œ ìŠ¤í… í•™ìŠµ
losses = train_gan_step(generator, discriminator, dummy_real_data)

print("GAN í•™ìŠµ ê²°ê³¼:")
print(f"íŒë³„ì ì†ì‹¤ (ì „ì²´): {losses['d_loss']:.4f}")
print(f"íŒë³„ì ì†ì‹¤ (ì‹¤ì œ): {losses['d_loss_real']:.4f}")
print(f"íŒë³„ì ì†ì‹¤ (ê°€ì§œ): {losses['d_loss_fake']:.4f}")
print(f"ìƒì„±ì ì†ì‹¤: {losses['g_loss']:.4f}")
print(f"ì‹¤ì œ ë°ì´í„° ì •í™•ë„: {losses['real_acc']:.2f}")
print(f"ê°€ì§œ ë°ì´í„° ì •í™•ë„: {losses['fake_acc']:.2f}")
```

### WGANê³¼ Wasserstein Distance

ê¸°ì¡´ GANì˜ í•™ìŠµ ë¶ˆì•ˆì •ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ **Wasserstein GAN(WGAN)** ì´ ì œì•ˆë˜ì—ˆë‹¤. WGANì€ Jensen-Shannon Divergence ëŒ€ì‹  **Wasserstein Distance(Earth Mover's Distance)** ë¥¼ ì‚¬ìš©í•œë‹¤.

Wasserstein DistanceëŠ” ë‘ ë¶„í¬ ê°„ì˜ "ìµœì†Œ ìš´ì†¡ ë¹„ìš©"ìœ¼ë¡œ í•´ì„ë˜ë©°, ë¶„í¬ê°€ ê²¹ì¹˜ì§€ ì•Šì•„ë„ ì˜ë¯¸ ìˆëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì œê³µí•œë‹¤:

$$ W(p_{data}, p_g) = \inf_{\gamma \in \Pi(p_{data}, p_g)} \mathbb{E}_{(x,y) \sim \gamma}[||x-y||] $$

[Wasserstein Distance vs JS Divergence ë¹„êµ ì‹œê°í™”]

## ğŸŒŠ Diffusion Modelì—ì„œì˜ í™œìš©

**Diffusion Model** ì€ forward processì™€ reverse process ëª¨ë‘ì—ì„œ KL Divergenceë¥¼ í•µì‹¬ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ìƒì„±ëª¨ë¸ì´ë‹¤. ë…¸ì´ì¦ˆë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€í•˜ê³  ì œê±°í•˜ëŠ” ê³¼ì •ì„ í™•ë¥ ì ìœ¼ë¡œ ëª¨ë¸ë§í•œë‹¤.

[Diffusion Process ì‹œê°í™”: Forwardì™€ Reverse Process]

### Diffusion Modelì˜ ìˆ˜í•™ì  ê¸°ë°˜

**Forward Process(í™•ì‚° ê³¼ì •)**: ë°ì´í„°ì— ì ì§„ì ìœ¼ë¡œ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€

$$ q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I) $$

$$ q(x_{1:T}|x_0) = \prod_{t=1}^{T} q(x_t|x_{t-1}) $$

**Reverse Process(ì—­í™•ì‚° ê³¼ì •)**: ë…¸ì´ì¦ˆì—ì„œ ë°ì´í„°ë¡œ ë³µì›

$$ p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^{T} p_\theta(x_{t-1}|x_t) $$

$$ p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t)) $$

### DDPMì˜ ì†ì‹¤í•¨ìˆ˜ì™€ KL Divergence

**Denoising Diffusion Probabilistic Model(DDPM)** ì˜ í•™ìŠµ ëª©í‘œëŠ” variational lower boundë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ë‹¤:

$$ \mathbb{E}[-\log p_\theta(x_0)] \leq \mathbb{E}_q[-\log p_\theta(x_0|x_1)] + \sum_{t=2}^{T} \mathbb{E}_q[D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))] + D_{KL}(q(x_T|x_0)||p(x_T)) $$

ì´ ì‹ì˜ ê° í•­:

1. **ì¬êµ¬ì„± í•­**: $$\mathbb{E}_q[-\log p_\theta(x_0|x_1)]$$
2. **ì¤‘ê°„ KL í•­**: $$D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))$$
3. **ì‚¬ì „ë¶„í¬ ë§¤ì¹­ í•­**: $$D_{KL}(q(x_T|x_0)||p(x_T))$$

### ë‹¨ìˆœí™”ëœ ì†ì‹¤í•¨ìˆ˜

ë³µì¡í•œ KL divergenceë¥¼ ë‹¨ìˆœí™”í•˜ë©´, ìµœì¢…ì ìœ¼ë¡œ **ë…¸ì´ì¦ˆ ì˜ˆì¸¡ ë¬¸ì œ**ë¡œ ê·€ê²°ëœë‹¤:

$$ L_{simple} = \mathbb{E}_{t,x_0,\epsilon}[||\epsilon - \epsilon_\theta(x_t, t)||^2] $$

ì—¬ê¸°ì„œ $$\epsilon$$ì€ ì‹¤ì œ ë…¸ì´ì¦ˆ, $$\epsilon_\theta(x_t, t)$$ëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë…¸ì´ì¦ˆì´ë‹¤.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SimpleDiffusionModel(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=512, time_dim=128):
        super().__init__()
        
        # ì‹œê°„ ì„ë² ë”© (sinusoidal embedding)
        self.time_embedding = nn.Sequential(
            nn.Linear(time_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # ë…¸ì´ì¦ˆ ì˜ˆì¸¡ ë„¤íŠ¸ì›Œí¬
        self.noise_predictor = nn.Sequential(
            nn.Linear(input_dim + hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )
    
    def positional_encoding(self, timesteps, dim):
        """ì‹œê°„ ìŠ¤í…ì„ ìœ„í•œ positional encoding"""
        half_dim = dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim) * -emb)
        emb = timesteps.unsqueeze(1) * emb.unsqueeze(0)
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        return emb
    
    def forward(self, x_t, t):
        # ì‹œê°„ ì„ë² ë”©
        t_emb = self.positional_encoding(t.float(), 128)
        t_emb = self.time_embedding(t_emb)
        
        # ì…ë ¥ê³¼ ì‹œê°„ ì„ë² ë”© ê²°í•©
        h = torch.cat([x_t, t_emb], dim=1)
        
        # ë…¸ì´ì¦ˆ ì˜ˆì¸¡
        predicted_noise = self.noise_predictor(h)
        return predicted_noise

class DDPMTrainer:
    def __init__(self, model, num_timesteps=1000):
        self.model = model
        self.num_timesteps = num_timesteps
        
        # ë² íƒ€ ìŠ¤ì¼€ì¤„ (linear schedule)
        self.betas = torch.linspace(1e-4, 0.02, num_timesteps)
        self.alphas = 1.0 - self.betas
        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)
        
        # ìƒ˜í”Œë§ì„ ìœ„í•œ ì‚¬ì „ ê³„ì‚°
        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)
        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)
    
    def q_sample(self, x_0, t, noise=None):
        """Forward process: x_0ì—ì„œ x_të¡œ ë…¸ì´ì¦ˆ ì¶”ê°€"""
        if noise is None:
            noise = torch.randn_like(x_0)
        
        sqrt_alpha_cumprod_t = self.sqrt_alpha_cumprod[t].unsqueeze(1)
        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[t].unsqueeze(1)
        
        return sqrt_alpha_cumprod_t * x_0 + sqrt_one_minus_alpha_cumprod_t * noise
    
    def compute_loss(self, x_0):
        """DDPM ì†ì‹¤í•¨ìˆ˜ ê³„ì‚°"""
        batch_size = x_0.size(0)
        device = x_0.device
        
        # ëœë¤ ì‹œê°„ ìŠ¤í… ìƒ˜í”Œë§
        t = torch.randint(0, self.num_timesteps, (batch_size,), device=device)
        
        # ë…¸ì´ì¦ˆ ìƒì„±
        noise = torch.randn_like(x_0)
        
        # Forward processë¡œ ë…¸ì´ì¦ˆ ì¶”ê°€
        x_t = self.q_sample(x_0, t, noise)
        
        # ëª¨ë¸ë¡œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡
        predicted_noise = self.model(x_t, t)
        
        # MSE ì†ì‹¤ (ë‹¨ìˆœí™”ëœ KL divergence)
        loss = F.mse_loss(predicted_noise, noise)
        
        return loss
    
    def p_sample_step(self, x_t, t):
        """Reverse process: x_tì—ì„œ x_{t-1}ë¡œ í•œ ìŠ¤í… ì—­í™•ì‚°"""
        # ëª¨ë¸ë¡œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡
        predicted_noise = self.model(x_t, t)
        
        # ì•ŒíŒŒ ê°’ë“¤ ê°€ì ¸ì˜¤ê¸°
        alpha_t = self.alphas[t].unsqueeze(1)
        alpha_cumprod_t = self.alpha_cumprod[t].unsqueeze(1)
        beta_t = self.betas[t].unsqueeze(1)
        
        if t[0] > 0:
            alpha_cumprod_prev = self.alpha_cumprod[t-1].unsqueeze(1)
        else:
            alpha_cumprod_prev = torch.ones_like(alpha_cumprod_t)
        
        # í‰ê·  ê³„ì‚°
        mean = (1.0 / torch.sqrt(alpha_t)) * (
            x_t - (beta_t / torch.sqrt(1.0 - alpha_cumprod_t)) * predicted_noise
        )
        
        # ë¶„ì‚° ê³„ì‚°
        variance = beta_t * (1.0 - alpha_cumprod_prev) / (1.0 - alpha_cumprod_t)
        
        # ë…¸ì´ì¦ˆ ìƒ˜í”Œë§ (t=0ì¼ ë•ŒëŠ” ë…¸ì´ì¦ˆ ì—†ìŒ)
        if t[0] > 0:
            noise = torch.randn_like(x_t)
            return mean + torch.sqrt(variance) * noise
        else:
            return mean

# ì‚¬ìš© ì˜ˆì‹œ
def train_ddpm_step():
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = SimpleDiffusionModel()
    trainer = DDPMTrainer(model, num_timesteps=1000)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    # ë”ë¯¸ ë°ì´í„° (MNIST í˜•íƒœ)
    batch_size = 16
    x_0 = torch.randn(batch_size, 784) * 0.5  # ì •ê·œí™”ëœ ì´ë¯¸ì§€
    
    # í•œ ìŠ¤í… í•™ìŠµ
    model.train()
    optimizer.zero_grad()
    
    loss = trainer.compute_loss(x_0)
    loss.backward()
    optimizer.step()
    
    print(f"DDPM ì†ì‹¤: {loss.item():.6f}")
    
    # ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸ (í•œ ìŠ¤í…ë§Œ)
    model.eval()
    with torch.no_grad():
        # ìˆœìˆ˜ ë…¸ì´ì¦ˆì—ì„œ ì‹œì‘
        x_t = torch.randn(1, 784)
        t = torch.tensor([999])  # ë§ˆì§€ë§‰ ì‹œê°„ ìŠ¤í…
        
        # í•œ ìŠ¤í… ì—­í™•ì‚°
        x_prev = trainer.p_sample_step(x_t, t)
        
        print(f"ìƒ˜í”Œë§ - ì…ë ¥ ë…¸ì´ì¦ˆ í‰ê· : {x_t.mean().item():.4f}")
        print(f"ìƒ˜í”Œë§ - ì¶œë ¥ í‰ê· : {x_prev.mean().item():.4f}")

train_ddpm_step()
```

### Score-based Modelsê³¼ KL Divergence

ìµœê·¼ì˜ **Score-based Generative Models** ì€ KL divergenceë¥¼ ì§ì ‘ ìµœì†Œí™”í•˜ëŠ” ëŒ€ì‹ , **score function** (í™•ë¥ ë°€ë„ì˜ ë¡œê·¸ ê·¸ë˜ë””ì–¸íŠ¸)ì„ í•™ìŠµí•œë‹¤:

$$ s_\theta(x) \approx \nabla_x \log p_{data}(x) $$

ì´ëŠ” **Stein's Identity** ë¥¼ í†µí•´ KL divergenceì™€ ì—°ê²°ëœë‹¤:

$$ \nabla_x D_{KL}(p_{data}||p_\theta) = \mathbb{E}_{p_{data}}[s_\theta(x) - \nabla_x \log p_\theta(x)] $$

[Score Function ì‹œê°í™”: í™•ë¥ ë°€ë„ì˜ ê·¸ë˜ë””ì–¸íŠ¸ í•„ë“œ]

## ğŸ”— ìƒì„±ëª¨ë¸ë“¤ì˜ ë¹„êµì™€ í†µí•©ì  ê´€ì 

ê° ìƒì„±ëª¨ë¸ì´ MLEì™€ KL Divergenceë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ ë¹„êµí•´ë³´ë©´ í¥ë¯¸ë¡œìš´ íŒ¨í„´ì„ ë°œê²¬í•  ìˆ˜ ìˆë‹¤.

### ìƒì„±ëª¨ë¸ë³„ í•µì‹¬ ì›ë¦¬ ë¹„êµ

|ëª¨ë¸|MLE í™œìš©|KL Divergence í™œìš©|í•µì‹¬ ì•„ì´ë””ì–´|
|---|---|---|---|
|**VAE**|ELBO ìµœëŒ€í™”|ì ì¬ë¶„í¬ ì •ê·œí™”|ë³€ë¶„ì¶”ë¡ ìœ¼ë¡œ ìƒì„±ë¶„í¬ ê·¼ì‚¬|
|**GAN**|íŒë³„ì í•™ìŠµ|JS Divergence ìµœì†Œí™”|ì ëŒ€ì  í•™ìŠµìœ¼ë¡œ ë¶„í¬ ë§¤ì¹­|
|**Diffusion**|ë³€ë¶„í•˜í•œ ìµœëŒ€í™”|ê° ìŠ¤í…ë³„ ë¶„í¬ ë§¤ì¹­|ì ì§„ì  ë…¸ì´ì¦ˆ ì œê±°|
|**Flow-based**|ì§ì ‘ MLE|ì •ê·œí™” íë¦„|ê°€ì—­ë³€í™˜ìœ¼ë¡œ ì •í™•í•œ likelihood|

### í†µí•©ì  ê´€ì : ì •ë³´ì´ë¡ ì  í•´ì„

ëª¨ë“  ìƒì„±ëª¨ë¸ì˜ ê³µí†µ ëª©í‘œëŠ” **ì •ë³´ì´ë¡ ì  ê´€ì ì—ì„œ ë°ì´í„° ë¶„í¬ë¥¼ í•™ìŠµ**í•˜ëŠ” ê²ƒì´ë‹¤:

1. **ì••ì¶• ê´€ì **: ë°ì´í„°ì˜ ë³¸ì§ˆì  êµ¬ì¡°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶•
2. **ë³µì› ê´€ì **: ì••ì¶•ëœ í‘œí˜„ì—ì„œ ì›ë³¸ ë°ì´í„°ë¥¼ ë³µì›
3. **ìƒì„± ê´€ì **: í•™ìŠµëœ ë¶„í¬ì—ì„œ ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„±

```python
import torch
import matplotlib.pyplot as plt
import numpy as np

def compare_generative_models():
    """ìƒì„±ëª¨ë¸ë“¤ì˜ ì†ì‹¤í•¨ìˆ˜ ë¹„êµ"""
    
    # ë”ë¯¸ ë°ì´í„° ì„¤ì •
    batch_size, data_dim, latent_dim = 32, 784, 64
    x = torch.randn(batch_size, data_dim) * 0.5
    
    print("=== ìƒì„±ëª¨ë¸ë³„ ì†ì‹¤í•¨ìˆ˜ ë¹„êµ ===\n")
    
    # 1. VAE ì†ì‹¤
    mu = torch.randn(batch_size, latent_dim)
    logvar = torch.randn(batch_size, latent_dim)
    recon_x = torch.sigmoid(torch.randn(batch_size, data_dim))
    
    recon_loss = F.binary_cross_entropy(recon_x, torch.sigmoid(x))
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_size
    vae_loss = recon_loss + kl_loss
    
    print(f"VAE ì†ì‹¤:")
    print(f"  ì¬êµ¬ì„± ì†ì‹¤ (NLL): {recon_loss.item():.4f}")
    print(f"  KL ì •ê·œí™”: {kl_loss.item():.4f}")
    print(f"  ì´ ì†ì‹¤ (ELBO): {vae_loss.item():.4f}\n")
    
    # 2. GAN ì†ì‹¤
    real_output = torch.sigmoid(torch.randn(batch_size, 1))
    fake_output = torch.sigmoid(torch.randn(batch_size, 1))
    
    d_loss_real = F.binary_cross_entropy(real_output, torch.ones_like(real_output))
    d_loss_fake = F.binary_cross_entropy(fake_output, torch.zeros_like(fake_output))
    d_loss = d_loss_real + d_loss_fake
    g_loss = F.binary_cross_entropy(fake_output, torch.ones_like(fake_output))
    
    print(f"GAN ì†ì‹¤:")
    print(f"  íŒë³„ì ì†ì‹¤ (MLE): {d_loss.item():.4f}")
    print(f"  ìƒì„±ì ì†ì‹¤ (JS): {g_loss.item():.4f}\n")
    
    # 3. Diffusion ì†ì‹¤
    noise = torch.randn_like(x)
    predicted_noise = torch.randn_like(x)
    diffusion_loss = F.mse_loss(predicted_noise, noise)
    
    print(f"Diffusion ì†ì‹¤:")
    print(f"  ë…¸ì´ì¦ˆ ì˜ˆì¸¡ (ë‹¨ìˆœí™”ëœ KL): {diffusion_loss.item():.4f}\n")
    
    # ì •ë³´ì´ë¡ ì  í•´ì„
    print("=== ì •ë³´ì´ë¡ ì  í•´ì„ ===")
    print("VAE: I(X;Z) ìµœëŒ€í™”, KL(q(z|x)||p(z)) ìµœì†Œí™”")
    print("GAN: JS(p_data||p_g) ìµœì†Œí™”")
    print("Diffusion: KL(q(x_{t-1}|x_t,x_0)||p_Î¸(x_{t-1}|x_t)) ìµœì†Œí™”")

compare_generative_models()

def information_theory_perspective():
    """ì •ë³´ì´ë¡  ê´€ì ì—ì„œì˜ ìƒì„±ëª¨ë¸ ë¶„ì„"""
    
    print("\n=== ì •ë³´ì´ë¡ ì  í†µí•© ê´€ì  ===\n")
    
    # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ì˜ˆì‹œ
    def entropy(p):
        """ì´ì‚° ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        p = p + 1e-10  # ìˆ˜ì¹˜ ì•ˆì •ì„±
        return -torch.sum(p * torch.log(p))
    
    def kl_divergence(p, q):
        """ì´ì‚° ë¶„í¬ì˜ KL divergence ê³„ì‚°"""
        p, q = p + 1e-10, q + 1e-10
        return torch.sum(p * torch.log(p / q))
    
    # ì˜ˆì‹œ ë¶„í¬ë“¤
    p_data = torch.tensor([0.5, 0.3, 0.2])  # ì‹¤ì œ ë°ì´í„° ë¶„í¬
    p_model = torch.tensor([0.4, 0.4, 0.2])  # ëª¨ë¸ ë¶„í¬
    p_uniform = torch.tensor([1/3, 1/3, 1/3])  # ê· ë“± ë¶„í¬
    
    h_data = entropy(p_data)
    h_model = entropy(p_model)
    h_uniform = entropy(p_uniform)
    
    kl_data_model = kl_divergence(p_data, p_model)
    kl_data_uniform = kl_divergence(p_data, p_uniform)
    
    print(f"ë°ì´í„° ë¶„í¬ ì—”íŠ¸ë¡œí”¼: {h_data.item():.4f}")
    print(f"ëª¨ë¸ ë¶„í¬ ì—”íŠ¸ë¡œí”¼: {h_model.item():.4f}")
    print(f"ê· ë“± ë¶„í¬ ì—”íŠ¸ë¡œí”¼: {h_uniform.item():.4f}\n")
    
    print(f"KL(p_data || p_model): {kl_data_model.item():.4f}")
    print(f"KL(p_data || p_uniform): {kl_data_uniform.item():.4f}\n")
    
    print("í•´ì„:")
    print("- ë‚®ì€ KL divergenceëŠ” ë” ì¢‹ì€ ëª¨ë¸ì„ ì˜ë¯¸")
    print("- ë†’ì€ ì—”íŠ¸ë¡œí”¼ëŠ” ë” ë‹¤ì–‘í•œ ìƒì„±ì„ ì˜ë¯¸")
    print("- ìƒì„±ëª¨ë¸ì€ KL divergenceì™€ ì—”íŠ¸ë¡œí”¼ì˜ ê· í˜•ì„ ë§ì¶¤")

information_theory_perspective()
```

### ì‹¤ì „ ì‘ìš©ì—ì„œì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼

ì‹¤ì œ ì‚°ì—… ì‘ìš©ì—ì„œëŠ” ì—¬ëŸ¬ ê¸°ë²•ì„ ì¡°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì´ ì£¼ë¡œ ì‚¬ìš©ëœë‹¤:

- **VAE + GAN**: VAEì˜ ì•ˆì •ì„±ê³¼ GANì˜ ìƒì„± í’ˆì§ˆ ê²°í•©
- **Diffusion + Classifier Guidance**: ì¡°ê±´ë¶€ ìƒì„±ì„ ìœ„í•œ ë¶„ë¥˜ê¸° ê²°í•©
- **Flow + VAE**: ì •í™•í•œ likelihoodì™€ íš¨ìœ¨ì  ìƒ˜í”Œë§ ê²°í•©

[í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨]

## ğŸ“ ë§ˆë¬´ë¦¬: ì‹¤ë¬´ì—ì„œì˜ í™œìš© ê°€ì´ë“œ

### ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

ìƒì„±ëª¨ë¸ì„ ì„ íƒí•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œë“¤:

1. **ì •í™•í•œ likelihood í•„ìš”**: Flow-based models
2. **ì•ˆì •ì  í•™ìŠµ ì¤‘ìš”**: VAE, Diffusion
3. **ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±**: GAN, Diffusion
4. **ë¹ ë¥¸ ìƒ˜í”Œë§ í•„ìš”**: VAE, GAN
5. **í•´ì„ ê°€ëŠ¥ì„± ì¤‘ìš”**: VAE

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ íŒ

```python
def hyperparameter_tuning_guide():
    """ìƒì„±ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ"""
    
    tuning_guide = {
        "VAE": {
            "beta": "0.1~10 (ë†’ì„ìˆ˜ë¡ ë¶„ë¦¬ëœ í‘œí˜„, ë‚®ì„ìˆ˜ë¡ ì¬êµ¬ì„± í’ˆì§ˆ)",
            "latent_dim": "16~512 (ë°ì´í„° ë³µì¡ë„ì— ë”°ë¼)",
            "learning_rate": "1e-4~1e-3",
            "architecture": "ì ì§„ì  í¬ê¸° ê°ì†Œ/ì¦ê°€"
        },
        
        "GAN": {
            "learning_rate": "1e-4~2e-4 (Gì™€ D ê· í˜• ì¤‘ìš”)",
            "batch_size": "64~512 (í´ìˆ˜ë¡ ì•ˆì •ì )",
            "noise_dim": "100~512",
            "discriminator_steps": "1~5 (íŒë³„ì ê°•í™” ì‹œ)"
        },
        
        "Diffusion": {
            "num_timesteps": "1000~4000",
            "beta_schedule": "linear, cosine, quadratic",
            "learning_rate": "1e-5~1e-4",
            "ema_decay": "0.999~0.9999"
        }
    }
    
    for model, params in tuning_guide.items():
        print(f"=== {model} í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì´ë“œ ===")
        for param, description in params.items():
            print(f"{param}: {description}")
        print()

hyperparameter_tuning_guide()
```

### ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­

ìƒì„±ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì£¼ìš” ì§€í‘œë“¤:

- **FID (FrÃ©chet Inception Distance)**: ìƒì„± ì´ë¯¸ì§€ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±
- **IS (Inception Score)**: ìƒì„± ì´ë¯¸ì§€ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±
- **LPIPS (Learned Perceptual Image Patch Similarity)**: ì§€ê°ì  ìœ ì‚¬ë„
- **Precision/Recall**: ìƒì„± í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì˜ ë¶„ë¦¬ ì¸¡ì •

> ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •ê³¼ KL DivergenceëŠ” í˜„ëŒ€ ìƒì„±ëª¨ë¸ì˜ ìˆ˜í•™ì  ê¸°ë°˜ì´ë‹¤. ì´ ë‘ ê°œë…ì„ ê¹Šì´ ì´í•´í•˜ë©´ ìƒˆë¡œìš´ ìƒì„±ëª¨ë¸ ë…¼ë¬¸ì„ ì½ê±°ë‚˜ ì§ì ‘ ëª¨ë¸ì„ ì„¤ê³„í•  ë•Œ í•µì‹¬ ì›ë¦¬ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. {: .prompt-tip}

ì´ì œ ì—¬ëŸ¬ë¶„ë„ VAE ë…¼ë¬¸ì—ì„œ ELBOë¥¼ ë³´ê±°ë‚˜, GAN ë…¼ë¬¸ì—ì„œ Jensen-Shannon Divergenceë¥¼ ë³´ê±°ë‚˜, Diffusion ë…¼ë¬¸ì—ì„œ KL í•­ë“¤ì„ ë³¼ ë•Œ ê·¸ ìˆ˜í•™ì  ì˜ë¯¸ì™€ ì§ê´€ì„ ì •í™•íˆ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤!